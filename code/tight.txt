Good job.  You have followed directions.  Asserter passes.
[45mALN> [0m [94m[pre-loop] adding layer conv_1 with input input[0m 
{'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fe534071490>, 'num_filters': 48}
[45mALN> [0m [45mloop outputs: [0m 
[45mALN> [0m [91m[repeating section] adding layer conv_2_unroll=0 with input conv_1[0m 
{'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fe53408e4d0>, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_3_unroll=0 with input conv_2_unroll=0[0m 
{'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fe53408ed10>, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_4_unroll=0 with input conv_3_unroll=0[0m 
{'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fe53401e2d0>, 'num_filters': 48}
[45mALN> [0m [91madding loop:[0m 
[45mALN> [0m [45mloop outputs: ('conv_2', ('conv_4_unroll=0', 'sum'))[0m 
[45mALN> [0m [91m[repeating section] adding layer conv_2_unroll=1 with input ['conv_1', 'conv_4_unroll=0'][0m 
{'b': conv_2_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_2_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_3_unroll=1 with input conv_2_unroll=1[0m 
{'b': conv_3_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_3_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_4_unroll=1 with input conv_3_unroll=1[0m 
{'b': conv_4_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_4_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91madding loop:[0m 
[45mALN> [0m [45mloop outputs: ('conv_2', ('conv_4_unroll=1', 'sum'))[0m 
[45mALN> [0m [91m[repeating section] adding layer conv_2_unroll=2 with input ['conv_1', 'conv_4_unroll=1'][0m 
{'b': conv_2_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_2_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_3_unroll=2 with input conv_2_unroll=2[0m 
{'b': conv_3_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_3_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_4_unroll=2 with input conv_3_unroll=2[0m 
{'b': conv_4_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_4_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91madding loop:[0m 
[45mALN> [0m [45mloop outputs: ('conv_2', ('conv_4_unroll=2', 'sum'))[0m 
[45mALN> [0m [91m[repeating section] adding layer conv_2_unroll=3 with input ['conv_1', 'conv_4_unroll=2'][0m 
{'b': conv_2_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_2_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_3_unroll=3 with input conv_2_unroll=3[0m 
{'b': conv_3_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_3_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_4_unroll=3 with input conv_3_unroll=3[0m 
{'b': conv_4_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_4_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91madding loop:[0m 
[45mALN> [0m [45mloop outputs: ('conv_2', ('conv_4_unroll=3', 'sum'))[0m 
[45mALN> [0m [91m[repeating section] adding layer conv_2_unroll=4 with input ['conv_1', 'conv_4_unroll=3'][0m 
{'b': conv_2_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_2_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_3_unroll=4 with input conv_2_unroll=4[0m 
{'b': conv_3_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_3_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_4_unroll=4 with input conv_3_unroll=4[0m 
{'b': conv_4_unroll=0.b, 'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': conv_4_unroll=0.W, 'num_filters': 48}
[45mALN> [0m [94m[after repeating section] adding layer conv_5 with input conv_4_unroll=4[0m 
{'nonlinearity': <function rectify at 0x7fe5365aae60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fe53408e490>, 'num_filters': 3}
[45mALN> [0m [94m[after repeating section] adding layer fc_1 with input conv_5[0m 
[45mALN> [0m [45mmarking layer fc_1 as output[0m 
[91mModel has 98809 total parameters[0m 
LoopyCNN instance with the following hyperparameters, layers and loops:[95m
HYPERPARAMETERS:[0m
	n_unrolls=5
	use_batchnorm=True[95m

ARCHITECTURE:[0m
main_stack:
	[93minput [input layer; output_dim=(3, 32, 32)][0m
	[96mconv_1 [conv2d layer; num_filters=48][0m
	[96mconv_2 [conv2d layer; num_filters=48][0m
	[96mconv_3 [conv2d layer; num_filters=48][0m
	[96mconv_4 [conv2d layer; num_filters=48][0m
	[96mconv_5 [conv2d layer; num_filters=3][0m
	[97mfc_1 [dense layer; output_dim=10][0m
loop:
	[96mconv_4 [conv2d layer; num_filters=48][0m
	[96mconv_2 [conv2d layer; num_filters=48][0m
(20000, 3, 32, 32) (20000,)
(1000, 3, 32, 32) (1000,)
*------------------------------------------------------------------------------*
Epoch 0, batch 19:
batchly_train_loss:  2.40071524649
cumulative_train_loss:  2.5270681542
*------------------------------------------------------------------------------*
Epoch 0, batch 39:
batchly_train_loss:  1.93020730628
cumulative_train_loss:  2.22098566809
*------------------------------------------------------------------------------*
Epoch 0, batch 59:
batchly_train_loss:  2.21173195246
cumulative_train_loss:  2.21784881534
*------------------------------------------------------------------------------*
Epoch 0, batch 79:
batchly_train_loss:  2.49051843666
cumulative_train_loss:  2.28687909922
*------------------------------------------------------------------------------*
Epoch 0, batch 99:
batchly_train_loss:  2.48215009452
cumulative_train_loss:  2.32632778513
*------------------------------------------------------------------------------*
Epoch 0, batch 119:
batchly_train_loss:  2.60140897641
cumulative_train_loss:  2.37255991812
*------------------------------------------------------------------------------*
Epoch 0, batch 139:
batchly_train_loss:  2.92671684037
cumulative_train_loss:  2.45229472708
*------------------------------------------------------------------------------*
Epoch 0, batch 159:
batchly_train_loss:  2.85879673976
cumulative_train_loss:  2.50342705572
*------------------------------------------------------------------------------*
Epoch 0, batch 179:
batchly_train_loss:  2.75901002949
cumulative_train_loss:  2.53198381256
*------------------------------------------------------------------------------*
Epoch 0, batch 199:
batchly_train_loss:  2.02746119196
cumulative_train_loss:  2.48127802155
*------------------------------------------------------------------------------*
Epoch 0, batch 219:
batchly_train_loss:  2.22629599367
cumulative_train_loss:  2.45799199161
*------------------------------------------------------------------------------*
Epoch 0, batch 239:
batchly_train_loss:  2.18207908986
cumulative_train_loss:  2.43490304585
*------------------------------------------------------------------------------*
Epoch 0, batch 259:
batchly_train_loss:  2.4678872827
cumulative_train_loss:  2.43745009117
*------------------------------------------------------------------------------*
Epoch 0, batch 279:
batchly_train_loss:  2.12315560563
cumulative_train_loss:  2.41492002052
*------------------------------------------------------------------------------*
Epoch 0, batch 299:
batchly_train_loss:  2.14399574767
cumulative_train_loss:  2.39679799558
*------------------------------------------------------------------------------*
Epoch 0, batch 319:
batchly_train_loss:  2.23777976511
cumulative_train_loss:  2.38682820057
*------------------------------------------------------------------------------*
Epoch 0, batch 339:
batchly_train_loss:  2.06946056964
cumulative_train_loss:  2.36810444653
*------------------------------------------------------------------------------*
Epoch 0, batch 359:
batchly_train_loss:  2.4792764216
cumulative_train_loss:  2.37429787133
*------------------------------------------------------------------------------*
Epoch 0, batch 379:
batchly_train_loss:  2.32752742694
cumulative_train_loss:  2.371829774
*------------------------------------------------------------------------------*
Epoch 0, batch 399:
batchly_train_loss:  2.14461394349
cumulative_train_loss:  2.36044050931
*------------------------------------------------------------------------------*
Epoch 0, batch 419:
batchly_train_loss:  2.15022230568
cumulative_train_loss:  2.35040622751
*------------------------------------------------------------------------------*
Epoch 0, batch 439:
batchly_train_loss:  2.04232267614
cumulative_train_loss:  2.33637053041
*------------------------------------------------------------------------------*
Epoch 0, batch 459:
batchly_train_loss:  2.11461079057
cumulative_train_loss:  2.32670779665
*------------------------------------------------------------------------------*
Epoch 0, batch 479:
batchly_train_loss:  2.07970910422
cumulative_train_loss:  2.31639469884
*------------------------------------------------------------------------------*
Epoch 0, batch 499:
batchly_train_loss:  2.15571243229
cumulative_train_loss:  2.30995452784
*------------------------------------------------------------------------------*
Epoch 0, batch 519:
batchly_train_loss:  2.12644352128
cumulative_train_loss:  2.30288281275
*------------------------------------------------------------------------------*
Epoch 0, batch 539:
batchly_train_loss:  2.12374635857
cumulative_train_loss:  2.29623582002
*------------------------------------------------------------------------------*
Epoch 0, batch 559:
batchly_train_loss:  2.0886781598
cumulative_train_loss:  2.28880978566
*------------------------------------------------------------------------------*
Epoch 0, batch 579:
batchly_train_loss:  2.29274410682
cumulative_train_loss:  2.28894568622
*------------------------------------------------------------------------------*
Epoch 0, batch 599:
batchly_train_loss:  2.23668886718
cumulative_train_loss:  2.28720088425
*------------------------------------------------------------------------------*
Epoch 0, batch 619:
batchly_train_loss:  2.03096089243
cumulative_train_loss:  2.27892172458
*------------------------------------------------------------------------------*
Epoch 0, batch 639:
batchly_train_loss:  1.69975091128
cumulative_train_loss:  2.26079431258
*------------------------------------------------------------------------------*
Epoch 0, batch 659:
batchly_train_loss:  1.9361478925
cumulative_train_loss:  2.25094161394
*------------------------------------------------------------------------------*
Epoch 0, batch 679:
batchly_train_loss:  2.16650610489
cumulative_train_loss:  2.24845455919
*------------------------------------------------------------------------------*
Epoch 0, batch 699:
batchly_train_loss:  1.97401510066
cumulative_train_loss:  2.24060221416
*------------------------------------------------------------------------------*
Epoch 0, batch 719:
batchly_train_loss:  2.133316371
cumulative_train_loss:  2.23761790698
*------------------------------------------------------------------------------*
Epoch 0, batch 739:
batchly_train_loss:  2.33602841384
cumulative_train_loss:  2.24028124952
*------------------------------------------------------------------------------*
Epoch 0, batch 759:
batchly_train_loss:  2.11622837871
cumulative_train_loss:  2.23701239917
*------------------------------------------------------------------------------*
Epoch 0, batch 779:
batchly_train_loss:  2.03772700337
cumulative_train_loss:  2.23189595769
*------------------------------------------------------------------------------*
Epoch 0, batch 799:
batchly_train_loss:  2.29050408608
cumulative_train_loss:  2.23336299469
*------------------------------------------------------------------------------*
Epoch 0, batch 819:
batchly_train_loss:  2.27643252344
cumulative_train_loss:  2.23441475364
*------------------------------------------------------------------------------*
Epoch 0, batch 839:
batchly_train_loss:  2.21399997798
cumulative_train_loss:  2.23392810821
*------------------------------------------------------------------------------*
Epoch 0, batch 859:
batchly_train_loss:  2.21575869035
cumulative_train_loss:  2.23350507171
*------------------------------------------------------------------------------*
Epoch 0, batch 879:
batchly_train_loss:  2.12599155844
cumulative_train_loss:  2.23105880292
*------------------------------------------------------------------------------*
Epoch 0, batch 899:
batchly_train_loss:  2.11300459085
cumulative_train_loss:  2.22843245782
*------------------------------------------------------------------------------*
Epoch 0, batch 919:
batchly_train_loss:  2.04809206893
cumulative_train_loss:  2.2245077486
*------------------------------------------------------------------------------*
Epoch 0, batch 939:
batchly_train_loss:  2.12882380254
cumulative_train_loss:  2.22246975188
*------------------------------------------------------------------------------*
Epoch 0, batch 959:
batchly_train_loss:  2.11292501916
cumulative_train_loss:  2.22018519019
*------------------------------------------------------------------------------*
Epoch 0, batch 979:
batchly_train_loss:  2.31660959673
cumulative_train_loss:  2.22215504528
*------------------------------------------------------------------------------*
Epoch 0, batch 999:
batchly_train_loss:  2.29072610047
cumulative_train_loss:  2.22352783918
*------------------------------------------------------------------------------*
Epoch 0, batch 1019:
batchly_train_loss:  2.17508494605
cumulative_train_loss:  2.22257704638
*------------------------------------------------------------------------------*
Epoch 0, batch 1039:
batchly_train_loss:  2.10377180512
cumulative_train_loss:  2.22029013124
*------------------------------------------------------------------------------*
Epoch 0, batch 1059:
batchly_train_loss:  2.15969580324
cumulative_train_loss:  2.21914576244
*------------------------------------------------------------------------------*
Epoch 0, batch 1079:
batchly_train_loss:  2.02349355763
cumulative_train_loss:  2.21551921555
*------------------------------------------------------------------------------*
Epoch 0, batch 1099:
batchly_train_loss:  2.08487496704
cumulative_train_loss:  2.2131417042
*------------------------------------------------------------------------------*
Epoch 0, batch 1119:
batchly_train_loss:  2.09459484037
cumulative_train_loss:  2.21102290414
*------------------------------------------------------------------------------*
Epoch 0, batch 1139:
batchly_train_loss:  2.07295017539
cumulative_train_loss:  2.20859844885
*------------------------------------------------------------------------------*
Epoch 0, batch 1159:
batchly_train_loss:  2.31222741587
cumulative_train_loss:  2.21038669677
*------------------------------------------------------------------------------*
Epoch 0, batch 1179:
batchly_train_loss:  1.92101423171
cumulative_train_loss:  2.20547791873
*------------------------------------------------------------------------------*
Epoch 0, batch 1199:
batchly_train_loss:  2.0794909621
cumulative_train_loss:  2.20337638484
*------------------------------------------------------------------------------*
Epoch 0, batch 1219:
batchly_train_loss:  1.95938607865
cumulative_train_loss:  2.19937326251
*------------------------------------------------------------------------------*
Epoch 0, batch 1239:
batchly_train_loss:  2.03364276216
cumulative_train_loss:  2.19669803248
*------------------------------------------------------------------------------*
Epoch 0, batch 1259:
batchly_train_loss:  2.11226356953
cumulative_train_loss:  2.19535673839
*------------------------------------------------------------------------------*
Epoch 0, batch 1279:
batchly_train_loss:  2.12943645564
cumulative_train_loss:  2.19432592865
*------------------------------------------------------------------------------*
Epoch 0, batch 1299:
batchly_train_loss:  2.07245570423
cumulative_train_loss:  2.19244955876
*------------------------------------------------------------------------------*
Epoch 0, batch 1319:
batchly_train_loss:  2.22798877492
cumulative_train_loss:  2.19298843998
*------------------------------------------------------------------------------*
Epoch 0, batch 1339:
batchly_train_loss:  2.00906440766
cumulative_train_loss:  2.19024125503
*------------------------------------------------------------------------------*
Epoch 0, batch 1359:
batchly_train_loss:  1.50157269973
cumulative_train_loss:  2.18010632412
*------------------------------------------------------------------------------*
Epoch 0, batch 1379:
batchly_train_loss:  2.04804286789
cumulative_train_loss:  2.17819097305
*------------------------------------------------------------------------------*
Epoch 0, batch 1399:
batchly_train_loss:  1.72551795024
cumulative_train_loss:  2.17171959317
*------------------------------------------------------------------------------*
Epoch 0, batch 1419:
batchly_train_loss:  1.85991789591
cumulative_train_loss:  2.16732492513
*------------------------------------------------------------------------------*
Epoch 0, batch 1439:
batchly_train_loss:  2.10902783617
cumulative_train_loss:  2.16651468067
*------------------------------------------------------------------------------*
Epoch 0, batch 1459:
batchly_train_loss:  2.12499892244
cumulative_train_loss:  2.16594558186
*------------------------------------------------------------------------------*
Epoch 0, batch 1479:
batchly_train_loss:  2.06688723962
cumulative_train_loss:  2.16460605052
*------------------------------------------------------------------------------*
Epoch 0, batch 1499:
batchly_train_loss:  2.1742082348
cumulative_train_loss:  2.16473416506
*------------------------------------------------------------------------------*
Epoch 0, batch 1519:
batchly_train_loss:  2.0100569053
cumulative_train_loss:  2.16269759811
*------------------------------------------------------------------------------*
Epoch 0, batch 1539:
batchly_train_loss:  2.0462374141
cumulative_train_loss:  2.16118414542
*------------------------------------------------------------------------------*
Epoch 0, batch 1559:
batchly_train_loss:  2.05969681451
cumulative_train_loss:  2.15988219121
*------------------------------------------------------------------------------*
Epoch 0, batch 1579:
batchly_train_loss:  2.00873559967
cumulative_train_loss:  2.15796773153
*------------------------------------------------------------------------------*
Epoch 0, batch 1599:
batchly_train_loss:  2.09597211795
cumulative_train_loss:  2.15719230172
*------------------------------------------------------------------------------*
Epoch 0, batch 1619:
batchly_train_loss:  1.86565561237
cumulative_train_loss:  2.15359086022
*------------------------------------------------------------------------------*
Epoch 0, batch 1639:
batchly_train_loss:  1.80280470569
cumulative_train_loss:  2.14931037023
*------------------------------------------------------------------------------*
Epoch 0, batch 1659:
batchly_train_loss:  1.88369848888
cumulative_train_loss:  2.14610829813
*------------------------------------------------------------------------------*
Epoch 0, batch 1679:
batchly_train_loss:  2.01351252904
cumulative_train_loss:  2.14452883691
*------------------------------------------------------------------------------*
Epoch 0, batch 1699:
batchly_train_loss:  2.13075239091
cumulative_train_loss:  2.14436666568
*------------------------------------------------------------------------------*
Epoch 0, batch 1719:
batchly_train_loss:  1.8801440495
cumulative_train_loss:  2.14129252238
*------------------------------------------------------------------------------*
Epoch 0, batch 1739:
batchly_train_loss:  1.6196282164
cumulative_train_loss:  2.1352929329
*------------------------------------------------------------------------------*
Epoch 0, batch 1759:
batchly_train_loss:  1.90045109175
cumulative_train_loss:  2.13262275847
*------------------------------------------------------------------------------*
Epoch 0, batch 1779:
batchly_train_loss:  2.15105027689
cumulative_train_loss:  2.13282992562
*------------------------------------------------------------------------------*
Epoch 0, batch 1799:
batchly_train_loss:  1.77008141981
cumulative_train_loss:  2.12879714623
*------------------------------------------------------------------------------*
Epoch 0, batch 1819:
batchly_train_loss:  2.17472552326
cumulative_train_loss:  2.12930213114
*------------------------------------------------------------------------------*
Epoch 0, batch 1839:
batchly_train_loss:  2.2004654169
cumulative_train_loss:  2.13007606573
*------------------------------------------------------------------------------*
Epoch 0, batch 1859:
batchly_train_loss:  2.00656861823
cumulative_train_loss:  2.12874731428
*------------------------------------------------------------------------------*
Epoch 0, batch 1879:
batchly_train_loss:  1.83888767439
cumulative_train_loss:  2.12566206
*------------------------------------------------------------------------------*
Epoch 0, batch 1899:
batchly_train_loss:  1.85900684951
cumulative_train_loss:  2.12285368495
*------------------------------------------------------------------------------*
Epoch 0, batch 1919:
batchly_train_loss:  1.93596076443
cumulative_train_loss:  2.12090586921
*------------------------------------------------------------------------------*
Epoch 0, batch 1939:
batchly_train_loss:  2.04178408572
cumulative_train_loss:  2.12008976004
*------------------------------------------------------------------------------*
Epoch 0, batch 1959:
batchly_train_loss:  1.60564976875
cumulative_train_loss:  2.11483769275
*------------------------------------------------------------------------------*
Epoch 0, batch 1979:
batchly_train_loss:  2.0476542677
cumulative_train_loss:  2.11415872939
*------------------------------------------------------------------------------*
Epoch 0, batch 1999:
batchly_train_loss:  2.1748111677
cumulative_train_loss:  2.11476555718
*------------------------------------------------------------------------------*
Epoch 0, batch 2019:
batchly_train_loss:  2.30594374983
cumulative_train_loss:  2.1166593481
*------------------------------------------------------------------------------*
Epoch 0, batch 2039:
batchly_train_loss:  2.08686481177
cumulative_train_loss:  2.11636710154
*------------------------------------------------------------------------------*
Epoch 0, batch 2059:
batchly_train_loss:  2.07713047047
cumulative_train_loss:  2.11598597836
*------------------------------------------------------------------------------*
Epoch 0, batch 2079:
batchly_train_loss:  1.90889046311
cumulative_train_loss:  2.11399371751
*------------------------------------------------------------------------------*
Epoch 0, batch 2099:
batchly_train_loss:  2.23532393233
cumulative_train_loss:  2.11514979388
*------------------------------------------------------------------------------*
Epoch 0, batch 2119:
batchly_train_loss:  2.05527183247
cumulative_train_loss:  2.11458464087
*------------------------------------------------------------------------------*
Epoch 0, batch 2139:
batchly_train_loss:  1.99384324811
cumulative_train_loss:  2.11345568909
*------------------------------------------------------------------------------*
Epoch 0, batch 2159:
batchly_train_loss:  1.95388756737
cumulative_train_loss:  2.11197752215
*------------------------------------------------------------------------------*
Epoch 0, batch 2179:
batchly_train_loss:  2.06309859336
cumulative_train_loss:  2.11152888581
*------------------------------------------------------------------------------*
Epoch 0, batch 2199:
batchly_train_loss:  1.85377404304
cumulative_train_loss:  2.10918459438
*------------------------------------------------------------------------------*
Epoch 0, batch 2219:
batchly_train_loss:  1.96103570164
cumulative_train_loss:  2.1078493182
*------------------------------------------------------------------------------*
Epoch 0, batch 2239:
batchly_train_loss:  1.99054923104
cumulative_train_loss:  2.10680152823
*------------------------------------------------------------------------------*
Epoch 0, batch 2259:
batchly_train_loss:  1.95352972221
cumulative_train_loss:  2.10544454013
*------------------------------------------------------------------------------*
Epoch 0, batch 2279:
batchly_train_loss:  1.6445122428
cumulative_train_loss:  2.10139950022
*------------------------------------------------------------------------------*
Epoch 0, batch 2299:
batchly_train_loss:  2.01768012222
cumulative_train_loss:  2.10067118897
*------------------------------------------------------------------------------*
Epoch 0, batch 2319:
batchly_train_loss:  1.93657425639
cumulative_train_loss:  2.09925595023
*------------------------------------------------------------------------------*
Epoch 0, batch 2339:
batchly_train_loss:  2.07947462261
cumulative_train_loss:  2.09908680677
*------------------------------------------------------------------------------*
Epoch 0, batch 2359:
batchly_train_loss:  2.04560139585
cumulative_train_loss:  2.09863334843
*------------------------------------------------------------------------------*
Epoch 0, batch 2379:
batchly_train_loss:  2.24814438222
cumulative_train_loss:  2.09989027179
*------------------------------------------------------------------------------*
Epoch 0, batch 2399:
batchly_train_loss:  1.96264903015
cumulative_train_loss:  2.09874611804
*------------------------------------------------------------------------------*
Epoch 0, batch 2419:
batchly_train_loss:  1.84200011608
cumulative_train_loss:  2.09662337309
*------------------------------------------------------------------------------*
Epoch 0, batch 2439:
batchly_train_loss:  2.39891341806
cumulative_train_loss:  2.09910217625
*------------------------------------------------------------------------------*
Epoch 0, batch 2459:
batchly_train_loss:  2.00592732395
cumulative_train_loss:  2.09834434906
*------------------------------------------------------------------------------*
Epoch 0, batch 2479:
batchly_train_loss:  1.97887100343
cumulative_train_loss:  2.09738046568
*------------------------------------------------------------------------------*
Epoch 0, batch 2499:
batchly_train_loss:  1.73224921386
cumulative_train_loss:  2.09445824678
*------------------------------------------------------------------------------*
Epoch 0, batch 2519:
batchly_train_loss:  2.04134608202
cumulative_train_loss:  2.09403655432
*------------------------------------------------------------------------------*
Epoch 0, batch 2539:
batchly_train_loss:  1.80658468339
cumulative_train_loss:  2.09177226231
*------------------------------------------------------------------------------*
Epoch 0, batch 2559:
batchly_train_loss:  2.11870891494
cumulative_train_loss:  2.09198278714
*------------------------------------------------------------------------------*
Epoch 0, batch 2579:
batchly_train_loss:  2.23621766201
cumulative_train_loss:  2.09310132049
*------------------------------------------------------------------------------*
Epoch 0, batch 2599:
batchly_train_loss:  1.84109082454
cumulative_train_loss:  2.09116203233
*------------------------------------------------------------------------------*
Epoch 0, batch 2619:
batchly_train_loss:  1.9864677065
cumulative_train_loss:  2.09036253385
*------------------------------------------------------------------------------*
Epoch 0, batch 2639:
batchly_train_loss:  2.28215436166
cumulative_train_loss:  2.09181605282
*------------------------------------------------------------------------------*
Epoch 0, batch 2659:
batchly_train_loss:  2.12871769902
cumulative_train_loss:  2.09209361315
*------------------------------------------------------------------------------*
Epoch 0, batch 2679:
batchly_train_loss:  2.18106968621
cumulative_train_loss:  2.09275786155
*------------------------------------------------------------------------------*
Epoch 0, batch 2699:
batchly_train_loss:  1.74981909376
cumulative_train_loss:  2.09021663319
*------------------------------------------------------------------------------*
Epoch 0, batch 2719:
batchly_train_loss:  2.27835080984
cumulative_train_loss:  2.09160048149
*------------------------------------------------------------------------------*
Epoch 0, batch 2739:
batchly_train_loss:  2.07638644286
cumulative_train_loss:  2.09148938957
*------------------------------------------------------------------------------*
Epoch 0, batch 2759:
batchly_train_loss:  2.05923431429
cumulative_train_loss:  2.09125557242
*------------------------------------------------------------------------------*
Epoch 0, batch 2779:
batchly_train_loss:  2.07083112838
cumulative_train_loss:  2.0911085811
*------------------------------------------------------------------------------*
Epoch 0, batch 2799:
batchly_train_loss:  1.96812682235
cumulative_train_loss:  2.09022982613
*------------------------------------------------------------------------------*
Epoch 0, batch 2819:
batchly_train_loss:  2.07984862281
cumulative_train_loss:  2.09015617445
*------------------------------------------------------------------------------*
Epoch 0, batch 2839:
batchly_train_loss:  1.90767396917
cumulative_train_loss:  2.08887063585
*------------------------------------------------------------------------------*
Epoch 0, batch 2859:
batchly_train_loss:  1.92734005065
cumulative_train_loss:  2.08774065624
*------------------------------------------------------------------------------*
Epoch 0, batch 2879:
batchly_train_loss:  1.84254109407
cumulative_train_loss:  2.08603729005
*------------------------------------------------------------------------------*
Epoch 0, batch 2899:
batchly_train_loss:  2.02091176094
cumulative_train_loss:  2.08558799354
*------------------------------------------------------------------------------*
Epoch 0, batch 2919:
batchly_train_loss:  1.95787369858
cumulative_train_loss:  2.08471293842
*------------------------------------------------------------------------------*
Epoch 0, batch 2939:
batchly_train_loss:  1.89273963706
cumulative_train_loss:  2.08340655325
*------------------------------------------------------------------------------*
Epoch 0, batch 2959:
batchly_train_loss:  1.85944886338
cumulative_train_loss:  2.08189281422
*------------------------------------------------------------------------------*
Epoch 0, batch 2979:
batchly_train_loss:  1.4081964794
cumulative_train_loss:  2.07736984453
*------------------------------------------------------------------------------*
Epoch 0, batch 2999:
batchly_train_loss:  1.76408389185
cumulative_train_loss:  2.07528057509
*------------------------------------------------------------------------------*
Epoch 0, batch 3019:
batchly_train_loss:  2.03247289597
cumulative_train_loss:  2.07499698662
*------------------------------------------------------------------------------*
Epoch 0, batch 3039:
batchly_train_loss:  1.97053777331
cumulative_train_loss:  2.07430952882
*------------------------------------------------------------------------------*
Epoch 0, batch 3059:
batchly_train_loss:  2.11699224236
cumulative_train_loss:  2.074588592
*------------------------------------------------------------------------------*
Epoch 0, batch 3079:
batchly_train_loss:  1.98153887711
cumulative_train_loss:  2.07398417683
*------------------------------------------------------------------------------*
Epoch 0, batch 3099:
batchly_train_loss:  1.87219278063
cumulative_train_loss:  2.07268187676
*------------------------------------------------------------------------------*
Epoch 0, batch 3119:
batchly_train_loss:  1.92642757711
cumulative_train_loss:  2.07174404861
*------------------------------------------------------------------------------*
Epoch 0, batch 3139:
batchly_train_loss:  2.01115252716
cumulative_train_loss:  2.07135799241
*------------------------------------------------------------------------------*
Epoch 0, batch 3159:
batchly_train_loss:  1.51746063426
cumulative_train_loss:  2.06785120318
*------------------------------------------------------------------------------*
Epoch 0, batch 3179:
batchly_train_loss:  1.81427607858
cumulative_train_loss:  2.06625588941
*------------------------------------------------------------------------------*
Epoch 0, batch 3199:
batchly_train_loss:  1.79990213585
cumulative_train_loss:  2.06459065806
*------------------------------------------------------------------------------*
Epoch 0, batch 3219:
batchly_train_loss:  1.77634138657
cumulative_train_loss:  2.06279973373
*------------------------------------------------------------------------------*
Epoch 0, batch 3239:
batchly_train_loss:  1.89528232334
cumulative_train_loss:  2.06176535639
*------------------------------------------------------------------------------*
Epoch 0, batch 3259:
batchly_train_loss:  2.01237898843
cumulative_train_loss:  2.06146227957
*------------------------------------------------------------------------------*
Epoch 0, batch 3279:
batchly_train_loss:  2.1743441461
cumulative_train_loss:  2.06215079354
*------------------------------------------------------------------------------*
Epoch 0, batch 3299:
batchly_train_loss:  2.0640659163
cumulative_train_loss:  2.06216240387
*------------------------------------------------------------------------------*
Epoch 0, batch 3319:
batchly_train_loss:  1.96333755843
cumulative_train_loss:  2.0615668941
*------------------------------------------------------------------------------*
Epoch 0, batch 3339:
batchly_train_loss:  1.88992700734
cumulative_train_loss:  2.06053880254
*------------------------------------------------------------------------------*
Epoch 0, batch 3359:
batchly_train_loss:  1.89115691779
cumulative_train_loss:  2.05953027687
*------------------------------------------------------------------------------*
Epoch 0, batch 3379:
batchly_train_loss:  2.28858345372
cumulative_train_loss:  2.06088602222
*------------------------------------------------------------------------------*
Epoch 0, batch 3399:
batchly_train_loss:  2.12882811392
cumulative_train_loss:  2.06128579917
*------------------------------------------------------------------------------*
Epoch 0, batch 3419:
batchly_train_loss:  1.87146767762
cumulative_train_loss:  2.060175427
*------------------------------------------------------------------------------*
Epoch 0, batch 3439:
batchly_train_loss:  1.71250749651
cumulative_train_loss:  2.05815351406
*------------------------------------------------------------------------------*
Epoch 0, batch 3459:
batchly_train_loss:  1.79515841861
cumulative_train_loss:  2.05663287171
*------------------------------------------------------------------------------*
Epoch 0, batch 3479:
batchly_train_loss:  1.81136223119
cumulative_train_loss:  2.05522286515
*------------------------------------------------------------------------------*
Epoch 0, batch 3499:
batchly_train_loss:  2.4094806854
cumulative_train_loss:  2.0572477741
*------------------------------------------------------------------------------*
Epoch 0, batch 3519:
batchly_train_loss:  1.96988896832
cumulative_train_loss:  2.05675127619
*------------------------------------------------------------------------------*
Epoch 0, batch 3539:
batchly_train_loss:  1.70402469546
cumulative_train_loss:  2.05475790756
*------------------------------------------------------------------------------*
Epoch 0, batch 3559:
batchly_train_loss:  1.59935252427
cumulative_train_loss:  2.05219873148
*------------------------------------------------------------------------------*
Epoch 0, batch 3579:
batchly_train_loss:  1.89305914631
cumulative_train_loss:  2.05130943511
*------------------------------------------------------------------------------*
Epoch 0, batch 3599:
batchly_train_loss:  1.67845203355
cumulative_train_loss:  2.04923742954
*------------------------------------------------------------------------------*
Epoch 0, batch 3619:
batchly_train_loss:  1.60488577692
cumulative_train_loss:  2.04678176968
*------------------------------------------------------------------------------*
Epoch 0, batch 3639:
batchly_train_loss:  2.00790939374
cumulative_train_loss:  2.0465681265
*------------------------------------------------------------------------------*
Epoch 0, batch 3659:
batchly_train_loss:  2.15775140792
cumulative_train_loss:  2.04717585146
*------------------------------------------------------------------------------*
Epoch 0, batch 3679:
batchly_train_loss:  1.94096693374
cumulative_train_loss:  2.04659847218
*------------------------------------------------------------------------------*
Epoch 0, batch 3699:
batchly_train_loss:  1.76254289006
cumulative_train_loss:  2.04506262151
*------------------------------------------------------------------------------*
Epoch 0, batch 3719:
batchly_train_loss:  1.93939547259
cumulative_train_loss:  2.0444943658
*------------------------------------------------------------------------------*
Epoch 0, batch 3739:
batchly_train_loss:  2.09080442653
cumulative_train_loss:  2.04474207942
*------------------------------------------------------------------------------*
Epoch 0, batch 3759:
batchly_train_loss:  1.81328689551
cumulative_train_loss:  2.04351060731
*------------------------------------------------------------------------------*
Epoch 0, batch 3779:
batchly_train_loss:  2.18297323715
cumulative_train_loss:  2.04424870008
*------------------------------------------------------------------------------*
Epoch 0, batch 3799:
batchly_train_loss:  2.22556566511
cumulative_train_loss:  2.04520325109
*------------------------------------------------------------------------------*
Epoch 0, batch 3819:
batchly_train_loss:  2.04528877522
cumulative_train_loss:  2.04520369898
*------------------------------------------------------------------------------*
Epoch 0, batch 3839:
batchly_train_loss:  1.9636501847
cumulative_train_loss:  2.04477883045
*------------------------------------------------------------------------------*
Epoch 0, batch 3859:
batchly_train_loss:  2.00713684269
cumulative_train_loss:  2.0445837437
*------------------------------------------------------------------------------*
Epoch 0, batch 3879:
batchly_train_loss:  1.89189690309
cumulative_train_loss:  2.04379649524
*------------------------------------------------------------------------------*
Epoch 0, batch 3899:
batchly_train_loss:  1.80472162578
cumulative_train_loss:  2.04257015582
*------------------------------------------------------------------------------*
Epoch 0, batch 3919:
batchly_train_loss:  1.96810160109
cumulative_train_loss:  2.04219011726
*------------------------------------------------------------------------------*
Epoch 0, batch 3939:
batchly_train_loss:  2.02822088809
cumulative_train_loss:  2.04211918947
*------------------------------------------------------------------------------*
Epoch 0, batch 3959:
batchly_train_loss:  1.58702566827
cumulative_train_loss:  2.03982015678
*------------------------------------------------------------------------------*
Epoch 0, batch 3979:
batchly_train_loss:  1.67233138939
cumulative_train_loss:  2.03797301545
*------------------------------------------------------------------------------*
Epoch 0, batch 3999:
batchly_train_loss:  1.30427280344
cumulative_train_loss:  2.03430359703
================================================================================
Epoch 0 of 8 took 6515.358s
  training loss:		2.033824
evaluating model...
VALID_LOSS:  2.55788551448
VALID_ACC:  0.322727272727
FULL_TRAIN_LOSS:  2.13757292283
FULL_TRAIN_ACC:  0.349154228856
*------------------------------------------------------------------------------*
Epoch 1, batch 19:
batchly_train_loss:  1.9390579198
cumulative_train_loss:  1.92805544821
*------------------------------------------------------------------------------*
Epoch 1, batch 39:
batchly_train_loss:  1.70306229709
cumulative_train_loss:  1.81267434507
*------------------------------------------------------------------------------*
Epoch 1, batch 59:
batchly_train_loss:  2.25327105149
cumulative_train_loss:  1.96202916081
*------------------------------------------------------------------------------*
Epoch 1, batch 79:
batchly_train_loss:  1.85136402762
cumulative_train_loss:  1.93401267139
*------------------------------------------------------------------------------*
Epoch 1, batch 99:
batchly_train_loss:  1.78162286137
cumulative_train_loss:  1.90322685119
*------------------------------------------------------------------------------*
Epoch 1, batch 119:
batchly_train_loss:  1.81933135216
cumulative_train_loss:  1.88912676732
*------------------------------------------------------------------------------*
Epoch 1, batch 139:
batchly_train_loss:  2.05781721787
cumulative_train_loss:  1.91339877459
*------------------------------------------------------------------------------*
Epoch 1, batch 159:
batchly_train_loss:  1.85593070113
cumulative_train_loss:  1.90617008611
*------------------------------------------------------------------------------*
Epoch 1, batch 179:
batchly_train_loss:  2.15555345098
cumulative_train_loss:  1.93403414922
*------------------------------------------------------------------------------*
Epoch 1, batch 199:
batchly_train_loss:  1.87223474225
cumulative_train_loss:  1.92782315354
*------------------------------------------------------------------------------*
Epoch 1, batch 219:
batchly_train_loss:  1.80409741835
cumulative_train_loss:  1.91652399965
*------------------------------------------------------------------------------*
Epoch 1, batch 239:
batchly_train_loss:  2.12803756193
cumulative_train_loss:  1.93422387933
*------------------------------------------------------------------------------*
Epoch 1, batch 259:
batchly_train_loss:  1.72006701062
cumulative_train_loss:  1.9176866694
*------------------------------------------------------------------------------*
Epoch 1, batch 279:
batchly_train_loss:  1.89031087095
cumulative_train_loss:  1.91572424657
*------------------------------------------------------------------------------*
Epoch 1, batch 299:
batchly_train_loss:  1.4901782299
cumulative_train_loss:  1.88725963007
*------------------------------------------------------------------------------*
Epoch 1, batch 319:
batchly_train_loss:  1.70388944058
cumulative_train_loss:  1.87576306646
*------------------------------------------------------------------------------*
Epoch 1, batch 339:
batchly_train_loss:  2.22012690927
cumulative_train_loss:  1.89607951737
*------------------------------------------------------------------------------*
Epoch 1, batch 359:
batchly_train_loss:  1.78638933625
cumulative_train_loss:  1.88996864377
*------------------------------------------------------------------------------*
Epoch 1, batch 379:
batchly_train_loss:  1.7504873004
cumulative_train_loss:  1.88260815071
*------------------------------------------------------------------------------*
Epoch 1, batch 399:
batchly_train_loss:  1.94070031774
cumulative_train_loss:  1.88552003879
*------------------------------------------------------------------------------*
Epoch 1, batch 419:
batchly_train_loss:  1.7133726281
cumulative_train_loss:  1.87730297861
*------------------------------------------------------------------------------*
Epoch 1, batch 439:
batchly_train_loss:  1.73618971991
cumulative_train_loss:  1.87087412855
*------------------------------------------------------------------------------*
Epoch 1, batch 459:
batchly_train_loss:  1.67625385252
cumulative_train_loss:  1.86239394224
*------------------------------------------------------------------------------*
Epoch 1, batch 479:
batchly_train_loss:  1.67920349906
cumulative_train_loss:  1.85474507196
*------------------------------------------------------------------------------*
Epoch 1, batch 499:
batchly_train_loss:  2.1591100906
cumulative_train_loss:  1.8669440707
*------------------------------------------------------------------------------*
Epoch 1, batch 519:
batchly_train_loss:  1.66675431536
cumulative_train_loss:  1.85922962926
*------------------------------------------------------------------------------*
Epoch 1, batch 539:
batchly_train_loss:  1.77169831601
cumulative_train_loss:  1.85598171411
*------------------------------------------------------------------------------*
Epoch 1, batch 559:
batchly_train_loss:  1.91719653038
cumulative_train_loss:  1.85817186854
*------------------------------------------------------------------------------*
Epoch 1, batch 579:
batchly_train_loss:  1.86502541324
cumulative_train_loss:  1.85840860584
*------------------------------------------------------------------------------*
Epoch 1, batch 599:
batchly_train_loss:  1.51296344985
cumulative_train_loss:  1.84687454387
*------------------------------------------------------------------------------*
Epoch 1, batch 619:
batchly_train_loss:  1.52684722544
cumulative_train_loss:  1.83653440434
*------------------------------------------------------------------------------*
Epoch 1, batch 639:
batchly_train_loss:  1.780539853
cumulative_train_loss:  1.83478183622
*------------------------------------------------------------------------------*
Epoch 1, batch 659:
batchly_train_loss:  1.80475845634
cumulative_train_loss:  1.83387065625
*------------------------------------------------------------------------------*
Epoch 1, batch 679:
batchly_train_loss:  2.15744578549
cumulative_train_loss:  1.8434015879
*------------------------------------------------------------------------------*
Epoch 1, batch 699:
batchly_train_loss:  1.948942511
cumulative_train_loss:  1.8464213568
*------------------------------------------------------------------------------*
Epoch 1, batch 719:
batchly_train_loss:  1.93504378565
cumulative_train_loss:  1.84888651476
*------------------------------------------------------------------------------*
Epoch 1, batch 739:
batchly_train_loss:  1.69573305454
cumulative_train_loss:  1.84474163086
*------------------------------------------------------------------------------*
Epoch 1, batch 759:
batchly_train_loss:  2.05810888749
cumulative_train_loss:  1.85036395646
*------------------------------------------------------------------------------*
Epoch 1, batch 779:
batchly_train_loss:  1.82864859006
cumulative_train_loss:  1.84980643743
*------------------------------------------------------------------------------*
Epoch 1, batch 799:
batchly_train_loss:  1.75781025172
cumulative_train_loss:  1.84750365431
*------------------------------------------------------------------------------*
Epoch 1, batch 819:
batchly_train_loss:  1.8294802063
cumulative_train_loss:  1.84706352127
*------------------------------------------------------------------------------*
Epoch 1, batch 839:
batchly_train_loss:  1.9508470316
cumulative_train_loss:  1.84953750244
*------------------------------------------------------------------------------*
Epoch 1, batch 859:
batchly_train_loss:  1.8006014927
cumulative_train_loss:  1.84839813085
*------------------------------------------------------------------------------*
Epoch 1, batch 879:
batchly_train_loss:  1.63438586453
cumulative_train_loss:  1.84352868224
*------------------------------------------------------------------------------*
Epoch 1, batch 899:
batchly_train_loss:  1.44877295482
cumulative_train_loss:  1.83474657485
*------------------------------------------------------------------------------*
Epoch 1, batch 919:
batchly_train_loss:  1.70503755298
cumulative_train_loss:  1.83192374521
*------------------------------------------------------------------------------*
Epoch 1, batch 939:
batchly_train_loss:  1.56047415577
cumulative_train_loss:  1.82614207131
*------------------------------------------------------------------------------*
Epoch 1, batch 959:
batchly_train_loss:  1.79908379761
cumulative_train_loss:  1.82557776946
*------------------------------------------------------------------------------*
Epoch 1, batch 979:
batchly_train_loss:  1.92538806785
cumulative_train_loss:  1.82761679497
*------------------------------------------------------------------------------*
Epoch 1, batch 999:
batchly_train_loss:  1.93156185979
cumulative_train_loss:  1.82969777725
*------------------------------------------------------------------------------*
Epoch 1, batch 1019:
batchly_train_loss:  1.83665745687
cumulative_train_loss:  1.82983437547
*------------------------------------------------------------------------------*
Epoch 1, batch 1039:
batchly_train_loss:  1.58509969406
cumulative_train_loss:  1.82512340952
*------------------------------------------------------------------------------*
Epoch 1, batch 1059:
batchly_train_loss:  1.76739448135
cumulative_train_loss:  1.82403315592
*------------------------------------------------------------------------------*
Epoch 1, batch 1079:
batchly_train_loss:  1.84113217871
cumulative_train_loss:  1.82435009795
*------------------------------------------------------------------------------*
Epoch 1, batch 1099:
batchly_train_loss:  1.8967792151
cumulative_train_loss:  1.82566818925
*------------------------------------------------------------------------------*
Epoch 1, batch 1119:
batchly_train_loss:  1.72867506646
cumulative_train_loss:  1.82393462138
*------------------------------------------------------------------------------*
Epoch 1, batch 1139:
batchly_train_loss:  2.17041199398
cumulative_train_loss:  1.83001850852
*------------------------------------------------------------------------------*
Epoch 1, batch 1159:
batchly_train_loss:  1.88155401855
cumulative_train_loss:  1.83090781844
*------------------------------------------------------------------------------*
Epoch 1, batch 1179:
batchly_train_loss:  1.93465627205
cumulative_train_loss:  1.83266775828
*------------------------------------------------------------------------------*
Epoch 1, batch 1199:
batchly_train_loss:  2.02302134486
cumulative_train_loss:  1.83584296406
*------------------------------------------------------------------------------*
Epoch 1, batch 1219:
batchly_train_loss:  1.81548505915
cumulative_train_loss:  1.83550895414
*------------------------------------------------------------------------------*
Epoch 1, batch 1239:
batchly_train_loss:  1.6699040768
cumulative_train_loss:  1.83283575192
*------------------------------------------------------------------------------*
Epoch 1, batch 1259:
batchly_train_loss:  1.87710302053
cumulative_train_loss:  1.83353896508
*------------------------------------------------------------------------------*
Epoch 1, batch 1279:
batchly_train_loss:  2.23316074868
cumulative_train_loss:  1.83978793746
*------------------------------------------------------------------------------*
Epoch 1, batch 1299:
batchly_train_loss:  1.97926580022
cumulative_train_loss:  1.84193540263
*------------------------------------------------------------------------------*
Epoch 1, batch 1319:
batchly_train_loss:  1.70451842636
cumulative_train_loss:  1.83985174871
*------------------------------------------------------------------------------*
Epoch 1, batch 1339:
batchly_train_loss:  1.71388872692
cumulative_train_loss:  1.83797029954
*------------------------------------------------------------------------------*
Epoch 1, batch 1359:
batchly_train_loss:  1.53811859308
cumulative_train_loss:  1.83355747089
*------------------------------------------------------------------------------*
Epoch 1, batch 1379:
batchly_train_loss:  2.43949023639
cumulative_train_loss:  1.84234547329
*------------------------------------------------------------------------------*
Epoch 1, batch 1399:
batchly_train_loss:  2.16152842123
cumulative_train_loss:  1.84690848899
*------------------------------------------------------------------------------*
Epoch 1, batch 1419:
batchly_train_loss:  2.00353484076
cumulative_train_loss:  1.84911604856
*------------------------------------------------------------------------------*
Epoch 1, batch 1439:
batchly_train_loss:  1.66330370885
cumulative_train_loss:  1.84653352821
*------------------------------------------------------------------------------*
Epoch 1, batch 1459:
batchly_train_loss:  1.72993660308
cumulative_train_loss:  1.84493521532
*------------------------------------------------------------------------------*
Epoch 1, batch 1479:
batchly_train_loss:  1.57943275888
cumulative_train_loss:  1.84134491841
*------------------------------------------------------------------------------*
Epoch 1, batch 1499:
batchly_train_loss:  1.81901597279
cumulative_train_loss:  1.84104700052
*------------------------------------------------------------------------------*
Epoch 1, batch 1519:
batchly_train_loss:  1.95120441036
cumulative_train_loss:  1.84249739433
*------------------------------------------------------------------------------*
Epoch 1, batch 1539:
batchly_train_loss:  1.85599602564
cumulative_train_loss:  1.84267281514
*------------------------------------------------------------------------------*
Epoch 1, batch 1559:
batchly_train_loss:  1.91183414681
cumulative_train_loss:  1.84356006763
*------------------------------------------------------------------------------*
Epoch 1, batch 1579:
batchly_train_loss:  1.88016359487
cumulative_train_loss:  1.84402369686
*------------------------------------------------------------------------------*
Epoch 1, batch 1599:
batchly_train_loss:  1.75020241122
cumulative_train_loss:  1.84285019735
*------------------------------------------------------------------------------*
Epoch 1, batch 1619:
batchly_train_loss:  2.10277899006
cumulative_train_loss:  1.84606117688
*------------------------------------------------------------------------------*
Epoch 1, batch 1639:
batchly_train_loss:  1.57425997717
cumulative_train_loss:  1.84274450574
*------------------------------------------------------------------------------*
Epoch 1, batch 1659:
batchly_train_loss:  1.48411775095
cumulative_train_loss:  1.838421097
*------------------------------------------------------------------------------*
Epoch 1, batch 1679:
batchly_train_loss:  1.72854490602
cumulative_train_loss:  1.83711226804
*------------------------------------------------------------------------------*
Epoch 1, batch 1699:
batchly_train_loss:  2.05958072073
cumulative_train_loss:  1.83973108444
*------------------------------------------------------------------------------*
Epoch 1, batch 1719:
batchly_train_loss:  1.92530243395
cumulative_train_loss:  1.84072667896
*------------------------------------------------------------------------------*
Epoch 1, batch 1739:
batchly_train_loss:  2.10124539583
cumulative_train_loss:  1.84372286892
*------------------------------------------------------------------------------*
Epoch 1, batch 1759:
batchly_train_loss:  1.5765347725
cumulative_train_loss:  1.84068491444
*------------------------------------------------------------------------------*
Epoch 1, batch 1779:
batchly_train_loss:  2.10277847878
cumulative_train_loss:  1.8436314413
*------------------------------------------------------------------------------*
Epoch 1, batch 1799:
batchly_train_loss:  1.71420385773
cumulative_train_loss:  1.84219255766
*------------------------------------------------------------------------------*
Epoch 1, batch 1819:
batchly_train_loss:  1.70805602807
cumulative_train_loss:  1.84071771951
*------------------------------------------------------------------------------*
Epoch 1, batch 1839:
batchly_train_loss:  1.77085374178
cumulative_train_loss:  1.83995791552
*------------------------------------------------------------------------------*
Epoch 1, batch 1859:
batchly_train_loss:  1.60125231524
cumulative_train_loss:  1.83738980793
*------------------------------------------------------------------------------*
Epoch 1, batch 1879:
batchly_train_loss:  2.06293435514
cumulative_train_loss:  1.83979049497
*------------------------------------------------------------------------------*
Epoch 1, batch 1899:
batchly_train_loss:  1.62646756065
cumulative_train_loss:  1.83754380793
*------------------------------------------------------------------------------*
Epoch 1, batch 1919:
batchly_train_loss:  1.78163814294
cumulative_train_loss:  1.83696115378
*------------------------------------------------------------------------------*
Epoch 1, batch 1939:
batchly_train_loss:  1.66756005
cumulative_train_loss:  1.83521384998
*------------------------------------------------------------------------------*
Epoch 1, batch 1959:
batchly_train_loss:  1.64593235821
cumulative_train_loss:  1.83328142025
*------------------------------------------------------------------------------*
Epoch 1, batch 1979:
batchly_train_loss:  1.8558383496
cumulative_train_loss:  1.83350938316
*------------------------------------------------------------------------------*
Epoch 1, batch 1999:
batchly_train_loss:  1.87730240322
cumulative_train_loss:  1.83394753243
*------------------------------------------------------------------------------*
Epoch 1, batch 2019:
batchly_train_loss:  1.77803898749
cumulative_train_loss:  1.83339370831
*------------------------------------------------------------------------------*
Epoch 1, batch 2039:
batchly_train_loss:  1.98148286059
cumulative_train_loss:  1.83484627479
*------------------------------------------------------------------------------*
Epoch 1, batch 2059:
batchly_train_loss:  2.0437861834
cumulative_train_loss:  1.8368758028
*------------------------------------------------------------------------------*
Epoch 1, batch 2079:
batchly_train_loss:  1.85792988077
cumulative_train_loss:  1.83707834323
*------------------------------------------------------------------------------*
Epoch 1, batch 2099:
batchly_train_loss:  1.75234701228
cumulative_train_loss:  1.83627099372
*------------------------------------------------------------------------------*
Epoch 1, batch 2119:
batchly_train_loss:  2.07626229527
cumulative_train_loss:  1.83853613107
*------------------------------------------------------------------------------*
Epoch 1, batch 2139:
batchly_train_loss:  1.89658932014
cumulative_train_loss:  1.83907893788
*------------------------------------------------------------------------------*
Epoch 1, batch 2159:
batchly_train_loss:  1.58157081907
cumulative_train_loss:  1.83669349908
*------------------------------------------------------------------------------*
Epoch 1, batch 2179:
batchly_train_loss:  1.67282734515
cumulative_train_loss:  1.83518944994
*------------------------------------------------------------------------------*
Epoch 1, batch 2199:
batchly_train_loss:  1.5630977119
cumulative_train_loss:  1.83271476383
*------------------------------------------------------------------------------*
Epoch 1, batch 2219:
batchly_train_loss:  1.840534889
cumulative_train_loss:  1.83278524715
*------------------------------------------------------------------------------*
Epoch 1, batch 2239:
batchly_train_loss:  1.57704909934
cumulative_train_loss:  1.83050086888
*------------------------------------------------------------------------------*
Epoch 1, batch 2259:
batchly_train_loss:  1.80841273853
cumulative_train_loss:  1.83030531217
*------------------------------------------------------------------------------*
Epoch 1, batch 2279:
batchly_train_loss:  1.92167846101
cumulative_train_loss:  1.83110718272
*------------------------------------------------------------------------------*
Epoch 1, batch 2299:
batchly_train_loss:  1.6768810974
cumulative_train_loss:  1.82976550298
*------------------------------------------------------------------------------*
Epoch 1, batch 2319:
batchly_train_loss:  1.41736125416
cumulative_train_loss:  1.82620876086
*------------------------------------------------------------------------------*
Epoch 1, batch 2339:
batchly_train_loss:  1.92259519661
cumulative_train_loss:  1.82703292876
*------------------------------------------------------------------------------*
Epoch 1, batch 2359:
batchly_train_loss:  1.8104907003
cumulative_train_loss:  1.82689268096
*------------------------------------------------------------------------------*
Epoch 1, batch 2379:
batchly_train_loss:  1.75342714156
cumulative_train_loss:  1.82627506398
*------------------------------------------------------------------------------*
Epoch 1, batch 2399:
batchly_train_loss:  1.79014649744
cumulative_train_loss:  1.82597386709
*------------------------------------------------------------------------------*
Epoch 1, batch 2419:
batchly_train_loss:  1.82565066629
cumulative_train_loss:  1.82597119491
*------------------------------------------------------------------------------*
Epoch 1, batch 2439:
batchly_train_loss:  2.12541421765
cumulative_train_loss:  1.82842665225
*------------------------------------------------------------------------------*
Epoch 1, batch 2459:
batchly_train_loss:  1.83207954728
cumulative_train_loss:  1.82845636266
*------------------------------------------------------------------------------*
Epoch 1, batch 2479:
batchly_train_loss:  1.87999713074
cumulative_train_loss:  1.82887218169
*------------------------------------------------------------------------------*
Epoch 1, batch 2499:
batchly_train_loss:  1.75948773067
cumulative_train_loss:  1.82831688396
*------------------------------------------------------------------------------*
Epoch 1, batch 2519:
batchly_train_loss:  1.95325889059
cumulative_train_loss:  1.82930888084
*------------------------------------------------------------------------------*
Epoch 1, batch 2539:
batchly_train_loss:  2.02205392278
cumulative_train_loss:  1.83082715608
*------------------------------------------------------------------------------*
Epoch 1, batch 2559:
batchly_train_loss:  1.47262574705
cumulative_train_loss:  1.828027614
*------------------------------------------------------------------------------*
Epoch 1, batch 2579:
batchly_train_loss:  1.88665497658
cumulative_train_loss:  1.8284822659
*------------------------------------------------------------------------------*
Epoch 1, batch 2599:
batchly_train_loss:  1.83175942157
cumulative_train_loss:  1.82850748449
*------------------------------------------------------------------------------*
Epoch 1, batch 2619:
batchly_train_loss:  1.58441690757
cumulative_train_loss:  1.82664348619
*------------------------------------------------------------------------------*
Epoch 1, batch 2639:
batchly_train_loss:  1.64638115099
cumulative_train_loss:  1.82527734496
*------------------------------------------------------------------------------*
Epoch 1, batch 2659:
batchly_train_loss:  1.48581953524
cumulative_train_loss:  1.82272407073
*------------------------------------------------------------------------------*
Epoch 1, batch 2679:
batchly_train_loss:  1.60354785853
cumulative_train_loss:  1.82108781681
*------------------------------------------------------------------------------*
Epoch 1, batch 2699:
batchly_train_loss:  1.40584210626
cumulative_train_loss:  1.81801078301
*------------------------------------------------------------------------------*
Epoch 1, batch 2719:
batchly_train_loss:  1.82200056088
cumulative_train_loss:  1.81804013041
*------------------------------------------------------------------------------*
Epoch 1, batch 2739:
batchly_train_loss:  1.87303350315
cumulative_train_loss:  1.81844168844
*------------------------------------------------------------------------------*
Epoch 1, batch 2759:
batchly_train_loss:  1.74961465573
cumulative_train_loss:  1.81794276106
*------------------------------------------------------------------------------*
Epoch 1, batch 2779:
batchly_train_loss:  1.94266933751
cumulative_train_loss:  1.81884039745
*------------------------------------------------------------------------------*
Epoch 1, batch 2799:
batchly_train_loss:  1.89754203328
cumulative_train_loss:  1.81940275283
*------------------------------------------------------------------------------*
Epoch 1, batch 2819:
batchly_train_loss:  1.78820833424
cumulative_train_loss:  1.81918143734
*------------------------------------------------------------------------------*
Epoch 1, batch 2839:
batchly_train_loss:  1.81134420722
cumulative_train_loss:  1.81912622613
*------------------------------------------------------------------------------*
Epoch 1, batch 2859:
batchly_train_loss:  1.63782402325
cumulative_train_loss:  1.8178579351
*------------------------------------------------------------------------------*
Epoch 1, batch 2879:
batchly_train_loss:  1.71732013272
cumulative_train_loss:  1.81715951341
*------------------------------------------------------------------------------*
Epoch 1, batch 2899:
batchly_train_loss:  1.93767207334
cumulative_train_loss:  1.81799092121
*------------------------------------------------------------------------------*
Epoch 1, batch 2919:
batchly_train_loss:  1.77118445317
cumulative_train_loss:  1.81767021913
*------------------------------------------------------------------------------*
Epoch 1, batch 2939:
batchly_train_loss:  1.88996700995
cumulative_train_loss:  1.81816220138
*------------------------------------------------------------------------------*
Epoch 1, batch 2959:
batchly_train_loss:  2.11225868735
cumulative_train_loss:  1.82015001135
*------------------------------------------------------------------------------*
Epoch 1, batch 2979:
batchly_train_loss:  1.83977912167
cumulative_train_loss:  1.82028179457
*------------------------------------------------------------------------------*
Epoch 1, batch 2999:
batchly_train_loss:  1.83223880938
cumulative_train_loss:  1.82036153458
*------------------------------------------------------------------------------*
Epoch 1, batch 3019:
batchly_train_loss:  1.91629464576
cumulative_train_loss:  1.82099706364
*------------------------------------------------------------------------------*
Epoch 1, batch 3039:
batchly_train_loss:  1.81769139143
cumulative_train_loss:  1.82097530864
*------------------------------------------------------------------------------*
Epoch 1, batch 3059:
batchly_train_loss:  1.47542458693
cumulative_train_loss:  1.81871606888
*------------------------------------------------------------------------------*
Epoch 1, batch 3079:
batchly_train_loss:  2.1068495803
cumulative_train_loss:  1.82058767337
*------------------------------------------------------------------------------*
Epoch 1, batch 3099:
batchly_train_loss:  1.85064631179
cumulative_train_loss:  1.82078166265
*------------------------------------------------------------------------------*
Epoch 1, batch 3119:
batchly_train_loss:  1.69327906326
cumulative_train_loss:  1.81996407624
*------------------------------------------------------------------------------*
Epoch 1, batch 3139:
batchly_train_loss:  1.80602179641
cumulative_train_loss:  1.81987524362
*------------------------------------------------------------------------------*
Epoch 1, batch 3159:
batchly_train_loss:  1.89896275843
cumulative_train_loss:  1.82037595597
*------------------------------------------------------------------------------*
Epoch 1, batch 3179:
batchly_train_loss:  2.07867971615
cumulative_train_loss:  1.82200101894
*------------------------------------------------------------------------------*
Epoch 1, batch 3199:
batchly_train_loss:  1.8978669689
cumulative_train_loss:  1.82247532935
*------------------------------------------------------------------------------*
Epoch 1, batch 3219:
batchly_train_loss:  2.16220936861
cumulative_train_loss:  1.82458613419
*------------------------------------------------------------------------------*
Epoch 1, batch 3239:
batchly_train_loss:  1.75220281063
cumulative_train_loss:  1.82413918561
*------------------------------------------------------------------------------*
Epoch 1, batch 3259:
batchly_train_loss:  1.53581570962
cumulative_train_loss:  1.82236978717
*------------------------------------------------------------------------------*
Epoch 1, batch 3279:
batchly_train_loss:  1.60307827535
cumulative_train_loss:  1.82103223601
*------------------------------------------------------------------------------*
Epoch 1, batch 3299:
batchly_train_loss:  1.79995553885
cumulative_train_loss:  1.82090445973
*------------------------------------------------------------------------------*
Epoch 1, batch 3319:
batchly_train_loss:  2.03694628676
cumulative_train_loss:  1.82220630865
*------------------------------------------------------------------------------*
Epoch 1, batch 3339:
batchly_train_loss:  2.03358359489
cumulative_train_loss:  1.82347241997
*------------------------------------------------------------------------------*
Epoch 1, batch 3359:
batchly_train_loss:  1.75048285172
cumulative_train_loss:  1.82303782892
*------------------------------------------------------------------------------*
Epoch 1, batch 3379:
batchly_train_loss:  1.67018947812
cumulative_train_loss:  1.82213313314
*------------------------------------------------------------------------------*
Epoch 1, batch 3399:
batchly_train_loss:  1.77761553595
cumulative_train_loss:  1.82187118788
*------------------------------------------------------------------------------*
Epoch 1, batch 3419:
batchly_train_loss:  1.97885038569
cumulative_train_loss:  1.82278946339
*------------------------------------------------------------------------------*
Epoch 1, batch 3439:
batchly_train_loss:  1.91010542873
cumulative_train_loss:  1.82329726197
*------------------------------------------------------------------------------*
Epoch 1, batch 3459:
batchly_train_loss:  1.56542890596
cumulative_train_loss:  1.82180626251
*------------------------------------------------------------------------------*
Epoch 1, batch 3479:
batchly_train_loss:  1.44956273327
cumulative_train_loss:  1.81966631695
*------------------------------------------------------------------------------*
Epoch 1, batch 3499:
batchly_train_loss:  1.96415343717
cumulative_train_loss:  1.82049219361
*------------------------------------------------------------------------------*
Epoch 1, batch 3519:
batchly_train_loss:  1.38279652143
cumulative_train_loss:  1.81800457967
*------------------------------------------------------------------------------*
Epoch 1, batch 3539:
batchly_train_loss:  1.74619755022
cumulative_train_loss:  1.8175987756
*------------------------------------------------------------------------------*
Epoch 1, batch 3559:
batchly_train_loss:  1.89160889426
cumulative_train_loss:  1.81801467961
*------------------------------------------------------------------------------*
Epoch 1, batch 3579:
batchly_train_loss:  1.47365931214
cumulative_train_loss:  1.81609036909
*------------------------------------------------------------------------------*
Epoch 1, batch 3599:
batchly_train_loss:  1.73710196846
cumulative_train_loss:  1.81565142272
*------------------------------------------------------------------------------*
Epoch 1, batch 3619:
batchly_train_loss:  1.76146113224
cumulative_train_loss:  1.81535194612
*------------------------------------------------------------------------------*
Epoch 1, batch 3639:
batchly_train_loss:  1.52533508337
cumulative_train_loss:  1.81375800898
*------------------------------------------------------------------------------*
Epoch 1, batch 3659:
batchly_train_loss:  1.9228708225
cumulative_train_loss:  1.81435441681
*------------------------------------------------------------------------------*
Epoch 1, batch 3679:
batchly_train_loss:  1.43182720258
cumulative_train_loss:  1.81227489948
*------------------------------------------------------------------------------*
Epoch 1, batch 3699:
batchly_train_loss:  1.75053983686
cumulative_train_loss:  1.81194110622
*------------------------------------------------------------------------------*
Epoch 1, batch 3719:
batchly_train_loss:  1.88073545274
cumulative_train_loss:  1.81231106775
*------------------------------------------------------------------------------*
Epoch 1, batch 3739:
batchly_train_loss:  1.87714250133
cumulative_train_loss:  1.81265785263
*------------------------------------------------------------------------------*
Epoch 1, batch 3759:
batchly_train_loss:  1.97761321066
cumulative_train_loss:  1.81353550817
*------------------------------------------------------------------------------*
Epoch 1, batch 3779:
batchly_train_loss:  1.5796966261
cumulative_train_loss:  1.81229793801
*------------------------------------------------------------------------------*
Epoch 1, batch 3799:
batchly_train_loss:  2.09521806805
cumulative_train_loss:  1.81378738328
*------------------------------------------------------------------------------*
Epoch 1, batch 3819:
batchly_train_loss:  1.78413750486
cumulative_train_loss:  1.81363210767
*------------------------------------------------------------------------------*
Epoch 1, batch 3839:
batchly_train_loss:  1.59746138373
cumulative_train_loss:  1.8125059252
*------------------------------------------------------------------------------*
Epoch 1, batch 3859:
batchly_train_loss:  1.84002960256
cumulative_train_loss:  1.81264857189
*------------------------------------------------------------------------------*
Epoch 1, batch 3879:
batchly_train_loss:  1.98708233716
cumulative_train_loss:  1.8135479468
*------------------------------------------------------------------------------*
Epoch 1, batch 3899:
batchly_train_loss:  1.83546044264
cumulative_train_loss:  1.8136603474
*------------------------------------------------------------------------------*
Epoch 1, batch 3919:
batchly_train_loss:  1.80442644083
cumulative_train_loss:  1.81361322361
*------------------------------------------------------------------------------*
Epoch 1, batch 3939:
batchly_train_loss:  2.17497271374
cumulative_train_loss:  1.81544800142
*------------------------------------------------------------------------------*
Epoch 1, batch 3959:
batchly_train_loss:  2.01449622559
cumulative_train_loss:  1.81645354941
*------------------------------------------------------------------------------*
Epoch 1, batch 3979:
batchly_train_loss:  1.71583236937
cumulative_train_loss:  1.81594778826
*------------------------------------------------------------------------------*
Epoch 1, batch 3999:
batchly_train_loss:  2.18763529105
cumulative_train_loss:  1.8178066905
================================================================================
Epoch 1 of 8 took 6530.979s
  training loss:		1.817371
evaluating model...
VALID_LOSS:  2.03725876354
VALID_ACC:  0.258181818182
FULL_TRAIN_LOSS:  2.0050192763
FULL_TRAIN_ACC:  0.282537313433
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=1
*------------------------------------------------------------------------------*
Epoch 2, batch 19:
batchly_train_loss:  2.05540496766
cumulative_train_loss:  2.06395652016
*------------------------------------------------------------------------------*
Epoch 2, batch 39:
batchly_train_loss:  1.79942127553
cumulative_train_loss:  1.92829742035
*------------------------------------------------------------------------------*
Epoch 2, batch 59:
batchly_train_loss:  1.66673081142
cumulative_train_loss:  1.83963077325
*------------------------------------------------------------------------------*
Epoch 2, batch 79:
batchly_train_loss:  1.82109345252
cumulative_train_loss:  1.83493778066
*------------------------------------------------------------------------------*
Epoch 2, batch 99:
batchly_train_loss:  1.51434380152
cumulative_train_loss:  1.77017132023
*------------------------------------------------------------------------------*
Epoch 2, batch 119:
batchly_train_loss:  1.46299999705
cumulative_train_loss:  1.71854588776
*------------------------------------------------------------------------------*
Epoch 2, batch 139:
batchly_train_loss:  2.12060689799
cumulative_train_loss:  1.77639639283
*------------------------------------------------------------------------------*
Epoch 2, batch 159:
batchly_train_loss:  1.66755914363
cumulative_train_loss:  1.76270617281
*------------------------------------------------------------------------------*
Epoch 2, batch 179:
batchly_train_loss:  2.05202552875
cumulative_train_loss:  1.79503235783
*------------------------------------------------------------------------------*
Epoch 2, batch 199:
batchly_train_loss:  1.98950639137
cumulative_train_loss:  1.81457748683
*------------------------------------------------------------------------------*
Epoch 2, batch 219:
batchly_train_loss:  1.72811413551
cumulative_train_loss:  1.80668129036
*------------------------------------------------------------------------------*
Epoch 2, batch 239:
batchly_train_loss:  1.58422913779
cumulative_train_loss:  1.78806604747
*------------------------------------------------------------------------------*
Epoch 2, batch 259:
batchly_train_loss:  1.48079079897
cumulative_train_loss:  1.76433822905
*------------------------------------------------------------------------------*
Epoch 2, batch 279:
batchly_train_loss:  1.63602846808
cumulative_train_loss:  1.75514039672
*------------------------------------------------------------------------------*
Epoch 2, batch 299:
batchly_train_loss:  1.36888870405
cumulative_train_loss:  1.7293041631
*------------------------------------------------------------------------------*
Epoch 2, batch 319:
batchly_train_loss:  1.79262662686
cumulative_train_loss:  1.73327422352
*------------------------------------------------------------------------------*
Epoch 2, batch 339:
batchly_train_loss:  1.47492846733
cumulative_train_loss:  1.71803258599
*------------------------------------------------------------------------------*
Epoch 2, batch 359:
batchly_train_loss:  1.87833736499
cumulative_train_loss:  1.72696321435
*------------------------------------------------------------------------------*
Epoch 2, batch 379:
batchly_train_loss:  1.40268378251
cumulative_train_loss:  1.70985084327
*------------------------------------------------------------------------------*
Epoch 2, batch 399:
batchly_train_loss:  1.72977187367
cumulative_train_loss:  1.71084939116
*------------------------------------------------------------------------------*
Epoch 2, batch 419:
batchly_train_loss:  2.02592329518
cumulative_train_loss:  1.72588871832
*------------------------------------------------------------------------------*
Epoch 2, batch 439:
batchly_train_loss:  2.03677434882
cumulative_train_loss:  1.74005207279
*------------------------------------------------------------------------------*
Epoch 2, batch 459:
batchly_train_loss:  1.84138567183
cumulative_train_loss:  1.74446748015
*------------------------------------------------------------------------------*
Epoch 2, batch 479:
batchly_train_loss:  1.95562302974
cumulative_train_loss:  1.75328399579
*------------------------------------------------------------------------------*
Epoch 2, batch 499:
batchly_train_loss:  1.61449597259
cumulative_train_loss:  1.74772134957
*------------------------------------------------------------------------------*
Epoch 2, batch 519:
batchly_train_loss:  1.35226327328
cumulative_train_loss:  1.73248211735
*------------------------------------------------------------------------------*
Epoch 2, batch 539:
batchly_train_loss:  2.40815737349
cumulative_train_loss:  1.75755355542
*------------------------------------------------------------------------------*
Epoch 2, batch 559:
batchly_train_loss:  1.91078466969
cumulative_train_loss:  1.76303588509
*------------------------------------------------------------------------------*
Epoch 2, batch 579:
batchly_train_loss:  1.62273784207
cumulative_train_loss:  1.75818966599
*------------------------------------------------------------------------------*
Epoch 2, batch 599:
batchly_train_loss:  1.78262423285
cumulative_train_loss:  1.75900551129
*------------------------------------------------------------------------------*
Epoch 2, batch 619:
batchly_train_loss:  1.60018633281
cumulative_train_loss:  1.75387403541
*------------------------------------------------------------------------------*
Epoch 2, batch 639:
batchly_train_loss:  1.79384526217
cumulative_train_loss:  1.75512509103
*------------------------------------------------------------------------------*
Epoch 2, batch 659:
batchly_train_loss:  1.69603056084
cumulative_train_loss:  1.75333163032
*------------------------------------------------------------------------------*
Epoch 2, batch 679:
batchly_train_loss:  1.49711983719
cumulative_train_loss:  1.7457848912
*------------------------------------------------------------------------------*
Epoch 2, batch 699:
batchly_train_loss:  1.88112371466
cumulative_train_loss:  1.74965724666
*------------------------------------------------------------------------------*
Epoch 2, batch 719:
batchly_train_loss:  2.16581992274
cumulative_train_loss:  1.76123339899
*------------------------------------------------------------------------------*
Epoch 2, batch 739:
batchly_train_loss:  1.88160435543
cumulative_train_loss:  1.76449107034
*------------------------------------------------------------------------------*
Epoch 2, batch 759:
batchly_train_loss:  1.78745716245
cumulative_train_loss:  1.76509623746
*------------------------------------------------------------------------------*
Epoch 2, batch 779:
batchly_train_loss:  2.00037832754
cumulative_train_loss:  1.77113685595
*------------------------------------------------------------------------------*
Epoch 2, batch 799:
batchly_train_loss:  1.91395828681
cumulative_train_loss:  1.77471186047
*------------------------------------------------------------------------------*
Epoch 2, batch 819:
batchly_train_loss:  1.81910658253
cumulative_train_loss:  1.77579598067
*------------------------------------------------------------------------------*
Epoch 2, batch 839:
batchly_train_loss:  1.74052195258
cumulative_train_loss:  1.77495512184
*------------------------------------------------------------------------------*
Epoch 2, batch 859:
batchly_train_loss:  1.62809381071
cumulative_train_loss:  1.77153576651
*------------------------------------------------------------------------------*
Epoch 2, batch 879:
batchly_train_loss:  1.99894877162
cumulative_train_loss:  1.77671012385
*------------------------------------------------------------------------------*
Epoch 2, batch 899:
batchly_train_loss:  2.07236261091
cumulative_train_loss:  1.7832874873
*------------------------------------------------------------------------------*
Epoch 2, batch 919:
batchly_train_loss:  1.61901073116
cumulative_train_loss:  1.77971236747
*------------------------------------------------------------------------------*
Epoch 2, batch 939:
batchly_train_loss:  1.69798225527
cumulative_train_loss:  1.77797157701
*------------------------------------------------------------------------------*
Epoch 2, batch 959:
batchly_train_loss:  1.74507579747
cumulative_train_loss:  1.77728553364
*------------------------------------------------------------------------------*
Epoch 2, batch 979:
batchly_train_loss:  1.89571565692
cumulative_train_loss:  1.77970494372
*------------------------------------------------------------------------------*
Epoch 2, batch 999:
batchly_train_loss:  1.86497265113
cumulative_train_loss:  1.78141200493
*------------------------------------------------------------------------------*
Epoch 2, batch 1019:
batchly_train_loss:  1.91712853283
cumulative_train_loss:  1.78407572481
*------------------------------------------------------------------------------*
Epoch 2, batch 1039:
batchly_train_loss:  1.92461910661
cumulative_train_loss:  1.78678108346
*------------------------------------------------------------------------------*
Epoch 2, batch 1059:
batchly_train_loss:  1.58954348099
cumulative_train_loss:  1.78305610513
*------------------------------------------------------------------------------*
Epoch 2, batch 1079:
batchly_train_loss:  1.92493751111
cumulative_train_loss:  1.78568597364
*------------------------------------------------------------------------------*
Epoch 2, batch 1099:
batchly_train_loss:  1.68636346044
cumulative_train_loss:  1.78387846657
*------------------------------------------------------------------------------*
Epoch 2, batch 1119:
batchly_train_loss:  1.74410771327
cumulative_train_loss:  1.78316763988
*------------------------------------------------------------------------------*
Epoch 2, batch 1139:
batchly_train_loss:  1.68726420508
cumulative_train_loss:  1.7814836463
*------------------------------------------------------------------------------*
Epoch 2, batch 1159:
batchly_train_loss:  1.70298120487
cumulative_train_loss:  1.78012898812
*------------------------------------------------------------------------------*
Epoch 2, batch 1179:
batchly_train_loss:  1.55576029109
cumulative_train_loss:  1.77632290335
*------------------------------------------------------------------------------*
Epoch 2, batch 1199:
batchly_train_loss:  1.48351656007
cumulative_train_loss:  1.77143872748
*------------------------------------------------------------------------------*
Epoch 2, batch 1219:
batchly_train_loss:  1.32305409292
cumulative_train_loss:  1.7640821297
*------------------------------------------------------------------------------*
Epoch 2, batch 1239:
batchly_train_loss:  1.70201377467
cumulative_train_loss:  1.76308021921
*------------------------------------------------------------------------------*
Epoch 2, batch 1259:
batchly_train_loss:  1.79295458608
cumulative_train_loss:  1.76355479216
*------------------------------------------------------------------------------*
Epoch 2, batch 1279:
batchly_train_loss:  2.13037997908
cumulative_train_loss:  1.76929091705
*------------------------------------------------------------------------------*
Epoch 2, batch 1299:
batchly_train_loss:  1.66186961435
cumulative_train_loss:  1.76763700939
*------------------------------------------------------------------------------*
Epoch 2, batch 1319:
batchly_train_loss:  1.85656968819
cumulative_train_loss:  1.7689854958
*------------------------------------------------------------------------------*
Epoch 2, batch 1339:
batchly_train_loss:  1.82064903893
cumulative_train_loss:  1.76975716933
*------------------------------------------------------------------------------*
Epoch 2, batch 1359:
batchly_train_loss:  1.68163932253
cumulative_train_loss:  1.76846036511
*------------------------------------------------------------------------------*
Epoch 2, batch 1379:
batchly_train_loss:  1.37016844025
cumulative_train_loss:  1.76268383248
*------------------------------------------------------------------------------*
Epoch 2, batch 1399:
batchly_train_loss:  1.26795172249
cumulative_train_loss:  1.75561117901
*------------------------------------------------------------------------------*
Epoch 2, batch 1419:
batchly_train_loss:  1.19873906269
cumulative_train_loss:  1.74776238245
*------------------------------------------------------------------------------*
Epoch 2, batch 1439:
batchly_train_loss:  1.49408511309
cumulative_train_loss:  1.74423663861
*------------------------------------------------------------------------------*
Epoch 2, batch 1459:
batchly_train_loss:  1.78744944517
cumulative_train_loss:  1.74482900059
*------------------------------------------------------------------------------*
Epoch 2, batch 1479:
batchly_train_loss:  1.78610638146
cumulative_train_loss:  1.74538718018
*------------------------------------------------------------------------------*
Epoch 2, batch 1499:
batchly_train_loss:  1.75096844867
cumulative_train_loss:  1.74546164674
*------------------------------------------------------------------------------*
Epoch 2, batch 1519:
batchly_train_loss:  1.57720069573
cumulative_train_loss:  1.74324622935
*------------------------------------------------------------------------------*
Epoch 2, batch 1539:
batchly_train_loss:  1.50839812204
cumulative_train_loss:  1.74019427214
*------------------------------------------------------------------------------*
Epoch 2, batch 1559:
batchly_train_loss:  1.99980728089
cumulative_train_loss:  1.74352477898
*------------------------------------------------------------------------------*
Epoch 2, batch 1579:
batchly_train_loss:  1.71755325132
cumulative_train_loss:  1.74319581727
*------------------------------------------------------------------------------*
Epoch 2, batch 1599:
batchly_train_loss:  1.77707336659
cumulative_train_loss:  1.74361955147
*------------------------------------------------------------------------------*
Epoch 2, batch 1619:
batchly_train_loss:  1.72215683047
cumulative_train_loss:  1.74335441594
*------------------------------------------------------------------------------*
Epoch 2, batch 1639:
batchly_train_loss:  1.82184419338
cumulative_train_loss:  1.74431219236
*------------------------------------------------------------------------------*
Epoch 2, batch 1659:
batchly_train_loss:  1.92483539696
cumulative_train_loss:  1.74648848174
*------------------------------------------------------------------------------*
Epoch 2, batch 1679:
batchly_train_loss:  2.03356010872
cumulative_train_loss:  1.74990803656
*------------------------------------------------------------------------------*
Epoch 2, batch 1699:
batchly_train_loss:  1.75838357633
cumulative_train_loss:  1.75000780748
*------------------------------------------------------------------------------*
Epoch 2, batch 1719:
batchly_train_loss:  1.51559346885
cumulative_train_loss:  1.7472804737
*------------------------------------------------------------------------------*
Epoch 2, batch 1739:
batchly_train_loss:  1.92810702481
cumulative_train_loss:  1.74936013501
*------------------------------------------------------------------------------*
Epoch 2, batch 1759:
batchly_train_loss:  1.6660429656
cumulative_train_loss:  1.74841281074
*------------------------------------------------------------------------------*
Epoch 2, batch 1779:
batchly_train_loss:  1.78294243093
cumulative_train_loss:  1.74880100209
*------------------------------------------------------------------------------*
Epoch 2, batch 1799:
batchly_train_loss:  1.56406905316
cumulative_train_loss:  1.74674728392
*------------------------------------------------------------------------------*
Epoch 2, batch 1819:
batchly_train_loss:  1.42357258144
cumulative_train_loss:  1.74319396119
*------------------------------------------------------------------------------*
Epoch 2, batch 1839:
batchly_train_loss:  1.86039400718
cumulative_train_loss:  1.74446856746
*------------------------------------------------------------------------------*
Epoch 2, batch 1859:
batchly_train_loss:  1.88483829649
cumulative_train_loss:  1.7459787313
*------------------------------------------------------------------------------*
Epoch 2, batch 1879:
batchly_train_loss:  1.63063454371
cumulative_train_loss:  1.74475101243
*------------------------------------------------------------------------------*
Epoch 2, batch 1899:
batchly_train_loss:  1.69677317573
cumulative_train_loss:  1.74424571662
*------------------------------------------------------------------------------*
Epoch 2, batch 1919:
batchly_train_loss:  1.71778410738
cumulative_train_loss:  1.74396993122
*------------------------------------------------------------------------------*
Epoch 2, batch 1939:
batchly_train_loss:  1.77389580182
cumulative_train_loss:  1.74427860446
*------------------------------------------------------------------------------*
Epoch 2, batch 1959:
batchly_train_loss:  1.89744496414
cumulative_train_loss:  1.74584232432
*------------------------------------------------------------------------------*
Epoch 2, batch 1979:
batchly_train_loss:  1.71025020043
cumulative_train_loss:  1.74548262625
*------------------------------------------------------------------------------*
Epoch 2, batch 1999:
batchly_train_loss:  1.55906154793
cumulative_train_loss:  1.74361748289
*------------------------------------------------------------------------------*
Epoch 2, batch 2019:
batchly_train_loss:  1.38061158632
cumulative_train_loss:  1.74002158496
*------------------------------------------------------------------------------*
Epoch 2, batch 2039:
batchly_train_loss:  1.67610484232
cumulative_train_loss:  1.7393946429
*------------------------------------------------------------------------------*
Epoch 2, batch 2059:
batchly_train_loss:  1.49143509342
cumulative_train_loss:  1.73698609944
*------------------------------------------------------------------------------*
Epoch 2, batch 2079:
batchly_train_loss:  1.74061347591
cumulative_train_loss:  1.73702099484
*------------------------------------------------------------------------------*
Epoch 2, batch 2099:
batchly_train_loss:  1.71491553657
cumulative_train_loss:  1.73681036636
*------------------------------------------------------------------------------*
Epoch 2, batch 2119:
batchly_train_loss:  1.36292893974
cumulative_train_loss:  1.73328151854
*------------------------------------------------------------------------------*
Epoch 2, batch 2139:
batchly_train_loss:  1.48822730691
cumulative_train_loss:  1.73099022157
*------------------------------------------------------------------------------*
Epoch 2, batch 2159:
batchly_train_loss:  1.64468873453
cumulative_train_loss:  1.7301907636
*------------------------------------------------------------------------------*
Epoch 2, batch 2179:
batchly_train_loss:  1.85403851232
cumulative_train_loss:  1.73132750292
*------------------------------------------------------------------------------*
Epoch 2, batch 2199:
batchly_train_loss:  1.67267745389
cumulative_train_loss:  1.73079407819
*------------------------------------------------------------------------------*
Epoch 2, batch 2219:
batchly_train_loss:  1.74296355824
cumulative_train_loss:  1.73090376255
*------------------------------------------------------------------------------*
Epoch 2, batch 2239:
batchly_train_loss:  1.57701248334
cumulative_train_loss:  1.7295291196
*------------------------------------------------------------------------------*
Epoch 2, batch 2259:
batchly_train_loss:  1.96479146719
cumulative_train_loss:  1.73161200891
*------------------------------------------------------------------------------*
Epoch 2, batch 2279:
batchly_train_loss:  1.71053153514
cumulative_train_loss:  1.73142701133
*------------------------------------------------------------------------------*
Epoch 2, batch 2299:
batchly_train_loss:  1.7687130654
cumulative_train_loss:  1.73175137892
*------------------------------------------------------------------------------*
Epoch 2, batch 2319:
batchly_train_loss:  1.65634365042
cumulative_train_loss:  1.73110103197
*------------------------------------------------------------------------------*
Epoch 2, batch 2339:
batchly_train_loss:  1.65586352799
cumulative_train_loss:  1.73045770145
*------------------------------------------------------------------------------*
Epoch 2, batch 2359:
batchly_train_loss:  1.83085275666
cumulative_train_loss:  1.73130886767
*------------------------------------------------------------------------------*
Epoch 2, batch 2379:
batchly_train_loss:  1.91884518463
cumulative_train_loss:  1.73288546554
*------------------------------------------------------------------------------*
Epoch 2, batch 2399:
batchly_train_loss:  1.61733422437
cumulative_train_loss:  1.73192213715
*------------------------------------------------------------------------------*
Epoch 2, batch 2419:
batchly_train_loss:  1.57646513338
cumulative_train_loss:  1.7306368374
*------------------------------------------------------------------------------*
Epoch 2, batch 2439:
batchly_train_loss:  1.80741568644
cumulative_train_loss:  1.73126643026
*------------------------------------------------------------------------------*
Epoch 2, batch 2459:
batchly_train_loss:  2.0514308063
cumulative_train_loss:  1.73387045121
*------------------------------------------------------------------------------*
Epoch 2, batch 2479:
batchly_train_loss:  1.81063130173
cumulative_train_loss:  1.73448974004
*------------------------------------------------------------------------------*
Epoch 2, batch 2499:
batchly_train_loss:  1.94558823987
cumulative_train_loss:  1.73617920383
*------------------------------------------------------------------------------*
Epoch 2, batch 2519:
batchly_train_loss:  1.4821174972
cumulative_train_loss:  1.73416204062
*------------------------------------------------------------------------------*
Epoch 2, batch 2539:
batchly_train_loss:  1.50521864674
cumulative_train_loss:  1.73235862672
*------------------------------------------------------------------------------*
Epoch 2, batch 2559:
batchly_train_loss:  1.93636795963
cumulative_train_loss:  1.73395307246
*------------------------------------------------------------------------------*
Epoch 2, batch 2579:
batchly_train_loss:  1.78367184553
cumulative_train_loss:  1.73433863875
*------------------------------------------------------------------------------*
Epoch 2, batch 2599:
batchly_train_loss:  1.54684283363
cumulative_train_loss:  1.7328958084
*------------------------------------------------------------------------------*
Epoch 2, batch 2619:
batchly_train_loss:  1.82625213644
cumulative_train_loss:  1.73360872423
*------------------------------------------------------------------------------*
Epoch 2, batch 2639:
batchly_train_loss:  1.72525198428
cumulative_train_loss:  1.7335453916
*------------------------------------------------------------------------------*
Epoch 2, batch 2659:
batchly_train_loss:  1.8339341113
cumulative_train_loss:  1.73430047787
*------------------------------------------------------------------------------*
Epoch 2, batch 2679:
batchly_train_loss:  1.68529575219
cumulative_train_loss:  1.73393463445
*------------------------------------------------------------------------------*
Epoch 2, batch 2699:
batchly_train_loss:  1.60407848962
cumulative_train_loss:  1.7329723807
*------------------------------------------------------------------------------*
Epoch 2, batch 2719:
batchly_train_loss:  1.72498099754
cumulative_train_loss:  1.73291359891
*------------------------------------------------------------------------------*
Epoch 2, batch 2739:
batchly_train_loss:  1.54434901472
cumulative_train_loss:  1.73153671257
*------------------------------------------------------------------------------*
Epoch 2, batch 2759:
batchly_train_loss:  1.88374943915
cumulative_train_loss:  1.73264010313
*------------------------------------------------------------------------------*
Epoch 2, batch 2779:
batchly_train_loss:  1.49622441453
cumulative_train_loss:  1.7309386588
*------------------------------------------------------------------------------*
Epoch 2, batch 2799:
batchly_train_loss:  1.56816643769
cumulative_train_loss:  1.7297755847
*------------------------------------------------------------------------------*
Epoch 2, batch 2819:
batchly_train_loss:  1.9044263998
cumulative_train_loss:  1.73101468236
*------------------------------------------------------------------------------*
Epoch 2, batch 2839:
batchly_train_loss:  1.42044312841
cumulative_train_loss:  1.72882678835
*------------------------------------------------------------------------------*
Epoch 2, batch 2859:
batchly_train_loss:  1.4743178223
cumulative_train_loss:  1.72704638285
*------------------------------------------------------------------------------*
Epoch 2, batch 2879:
batchly_train_loss:  2.04432470852
cumulative_train_loss:  1.72925046987
*------------------------------------------------------------------------------*
Epoch 2, batch 2899:
batchly_train_loss:  1.76247957735
cumulative_train_loss:  1.72947971518
*------------------------------------------------------------------------------*
Epoch 2, batch 2919:
batchly_train_loss:  1.65429135235
cumulative_train_loss:  1.72896454996
*------------------------------------------------------------------------------*
Epoch 2, batch 2939:
batchly_train_loss:  1.87246330767
cumulative_train_loss:  1.72994106414
*------------------------------------------------------------------------------*
Epoch 2, batch 2959:
batchly_train_loss:  1.63048897543
cumulative_train_loss:  1.72926886347
*------------------------------------------------------------------------------*
Epoch 2, batch 2979:
batchly_train_loss:  2.07046744808
cumulative_train_loss:  1.73155955554
*------------------------------------------------------------------------------*
Epoch 2, batch 2999:
batchly_train_loss:  1.77074553113
cumulative_train_loss:  1.73182088249
*------------------------------------------------------------------------------*
Epoch 2, batch 3019:
batchly_train_loss:  1.94162384875
cumulative_train_loss:  1.73321076633
*------------------------------------------------------------------------------*
Epoch 2, batch 3039:
batchly_train_loss:  1.81021260102
cumulative_train_loss:  1.73371752405
*------------------------------------------------------------------------------*
Epoch 2, batch 3059:
batchly_train_loss:  1.66902616819
cumulative_train_loss:  1.73329456651
*------------------------------------------------------------------------------*
Epoch 2, batch 3079:
batchly_train_loss:  1.90637987504
cumulative_train_loss:  1.73441886211
*------------------------------------------------------------------------------*
Epoch 2, batch 3099:
batchly_train_loss:  1.65429542446
cumulative_train_loss:  1.73390176991
*------------------------------------------------------------------------------*
Epoch 2, batch 3119:
batchly_train_loss:  1.76639900326
cumulative_train_loss:  1.73411015229
*------------------------------------------------------------------------------*
Epoch 2, batch 3139:
batchly_train_loss:  1.71897180699
cumulative_train_loss:  1.73401369899
*------------------------------------------------------------------------------*
Epoch 2, batch 3159:
batchly_train_loss:  1.70057434793
cumulative_train_loss:  1.73380199054
*------------------------------------------------------------------------------*
Epoch 2, batch 3179:
batchly_train_loss:  1.47814775184
cumulative_train_loss:  1.73219359646
*------------------------------------------------------------------------------*
Epoch 2, batch 3199:
batchly_train_loss:  1.77324144563
cumulative_train_loss:  1.73245022571
*------------------------------------------------------------------------------*
Epoch 2, batch 3219:
batchly_train_loss:  1.64730681797
cumulative_train_loss:  1.73192122038
*------------------------------------------------------------------------------*
Epoch 2, batch 3239:
batchly_train_loss:  1.67766633663
cumulative_train_loss:  1.73158621029
*------------------------------------------------------------------------------*
Epoch 2, batch 3259:
batchly_train_loss:  1.49814318118
cumulative_train_loss:  1.73015360502
*------------------------------------------------------------------------------*
Epoch 2, batch 3279:
batchly_train_loss:  1.78089018098
cumulative_train_loss:  1.73046306874
*------------------------------------------------------------------------------*
Epoch 2, batch 3299:
batchly_train_loss:  1.63239316942
cumulative_train_loss:  1.72986852555
*------------------------------------------------------------------------------*
Epoch 2, batch 3319:
batchly_train_loss:  1.76259900285
cumulative_train_loss:  1.7300657565
*------------------------------------------------------------------------------*
Epoch 2, batch 3339:
batchly_train_loss:  1.83970510003
cumulative_train_loss:  1.73072247614
*------------------------------------------------------------------------------*
Epoch 2, batch 3359:
batchly_train_loss:  1.73503761826
cumulative_train_loss:  1.73074816916
*------------------------------------------------------------------------------*
Epoch 2, batch 3379:
batchly_train_loss:  1.80070020424
cumulative_train_loss:  1.73116220902
*------------------------------------------------------------------------------*
Epoch 2, batch 3399:
batchly_train_loss:  1.69846601285
cumulative_train_loss:  1.73096982187
*------------------------------------------------------------------------------*
Epoch 2, batch 3419:
batchly_train_loss:  1.536205668
cumulative_train_loss:  1.72983051708
*------------------------------------------------------------------------------*
Epoch 2, batch 3439:
batchly_train_loss:  1.6707298126
cumulative_train_loss:  1.72948680842
*------------------------------------------------------------------------------*
Epoch 2, batch 3459:
batchly_train_loss:  1.61940446158
cumulative_train_loss:  1.72885031032
*------------------------------------------------------------------------------*
Epoch 2, batch 3479:
batchly_train_loss:  1.57778223056
cumulative_train_loss:  1.7279818534
*------------------------------------------------------------------------------*
Epoch 2, batch 3499:
batchly_train_loss:  1.75597572669
cumulative_train_loss:  1.72814186411
*------------------------------------------------------------------------------*
Epoch 2, batch 3519:
batchly_train_loss:  1.67464565614
cumulative_train_loss:  1.72783782201
*------------------------------------------------------------------------------*
Epoch 2, batch 3539:
batchly_train_loss:  1.3883676627
cumulative_train_loss:  1.72591936957
*------------------------------------------------------------------------------*
Epoch 2, batch 3559:
batchly_train_loss:  1.56774249806
cumulative_train_loss:  1.72503048577
*------------------------------------------------------------------------------*
Epoch 2, batch 3579:
batchly_train_loss:  1.61960695903
cumulative_train_loss:  1.72444136296
*------------------------------------------------------------------------------*
Epoch 2, batch 3599:
batchly_train_loss:  1.86049227088
cumulative_train_loss:  1.72519741135
*------------------------------------------------------------------------------*
Epoch 2, batch 3619:
batchly_train_loss:  1.82066312585
cumulative_train_loss:  1.72572499198
*------------------------------------------------------------------------------*
Epoch 2, batch 3639:
batchly_train_loss:  1.7725990968
cumulative_train_loss:  1.72598261278
*------------------------------------------------------------------------------*
Epoch 2, batch 3659:
batchly_train_loss:  1.53216892189
cumulative_train_loss:  1.72492323213
*------------------------------------------------------------------------------*
Epoch 2, batch 3679:
batchly_train_loss:  1.73032510077
cumulative_train_loss:  1.72495259809
*------------------------------------------------------------------------------*
Epoch 2, batch 3699:
batchly_train_loss:  1.79279390115
cumulative_train_loss:  1.72531940697
*------------------------------------------------------------------------------*
Epoch 2, batch 3719:
batchly_train_loss:  1.50602842231
cumulative_train_loss:  1.72414010617
*------------------------------------------------------------------------------*
Epoch 2, batch 3739:
batchly_train_loss:  1.77483062802
cumulative_train_loss:  1.72441125098
*------------------------------------------------------------------------------*
Epoch 2, batch 3759:
batchly_train_loss:  2.01070596222
cumulative_train_loss:  1.7259345003
*------------------------------------------------------------------------------*
Epoch 2, batch 3779:
batchly_train_loss:  1.51525029085
cumulative_train_loss:  1.72481947406
*------------------------------------------------------------------------------*
Epoch 2, batch 3799:
batchly_train_loss:  1.91974237786
cumulative_train_loss:  1.72584565412
*------------------------------------------------------------------------------*
Epoch 2, batch 3819:
batchly_train_loss:  1.38420001335
cumulative_train_loss:  1.72405646512
*------------------------------------------------------------------------------*
Epoch 2, batch 3839:
batchly_train_loss:  1.59598092542
cumulative_train_loss:  1.72338923126
*------------------------------------------------------------------------------*
Epoch 2, batch 3859:
batchly_train_loss:  1.85111197447
cumulative_train_loss:  1.72405117862
*------------------------------------------------------------------------------*
Epoch 2, batch 3879:
batchly_train_loss:  1.83314526591
cumulative_train_loss:  1.72461366424
*------------------------------------------------------------------------------*
Epoch 2, batch 3899:
batchly_train_loss:  1.51290975136
cumulative_train_loss:  1.72352772471
*------------------------------------------------------------------------------*
Epoch 2, batch 3919:
batchly_train_loss:  2.3789073249
cumulative_train_loss:  1.7268723514
*------------------------------------------------------------------------------*
Epoch 2, batch 3939:
batchly_train_loss:  1.94941893514
cumulative_train_loss:  1.72800231628
*------------------------------------------------------------------------------*
Epoch 2, batch 3959:
batchly_train_loss:  1.81814083144
cumulative_train_loss:  1.7284576763
*------------------------------------------------------------------------------*
Epoch 2, batch 3979:
batchly_train_loss:  1.79123912798
cumulative_train_loss:  1.72877324027
*------------------------------------------------------------------------------*
Epoch 2, batch 3999:
batchly_train_loss:  1.33864249272
cumulative_train_loss:  1.72682209874
================================================================================
Epoch 2 of 8 took 6457.987s
  training loss:		1.726390
evaluating model...
VALID_LOSS:  1.63746880468
VALID_ACC:  0.489090909091
FULL_TRAIN_LOSS:  1.61072311435
FULL_TRAIN_ACC:  0.413383084577
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=2
*------------------------------------------------------------------------------*
Epoch 3, batch 19:
batchly_train_loss:  1.48023707314
cumulative_train_loss:  1.46730258038
*------------------------------------------------------------------------------*
Epoch 3, batch 39:
batchly_train_loss:  1.54459041991
cumulative_train_loss:  1.50693736988
*------------------------------------------------------------------------------*
Epoch 3, batch 59:
batchly_train_loss:  1.74217601892
cumulative_train_loss:  1.58667928481
*------------------------------------------------------------------------------*
Epoch 3, batch 79:
batchly_train_loss:  1.90183005749
cumulative_train_loss:  1.66646429055
*------------------------------------------------------------------------------*
Epoch 3, batch 99:
batchly_train_loss:  1.44477982227
cumulative_train_loss:  1.62167954949
*------------------------------------------------------------------------------*
Epoch 3, batch 119:
batchly_train_loss:  1.63235900572
cumulative_train_loss:  1.62347441608
*------------------------------------------------------------------------------*
Epoch 3, batch 139:
batchly_train_loss:  1.45577238197
cumulative_train_loss:  1.599344627
*------------------------------------------------------------------------------*
Epoch 3, batch 159:
batchly_train_loss:  1.68460766474
cumulative_train_loss:  1.61006953741
*------------------------------------------------------------------------------*
Epoch 3, batch 179:
batchly_train_loss:  1.66960901929
cumulative_train_loss:  1.61672199348
*------------------------------------------------------------------------------*
Epoch 3, batch 199:
batchly_train_loss:  1.45034324829
cumulative_train_loss:  1.60000051155
*------------------------------------------------------------------------------*
Epoch 3, batch 219:
batchly_train_loss:  1.47429845739
cumulative_train_loss:  1.58852087191
*------------------------------------------------------------------------------*
Epoch 3, batch 239:
batchly_train_loss:  1.75707616282
cumulative_train_loss:  1.60262591717
*------------------------------------------------------------------------------*
Epoch 3, batch 259:
batchly_train_loss:  2.13229856712
cumulative_train_loss:  1.6435272801
*------------------------------------------------------------------------------*
Epoch 3, batch 279:
batchly_train_loss:  1.82726483554
cumulative_train_loss:  1.65669843103
*------------------------------------------------------------------------------*
Epoch 3, batch 299:
batchly_train_loss:  1.73789657505
cumulative_train_loss:  1.66212974501
*------------------------------------------------------------------------------*
Epoch 3, batch 319:
batchly_train_loss:  1.78159800109
cumulative_train_loss:  1.6696199178
*------------------------------------------------------------------------------*
Epoch 3, batch 339:
batchly_train_loss:  1.91177462372
cumulative_train_loss:  1.68390633113
*------------------------------------------------------------------------------*
Epoch 3, batch 359:
batchly_train_loss:  1.34727400204
cumulative_train_loss:  1.66515244093
*------------------------------------------------------------------------------*
Epoch 3, batch 379:
batchly_train_loss:  1.47875205011
cumulative_train_loss:  1.6553160087
*------------------------------------------------------------------------------*
Epoch 3, batch 399:
batchly_train_loss:  1.94498978438
cumulative_train_loss:  1.66983599745
*------------------------------------------------------------------------------*
Epoch 3, batch 419:
batchly_train_loss:  1.71058878097
cumulative_train_loss:  1.67178123772
*------------------------------------------------------------------------------*
Epoch 3, batch 439:
batchly_train_loss:  1.68109475953
cumulative_train_loss:  1.67220554395
*------------------------------------------------------------------------------*
Epoch 3, batch 459:
batchly_train_loss:  1.71200409664
cumulative_train_loss:  1.67393968568
*------------------------------------------------------------------------------*
Epoch 3, batch 479:
batchly_train_loss:  1.26056262199
cumulative_train_loss:  1.65667968302
*------------------------------------------------------------------------------*
Epoch 3, batch 499:
batchly_train_loss:  1.58348728956
cumulative_train_loss:  1.65374612016
*------------------------------------------------------------------------------*
Epoch 3, batch 519:
batchly_train_loss:  1.13941157451
cumulative_train_loss:  1.63392590645
*------------------------------------------------------------------------------*
Epoch 3, batch 539:
batchly_train_loss:  1.50885628319
cumulative_train_loss:  1.6292851041
*------------------------------------------------------------------------------*
Epoch 3, batch 559:
batchly_train_loss:  1.54549606999
cumulative_train_loss:  1.62628728535
*------------------------------------------------------------------------------*
Epoch 3, batch 579:
batchly_train_loss:  1.57228461563
cumulative_train_loss:  1.62442190816
*------------------------------------------------------------------------------*
Epoch 3, batch 599:
batchly_train_loss:  1.35644319301
cumulative_train_loss:  1.61547437176
*------------------------------------------------------------------------------*
Epoch 3, batch 619:
batchly_train_loss:  1.91944417411
cumulative_train_loss:  1.62529569009
*------------------------------------------------------------------------------*
Epoch 3, batch 639:
batchly_train_loss:  1.45136435316
cumulative_train_loss:  1.61985182978
*------------------------------------------------------------------------------*
Epoch 3, batch 659:
batchly_train_loss:  1.61063886969
cumulative_train_loss:  1.61957222553
*------------------------------------------------------------------------------*
Epoch 3, batch 679:
batchly_train_loss:  1.54133932023
cumulative_train_loss:  1.61726786897
*------------------------------------------------------------------------------*
Epoch 3, batch 699:
batchly_train_loss:  2.37312636843
cumulative_train_loss:  1.6388947216
*------------------------------------------------------------------------------*
Epoch 3, batch 719:
batchly_train_loss:  1.78222571506
cumulative_train_loss:  1.64288167552
*------------------------------------------------------------------------------*
Epoch 3, batch 739:
batchly_train_loss:  1.69353838294
cumulative_train_loss:  1.64425262836
*------------------------------------------------------------------------------*
Epoch 3, batch 759:
batchly_train_loss:  1.71867554791
cumulative_train_loss:  1.64621370661
*------------------------------------------------------------------------------*
Epoch 3, batch 779:
batchly_train_loss:  1.36971376251
cumulative_train_loss:  1.63911486337
*------------------------------------------------------------------------------*
Epoch 3, batch 799:
batchly_train_loss:  1.56123077307
cumulative_train_loss:  1.63716532419
*------------------------------------------------------------------------------*
Epoch 3, batch 819:
batchly_train_loss:  1.46119592009
cumulative_train_loss:  1.63286814704
*------------------------------------------------------------------------------*
Epoch 3, batch 839:
batchly_train_loss:  1.71567501328
cumulative_train_loss:  1.63484208903
*------------------------------------------------------------------------------*
Epoch 3, batch 859:
batchly_train_loss:  1.48575836152
cumulative_train_loss:  1.63137098944
*------------------------------------------------------------------------------*
Epoch 3, batch 879:
batchly_train_loss:  1.30085271509
cumulative_train_loss:  1.62385066465
*------------------------------------------------------------------------------*
Epoch 3, batch 899:
batchly_train_loss:  1.49218388062
cumulative_train_loss:  1.62092148147
*------------------------------------------------------------------------------*
Epoch 3, batch 919:
batchly_train_loss:  1.48310866331
cumulative_train_loss:  1.61792229065
*------------------------------------------------------------------------------*
Epoch 3, batch 939:
batchly_train_loss:  1.70438532322
cumulative_train_loss:  1.61976388879
*------------------------------------------------------------------------------*
Epoch 3, batch 959:
batchly_train_loss:  1.64082040691
cumulative_train_loss:  1.62020302368
*------------------------------------------------------------------------------*
Epoch 3, batch 979:
batchly_train_loss:  1.87629937441
cumulative_train_loss:  1.62543481838
*------------------------------------------------------------------------------*
Epoch 3, batch 999:
batchly_train_loss:  1.35070858166
cumulative_train_loss:  1.61993479362
*------------------------------------------------------------------------------*
Epoch 3, batch 1019:
batchly_train_loss:  1.52511462669
cumulative_train_loss:  1.61807375011
*------------------------------------------------------------------------------*
Epoch 3, batch 1039:
batchly_train_loss:  1.67058529681
cumulative_train_loss:  1.61908455948
*------------------------------------------------------------------------------*
Epoch 3, batch 1059:
batchly_train_loss:  1.48743902775
cumulative_train_loss:  1.61659833603
*------------------------------------------------------------------------------*
Epoch 3, batch 1079:
batchly_train_loss:  1.84938439303
cumulative_train_loss:  1.62091318417
*------------------------------------------------------------------------------*
Epoch 3, batch 1099:
batchly_train_loss:  1.75813845315
cumulative_train_loss:  1.62341045931
*------------------------------------------------------------------------------*
Epoch 3, batch 1119:
batchly_train_loss:  1.25145435327
cumulative_train_loss:  1.61676245026
*------------------------------------------------------------------------------*
Epoch 3, batch 1139:
batchly_train_loss:  1.4901060403
cumulative_train_loss:  1.61453845711
*------------------------------------------------------------------------------*
Epoch 3, batch 1159:
batchly_train_loss:  1.56126941639
cumulative_train_loss:  1.61361923294
*------------------------------------------------------------------------------*
Epoch 3, batch 1179:
batchly_train_loss:  1.88633583092
cumulative_train_loss:  1.6182454687
*------------------------------------------------------------------------------*
Epoch 3, batch 1199:
batchly_train_loss:  1.72096362314
cumulative_train_loss:  1.61995886577
*------------------------------------------------------------------------------*
Epoch 3, batch 1219:
batchly_train_loss:  1.35604013571
cumulative_train_loss:  1.61562877996
*------------------------------------------------------------------------------*
Epoch 3, batch 1239:
batchly_train_loss:  1.78038474247
cumulative_train_loss:  1.61828827895
*------------------------------------------------------------------------------*
Epoch 3, batch 1259:
batchly_train_loss:  1.6314052629
cumulative_train_loss:  1.61849665042
*------------------------------------------------------------------------------*
Epoch 3, batch 1279:
batchly_train_loss:  1.874175511
cumulative_train_loss:  1.62249475614
*------------------------------------------------------------------------------*
Epoch 3, batch 1299:
batchly_train_loss:  1.61599539582
cumulative_train_loss:  1.622394689
*------------------------------------------------------------------------------*
Epoch 3, batch 1319:
batchly_train_loss:  1.88860644786
cumulative_train_loss:  1.62643125851
*------------------------------------------------------------------------------*
Epoch 3, batch 1339:
batchly_train_loss:  1.35667766529
cumulative_train_loss:  1.62240207863
*------------------------------------------------------------------------------*
Epoch 3, batch 1359:
batchly_train_loss:  1.40629565597
cumulative_train_loss:  1.61922170449
*------------------------------------------------------------------------------*
Epoch 3, batch 1379:
batchly_train_loss:  1.65329941471
cumulative_train_loss:  1.61971594249
*------------------------------------------------------------------------------*
Epoch 3, batch 1399:
batchly_train_loss:  1.48222574891
cumulative_train_loss:  1.6177503929
*------------------------------------------------------------------------------*
Epoch 3, batch 1419:
batchly_train_loss:  1.35691909268
cumulative_train_loss:  1.6140741237
*------------------------------------------------------------------------------*
Epoch 3, batch 1439:
batchly_train_loss:  1.47071084873
cumulative_train_loss:  1.61208158339
*------------------------------------------------------------------------------*
Epoch 3, batch 1459:
batchly_train_loss:  1.54766558292
cumulative_train_loss:  1.61119856762
*------------------------------------------------------------------------------*
Epoch 3, batch 1479:
batchly_train_loss:  1.85081918278
cumulative_train_loss:  1.61443887344
*------------------------------------------------------------------------------*
Epoch 3, batch 1499:
batchly_train_loss:  1.41235850966
cumulative_train_loss:  1.61174267112
*------------------------------------------------------------------------------*
Epoch 3, batch 1519:
batchly_train_loss:  2.14126422944
cumulative_train_loss:  1.61871464687
*------------------------------------------------------------------------------*
Epoch 3, batch 1539:
batchly_train_loss:  1.63822112219
cumulative_train_loss:  1.61896814233
*------------------------------------------------------------------------------*
Epoch 3, batch 1559:
batchly_train_loss:  1.64547950825
cumulative_train_loss:  1.61930824965
*------------------------------------------------------------------------------*
Epoch 3, batch 1579:
batchly_train_loss:  1.82515467101
cumulative_train_loss:  1.62191555074
*------------------------------------------------------------------------------*
Epoch 3, batch 1599:
batchly_train_loss:  2.13546613551
cumulative_train_loss:  1.62833894768
*------------------------------------------------------------------------------*
Epoch 3, batch 1619:
batchly_train_loss:  1.43442643231
cumulative_train_loss:  1.62594348733
*------------------------------------------------------------------------------*
Epoch 3, batch 1639:
batchly_train_loss:  1.82612533013
cumulative_train_loss:  1.62838621878
*------------------------------------------------------------------------------*
Epoch 3, batch 1659:
batchly_train_loss:  1.766914279
cumulative_train_loss:  1.63005623759
*------------------------------------------------------------------------------*
Epoch 3, batch 1679:
batchly_train_loss:  1.89783247148
cumulative_train_loss:  1.63324594854
*------------------------------------------------------------------------------*
Epoch 3, batch 1699:
batchly_train_loss:  1.67940011343
cumulative_train_loss:  1.63378925831
*------------------------------------------------------------------------------*
Epoch 3, batch 1719:
batchly_train_loss:  1.6225543371
cumulative_train_loss:  1.63365854369
*------------------------------------------------------------------------------*
Epoch 3, batch 1739:
batchly_train_loss:  1.60628919685
cumulative_train_loss:  1.63334377259
*------------------------------------------------------------------------------*
Epoch 3, batch 1759:
batchly_train_loss:  1.60772516993
cumulative_train_loss:  1.63305248661
*------------------------------------------------------------------------------*
Epoch 3, batch 1779:
batchly_train_loss:  1.69181145918
cumulative_train_loss:  1.63371307089
*------------------------------------------------------------------------------*
Epoch 3, batch 1799:
batchly_train_loss:  1.50147047687
cumulative_train_loss:  1.63224289197
*------------------------------------------------------------------------------*
Epoch 3, batch 1819:
batchly_train_loss:  1.88527173924
cumulative_train_loss:  1.63502495736
*------------------------------------------------------------------------------*
Epoch 3, batch 1839:
batchly_train_loss:  1.50195889147
cumulative_train_loss:  1.63357780058
*------------------------------------------------------------------------------*
Epoch 3, batch 1859:
batchly_train_loss:  1.38801349926
cumulative_train_loss:  1.63093590385
*------------------------------------------------------------------------------*
Epoch 3, batch 1879:
batchly_train_loss:  1.92693139212
cumulative_train_loss:  1.63408646786
*------------------------------------------------------------------------------*
Epoch 3, batch 1899:
batchly_train_loss:  1.48205484148
cumulative_train_loss:  1.63248529222
*------------------------------------------------------------------------------*
Epoch 3, batch 1919:
batchly_train_loss:  1.79094196897
cumulative_train_loss:  1.63413674274
*------------------------------------------------------------------------------*
Epoch 3, batch 1939:
batchly_train_loss:  1.57758855511
cumulative_train_loss:  1.63355347107
*------------------------------------------------------------------------------*
Epoch 3, batch 1959:
batchly_train_loss:  1.79109277244
cumulative_train_loss:  1.63516183556
*------------------------------------------------------------------------------*
Epoch 3, batch 1979:
batchly_train_loss:  1.5527463714
cumulative_train_loss:  1.63432893547
*------------------------------------------------------------------------------*
Epoch 3, batch 1999:
batchly_train_loss:  1.47137383392
cumulative_train_loss:  1.63269856927
*------------------------------------------------------------------------------*
Epoch 3, batch 2019:
batchly_train_loss:  1.68052828987
cumulative_train_loss:  1.63317236541
*------------------------------------------------------------------------------*
Epoch 3, batch 2039:
batchly_train_loss:  1.55018337544
cumulative_train_loss:  1.63235834883
*------------------------------------------------------------------------------*
Epoch 3, batch 2059:
batchly_train_loss:  1.37213268647
cumulative_train_loss:  1.62983065906
*------------------------------------------------------------------------------*
Epoch 3, batch 2079:
batchly_train_loss:  1.95433609481
cumulative_train_loss:  1.63295240447
*------------------------------------------------------------------------------*
Epoch 3, batch 2099:
batchly_train_loss:  1.5290870735
cumulative_train_loss:  1.63196273958
*------------------------------------------------------------------------------*
Epoch 3, batch 2119:
batchly_train_loss:  1.49941072041
cumulative_train_loss:  1.6307116587
*------------------------------------------------------------------------------*
Epoch 3, batch 2139:
batchly_train_loss:  1.83909525429
cumulative_train_loss:  1.63266007941
*------------------------------------------------------------------------------*
Epoch 3, batch 2159:
batchly_train_loss:  1.47260809763
cumulative_train_loss:  1.63117743021
*------------------------------------------------------------------------------*
Epoch 3, batch 2179:
batchly_train_loss:  1.50074648157
cumulative_train_loss:  1.62998026684
*------------------------------------------------------------------------------*
Epoch 3, batch 2199:
batchly_train_loss:  1.50335397281
cumulative_train_loss:  1.62882859523
*------------------------------------------------------------------------------*
Epoch 3, batch 2219:
batchly_train_loss:  1.65007809294
cumulative_train_loss:  1.62902011841
*------------------------------------------------------------------------------*
Epoch 3, batch 2239:
batchly_train_loss:  1.32732665628
cumulative_train_loss:  1.62632522371
*------------------------------------------------------------------------------*
Epoch 3, batch 2259:
batchly_train_loss:  1.38658016438
cumulative_train_loss:  1.62420264682
*------------------------------------------------------------------------------*
Epoch 3, batch 2279:
batchly_train_loss:  1.34953344985
cumulative_train_loss:  1.62179221069
*------------------------------------------------------------------------------*
Epoch 3, batch 2299:
batchly_train_loss:  1.7193008894
cumulative_train_loss:  1.62264048106
*------------------------------------------------------------------------------*
Epoch 3, batch 2319:
batchly_train_loss:  1.37764558237
cumulative_train_loss:  1.62052754532
*------------------------------------------------------------------------------*
Epoch 3, batch 2339:
batchly_train_loss:  1.60607695231
cumulative_train_loss:  1.62040398318
*------------------------------------------------------------------------------*
Epoch 3, batch 2359:
batchly_train_loss:  1.63017375103
cumulative_train_loss:  1.62048681292
*------------------------------------------------------------------------------*
Epoch 3, batch 2379:
batchly_train_loss:  1.68553669556
cumulative_train_loss:  1.62103368036
*------------------------------------------------------------------------------*
Epoch 3, batch 2399:
batchly_train_loss:  1.86449700164
cumulative_train_loss:  1.62306338709
*------------------------------------------------------------------------------*
Epoch 3, batch 2419:
batchly_train_loss:  1.59147851371
cumulative_train_loss:  1.62280224717
*------------------------------------------------------------------------------*
Epoch 3, batch 2439:
batchly_train_loss:  1.75701278344
cumulative_train_loss:  1.62390278457
*------------------------------------------------------------------------------*
Epoch 3, batch 2459:
batchly_train_loss:  1.4838437687
cumulative_train_loss:  1.62276363031
*------------------------------------------------------------------------------*
Epoch 3, batch 2479:
batchly_train_loss:  1.75003590093
cumulative_train_loss:  1.62379043362
*------------------------------------------------------------------------------*
Epoch 3, batch 2499:
batchly_train_loss:  2.25092111678
cumulative_train_loss:  1.62880948671
*------------------------------------------------------------------------------*
Epoch 3, batch 2519:
batchly_train_loss:  1.56501298575
cumulative_train_loss:  1.62830296427
*------------------------------------------------------------------------------*
Epoch 3, batch 2539:
batchly_train_loss:  1.69350280232
cumulative_train_loss:  1.62881655102
*------------------------------------------------------------------------------*
Epoch 3, batch 2559:
batchly_train_loss:  1.88575993204
cumulative_train_loss:  1.63082470562
*------------------------------------------------------------------------------*
Epoch 3, batch 2579:
batchly_train_loss:  1.70558283179
cumulative_train_loss:  1.63140445069
*------------------------------------------------------------------------------*
Epoch 3, batch 2599:
batchly_train_loss:  1.00603839539
cumulative_train_loss:  1.62659209166
*------------------------------------------------------------------------------*
Epoch 3, batch 2619:
batchly_train_loss:  1.75572201228
cumulative_train_loss:  1.62757819262
*------------------------------------------------------------------------------*
Epoch 3, batch 2639:
batchly_train_loss:  1.74696752042
cumulative_train_loss:  1.62848299996
*------------------------------------------------------------------------------*
Epoch 3, batch 2659:
batchly_train_loss:  1.56789769053
cumulative_train_loss:  1.6280273
*------------------------------------------------------------------------------*
Epoch 3, batch 2679:
batchly_train_loss:  1.67268411909
cumulative_train_loss:  1.62836068424
*------------------------------------------------------------------------------*
Epoch 3, batch 2699:
batchly_train_loss:  1.56946338059
cumulative_train_loss:  1.62792424627
*------------------------------------------------------------------------------*
Epoch 3, batch 2719:
batchly_train_loss:  1.44405969067
cumulative_train_loss:  1.62657180379
*------------------------------------------------------------------------------*
Epoch 3, batch 2739:
batchly_train_loss:  2.01784758618
cumulative_train_loss:  1.62942887413
*------------------------------------------------------------------------------*
Epoch 3, batch 2759:
batchly_train_loss:  1.80806565086
cumulative_train_loss:  1.6307238127
*------------------------------------------------------------------------------*
Epoch 3, batch 2779:
batchly_train_loss:  1.33924818939
cumulative_train_loss:  1.6286261112
*------------------------------------------------------------------------------*
Epoch 3, batch 2799:
batchly_train_loss:  1.31359020406
cumulative_train_loss:  1.62637505077
*------------------------------------------------------------------------------*
Epoch 3, batch 2819:
batchly_train_loss:  1.69022276804
cumulative_train_loss:  1.6268280321
*------------------------------------------------------------------------------*
Epoch 3, batch 2839:
batchly_train_loss:  1.68159996734
cumulative_train_loss:  1.62721388581
*------------------------------------------------------------------------------*
Epoch 3, batch 2859:
batchly_train_loss:  1.56800207114
cumulative_train_loss:  1.62679967235
*------------------------------------------------------------------------------*
Epoch 3, batch 2879:
batchly_train_loss:  1.73630290002
cumulative_train_loss:  1.62756037556
*------------------------------------------------------------------------------*
Epoch 3, batch 2899:
batchly_train_loss:  1.46737097305
cumulative_train_loss:  1.62645523998
*------------------------------------------------------------------------------*
Epoch 3, batch 2919:
batchly_train_loss:  1.54607048664
cumulative_train_loss:  1.62590447086
*------------------------------------------------------------------------------*
Epoch 3, batch 2939:
batchly_train_loss:  1.0626559466
cumulative_train_loss:  1.62207154453
*------------------------------------------------------------------------------*
Epoch 3, batch 2959:
batchly_train_loss:  1.55205072731
cumulative_train_loss:  1.62159827101
*------------------------------------------------------------------------------*
Epoch 3, batch 2979:
batchly_train_loss:  1.80364043797
cumulative_train_loss:  1.62282044064
*------------------------------------------------------------------------------*
Epoch 3, batch 2999:
batchly_train_loss:  1.72795093414
cumulative_train_loss:  1.6235215443
*------------------------------------------------------------------------------*
Epoch 3, batch 3019:
batchly_train_loss:  1.97413125758
cumulative_train_loss:  1.62584423204
*------------------------------------------------------------------------------*
Epoch 3, batch 3039:
batchly_train_loss:  1.51214682872
cumulative_train_loss:  1.62509597667
*------------------------------------------------------------------------------*
Epoch 3, batch 3059:
batchly_train_loss:  1.65956989888
cumulative_train_loss:  1.62532137008
*------------------------------------------------------------------------------*
Epoch 3, batch 3079:
batchly_train_loss:  1.85211246204
cumulative_train_loss:  1.6267945178
*------------------------------------------------------------------------------*
Epoch 3, batch 3099:
batchly_train_loss:  1.53804960809
cumulative_train_loss:  1.62622178524
*------------------------------------------------------------------------------*
Epoch 3, batch 3119:
batchly_train_loss:  1.58863801921
cumulative_train_loss:  1.62598078642
*------------------------------------------------------------------------------*
Epoch 3, batch 3139:
batchly_train_loss:  1.74458750191
cumulative_train_loss:  1.62673648388
*------------------------------------------------------------------------------*
Epoch 3, batch 3159:
batchly_train_loss:  1.65883792006
cumulative_train_loss:  1.62693972184
*------------------------------------------------------------------------------*
Epoch 3, batch 3179:
batchly_train_loss:  1.71331742094
cumulative_train_loss:  1.6274831487
*------------------------------------------------------------------------------*
Epoch 3, batch 3199:
batchly_train_loss:  1.78441132559
cumulative_train_loss:  1.6284642564
*------------------------------------------------------------------------------*
Epoch 3, batch 3219:
batchly_train_loss:  1.78684999453
cumulative_train_loss:  1.62944832436
*------------------------------------------------------------------------------*
Epoch 3, batch 3239:
batchly_train_loss:  1.59422168922
cumulative_train_loss:  1.62923080886
*------------------------------------------------------------------------------*
Epoch 3, batch 3259:
batchly_train_loss:  1.71077796489
cumulative_train_loss:  1.62973125167
*------------------------------------------------------------------------------*
Epoch 3, batch 3279:
batchly_train_loss:  1.72432729697
cumulative_train_loss:  1.63030823273
*------------------------------------------------------------------------------*
Epoch 3, batch 3299:
batchly_train_loss:  1.73516504777
cumulative_train_loss:  1.63094392122
*------------------------------------------------------------------------------*
Epoch 3, batch 3319:
batchly_train_loss:  1.34801187093
cumulative_train_loss:  1.62923899774
*------------------------------------------------------------------------------*
Epoch 3, batch 3339:
batchly_train_loss:  1.90604550237
cumulative_train_loss:  1.63089701814
*------------------------------------------------------------------------------*
Epoch 3, batch 3359:
batchly_train_loss:  1.76927752066
cumulative_train_loss:  1.63172095682
*------------------------------------------------------------------------------*
Epoch 3, batch 3379:
batchly_train_loss:  1.83812594737
cumulative_train_loss:  1.63294264958
*------------------------------------------------------------------------------*
Epoch 3, batch 3399:
batchly_train_loss:  1.82143084929
cumulative_train_loss:  1.63405172989
*------------------------------------------------------------------------------*
Epoch 3, batch 3419:
batchly_train_loss:  1.53564260533
cumulative_train_loss:  1.63347606961
*------------------------------------------------------------------------------*
Epoch 3, batch 3439:
batchly_train_loss:  1.53434436112
cumulative_train_loss:  1.63289955488
*------------------------------------------------------------------------------*
Epoch 3, batch 3459:
batchly_train_loss:  1.96887397415
cumulative_train_loss:  1.63484216499
*------------------------------------------------------------------------------*
Epoch 3, batch 3479:
batchly_train_loss:  1.73422175101
cumulative_train_loss:  1.63541347621
*------------------------------------------------------------------------------*
Epoch 3, batch 3499:
batchly_train_loss:  1.62154295615
cumulative_train_loss:  1.63533419344
*------------------------------------------------------------------------------*
Epoch 3, batch 3519:
batchly_train_loss:  1.26138534504
cumulative_train_loss:  1.6332088803
*------------------------------------------------------------------------------*
Epoch 3, batch 3539:
batchly_train_loss:  1.42010629646
cumulative_train_loss:  1.63200457069
*------------------------------------------------------------------------------*
Epoch 3, batch 3559:
batchly_train_loss:  1.81315486067
cumulative_train_loss:  1.6330225549
*------------------------------------------------------------------------------*
Epoch 3, batch 3579:
batchly_train_loss:  1.98035126846
cumulative_train_loss:  1.63496348094
*------------------------------------------------------------------------------*
Epoch 3, batch 3599:
batchly_train_loss:  1.68946437095
cumulative_train_loss:  1.63526634779
*------------------------------------------------------------------------------*
Epoch 3, batch 3619:
batchly_train_loss:  1.66450384421
cumulative_train_loss:  1.63542792555
*------------------------------------------------------------------------------*
Epoch 3, batch 3639:
batchly_train_loss:  1.56502160847
cumulative_train_loss:  1.63504097135
*------------------------------------------------------------------------------*
Epoch 3, batch 3659:
batchly_train_loss:  1.93911390157
cumulative_train_loss:  1.63670302618
*------------------------------------------------------------------------------*
Epoch 3, batch 3679:
batchly_train_loss:  1.67305508667
cumulative_train_loss:  1.63690064542
*------------------------------------------------------------------------------*
Epoch 3, batch 3699:
batchly_train_loss:  1.47234280876
cumulative_train_loss:  1.63601090313
*------------------------------------------------------------------------------*
Epoch 3, batch 3719:
batchly_train_loss:  1.53110500722
cumulative_train_loss:  1.63544674128
*------------------------------------------------------------------------------*
Epoch 3, batch 3739:
batchly_train_loss:  1.76288292516
cumulative_train_loss:  1.63612840046
*------------------------------------------------------------------------------*
Epoch 3, batch 3759:
batchly_train_loss:  1.67312612592
cumulative_train_loss:  1.63632524923
*------------------------------------------------------------------------------*
Epoch 3, batch 3779:
batchly_train_loss:  1.45771457842
cumulative_train_loss:  1.63537996915
*------------------------------------------------------------------------------*
Epoch 3, batch 3799:
batchly_train_loss:  1.63943815683
cumulative_train_loss:  1.63540133365
*------------------------------------------------------------------------------*
Epoch 3, batch 3819:
batchly_train_loss:  1.47520724675
cumulative_train_loss:  1.63456240154
*------------------------------------------------------------------------------*
Epoch 3, batch 3839:
batchly_train_loss:  1.49093189464
cumulative_train_loss:  1.63381413112
*------------------------------------------------------------------------------*
Epoch 3, batch 3859:
batchly_train_loss:  1.1184355789
cumulative_train_loss:  1.63114308395
*------------------------------------------------------------------------------*
Epoch 3, batch 3879:
batchly_train_loss:  1.72550632035
cumulative_train_loss:  1.63162961778
*------------------------------------------------------------------------------*
Epoch 3, batch 3899:
batchly_train_loss:  1.32709795
cumulative_train_loss:  1.63006751638
*------------------------------------------------------------------------------*
Epoch 3, batch 3919:
batchly_train_loss:  1.17279268811
cumulative_train_loss:  1.62773388623
*------------------------------------------------------------------------------*
Epoch 3, batch 3939:
batchly_train_loss:  1.7013703999
cumulative_train_loss:  1.62810777053
*------------------------------------------------------------------------------*
Epoch 3, batch 3959:
batchly_train_loss:  1.51391852709
cumulative_train_loss:  1.62753091151
*------------------------------------------------------------------------------*
Epoch 3, batch 3979:
batchly_train_loss:  1.45441874227
cumulative_train_loss:  1.62666078249
*------------------------------------------------------------------------------*
Epoch 3, batch 3999:
batchly_train_loss:  1.9368538804
cumulative_train_loss:  1.62821213581
================================================================================
Epoch 3 of 8 took 6453.623s
  training loss:		1.628055
evaluating model...
VALID_LOSS:  1.84929768249
VALID_ACC:  0.385454545455
FULL_TRAIN_LOSS:  1.66439823495
FULL_TRAIN_ACC:  0.343781094527
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=3
*------------------------------------------------------------------------------*
Epoch 4, batch 19:
batchly_train_loss:  1.65029456043
cumulative_train_loss:  1.59889301665
*------------------------------------------------------------------------------*
Epoch 4, batch 39:
batchly_train_loss:  1.35353211351
cumulative_train_loss:  1.47306691248
*------------------------------------------------------------------------------*
Epoch 4, batch 59:
batchly_train_loss:  1.48862524374
cumulative_train_loss:  1.47834092307
*------------------------------------------------------------------------------*
Epoch 4, batch 79:
batchly_train_loss:  1.66026642108
cumulative_train_loss:  1.52439801118
*------------------------------------------------------------------------------*
Epoch 4, batch 99:
batchly_train_loss:  1.81662519735
cumulative_train_loss:  1.58343380636
*------------------------------------------------------------------------------*
Epoch 4, batch 119:
batchly_train_loss:  1.68048640156
cumulative_train_loss:  1.5997451669
*------------------------------------------------------------------------------*
Epoch 4, batch 139:
batchly_train_loss:  1.82173345054
cumulative_train_loss:  1.63168592714
*------------------------------------------------------------------------------*
Epoch 4, batch 159:
batchly_train_loss:  1.57569628541
cumulative_train_loss:  1.62464320491
*------------------------------------------------------------------------------*
Epoch 4, batch 179:
batchly_train_loss:  1.46146654938
cumulative_train_loss:  1.60641117636
*------------------------------------------------------------------------------*
Epoch 4, batch 199:
batchly_train_loss:  1.40533703994
cumulative_train_loss:  1.58620272043
*------------------------------------------------------------------------------*
Epoch 4, batch 219:
batchly_train_loss:  1.19903843472
cumulative_train_loss:  1.55084525142
*------------------------------------------------------------------------------*
Epoch 4, batch 239:
batchly_train_loss:  1.61055162499
cumulative_train_loss:  1.55584160067
*------------------------------------------------------------------------------*
Epoch 4, batch 259:
batchly_train_loss:  1.4265566689
cumulative_train_loss:  1.54585820826
*------------------------------------------------------------------------------*
Epoch 4, batch 279:
batchly_train_loss:  1.40174611229
cumulative_train_loss:  1.53552759206
*------------------------------------------------------------------------------*
Epoch 4, batch 299:
batchly_train_loss:  1.42348697567
cumulative_train_loss:  1.52803323645
*------------------------------------------------------------------------------*
Epoch 4, batch 319:
batchly_train_loss:  1.06050184584
cumulative_train_loss:  1.49872092356
*------------------------------------------------------------------------------*
Epoch 4, batch 339:
batchly_train_loss:  1.67256103271
cumulative_train_loss:  1.5089769772
*------------------------------------------------------------------------------*
Epoch 4, batch 359:
batchly_train_loss:  1.95371521834
cumulative_train_loss:  1.53375348088
*------------------------------------------------------------------------------*
Epoch 4, batch 379:
batchly_train_loss:  1.3566091922
cumulative_train_loss:  1.52440549731
*------------------------------------------------------------------------------*
Epoch 4, batch 399:
batchly_train_loss:  1.94723919322
cumulative_train_loss:  1.54560016878
*------------------------------------------------------------------------------*
Epoch 4, batch 419:
batchly_train_loss:  1.48384919172
cumulative_train_loss:  1.54265262811
*------------------------------------------------------------------------------*
Epoch 4, batch 439:
batchly_train_loss:  1.92933448092
cumulative_train_loss:  1.56026911343
*------------------------------------------------------------------------------*
Epoch 4, batch 459:
batchly_train_loss:  1.54016604481
cumulative_train_loss:  1.55939316273
*------------------------------------------------------------------------------*
Epoch 4, batch 479:
batchly_train_loss:  1.38415862394
cumulative_train_loss:  1.55207648053
*------------------------------------------------------------------------------*
Epoch 4, batch 499:
batchly_train_loss:  1.90495146825
cumulative_train_loss:  1.56621976661
*------------------------------------------------------------------------------*
Epoch 4, batch 519:
batchly_train_loss:  1.53808675707
cumulative_train_loss:  1.56513564293
*------------------------------------------------------------------------------*
Epoch 4, batch 539:
batchly_train_loss:  1.57651942448
cumulative_train_loss:  1.56555804669
*------------------------------------------------------------------------------*
Epoch 4, batch 559:
batchly_train_loss:  1.41789252901
cumulative_train_loss:  1.56027484391
*------------------------------------------------------------------------------*
Epoch 4, batch 579:
batchly_train_loss:  1.39091645061
cumulative_train_loss:  1.55442481306
*------------------------------------------------------------------------------*
Epoch 4, batch 599:
batchly_train_loss:  1.73492916777
cumulative_train_loss:  1.56045166964
*------------------------------------------------------------------------------*
Epoch 4, batch 619:
batchly_train_loss:  1.47469334198
cumulative_train_loss:  1.55768080284
*------------------------------------------------------------------------------*
Epoch 4, batch 639:
batchly_train_loss:  1.63749684162
cumulative_train_loss:  1.56017895741
*------------------------------------------------------------------------------*
Epoch 4, batch 659:
batchly_train_loss:  1.39151619583
cumulative_train_loss:  1.55506020896
*------------------------------------------------------------------------------*
Epoch 4, batch 679:
batchly_train_loss:  1.66368825743
cumulative_train_loss:  1.55825985693
*------------------------------------------------------------------------------*
Epoch 4, batch 699:
batchly_train_loss:  1.18428344224
cumulative_train_loss:  1.54755953033
*------------------------------------------------------------------------------*
Epoch 4, batch 719:
batchly_train_loss:  1.8751129842
cumulative_train_loss:  1.55667089205
*------------------------------------------------------------------------------*
Epoch 4, batch 739:
batchly_train_loss:  1.68034573368
cumulative_train_loss:  1.56001797842
*------------------------------------------------------------------------------*
Epoch 4, batch 759:
batchly_train_loss:  1.52352568585
cumulative_train_loss:  1.55905638969
*------------------------------------------------------------------------------*
Epoch 4, batch 779:
batchly_train_loss:  1.75263979608
cumulative_train_loss:  1.56402643863
*------------------------------------------------------------------------------*
Epoch 4, batch 799:
batchly_train_loss:  1.54376903584
cumulative_train_loss:  1.56351936973
*------------------------------------------------------------------------------*
Epoch 4, batch 819:
batchly_train_loss:  1.64044092738
cumulative_train_loss:  1.56539779604
*------------------------------------------------------------------------------*
Epoch 4, batch 839:
batchly_train_loss:  1.41000866272
cumulative_train_loss:  1.56169364507
*------------------------------------------------------------------------------*
Epoch 4, batch 859:
batchly_train_loss:  1.68054347023
cumulative_train_loss:  1.56446081213
*------------------------------------------------------------------------------*
Epoch 4, batch 879:
batchly_train_loss:  1.51757352028
cumulative_train_loss:  1.56339397955
*------------------------------------------------------------------------------*
Epoch 4, batch 899:
batchly_train_loss:  1.59899771538
cumulative_train_loss:  1.56418605376
*------------------------------------------------------------------------------*
Epoch 4, batch 919:
batchly_train_loss:  1.24402330592
cumulative_train_loss:  1.55721842051
*------------------------------------------------------------------------------*
Epoch 4, batch 939:
batchly_train_loss:  1.61112688703
cumulative_train_loss:  1.55836663066
*------------------------------------------------------------------------------*
Epoch 4, batch 959:
batchly_train_loss:  1.81301327991
cumulative_train_loss:  1.56367730113
*------------------------------------------------------------------------------*
Epoch 4, batch 979:
batchly_train_loss:  1.09036979242
cumulative_train_loss:  1.55400809769
*------------------------------------------------------------------------------*
Epoch 4, batch 999:
batchly_train_loss:  1.75256825729
cumulative_train_loss:  1.55798327606
*------------------------------------------------------------------------------*
Epoch 4, batch 1019:
batchly_train_loss:  1.80257471737
cumulative_train_loss:  1.56278389316
*------------------------------------------------------------------------------*
Epoch 4, batch 1039:
batchly_train_loss:  1.39704140501
cumulative_train_loss:  1.5595934699
*------------------------------------------------------------------------------*
Epoch 4, batch 1059:
batchly_train_loss:  1.49741170218
cumulative_train_loss:  1.55841912113
*------------------------------------------------------------------------------*
Epoch 4, batch 1079:
batchly_train_loss:  1.45305936536
cumulative_train_loss:  1.55646620628
*------------------------------------------------------------------------------*
Epoch 4, batch 1099:
batchly_train_loss:  1.69352576153
cumulative_train_loss:  1.55896046571
*------------------------------------------------------------------------------*
Epoch 4, batch 1119:
batchly_train_loss:  1.57165843892
cumulative_train_loss:  1.55918741786
*------------------------------------------------------------------------------*
Epoch 4, batch 1139:
batchly_train_loss:  1.56639287409
cumulative_train_loss:  1.55931394036
*------------------------------------------------------------------------------*
Epoch 4, batch 1159:
batchly_train_loss:  0.961732524804
cumulative_train_loss:  1.54900192284
*------------------------------------------------------------------------------*
Epoch 4, batch 1179:
batchly_train_loss:  1.43949115599
cumulative_train_loss:  1.54714423383
*------------------------------------------------------------------------------*
Epoch 4, batch 1199:
batchly_train_loss:  1.34178747659
cumulative_train_loss:  1.54371876666
*------------------------------------------------------------------------------*
Epoch 4, batch 1219:
batchly_train_loss:  1.67384454126
cumulative_train_loss:  1.54585372604
*------------------------------------------------------------------------------*
Epoch 4, batch 1239:
batchly_train_loss:  1.20727550986
cumulative_train_loss:  1.54038837953
*------------------------------------------------------------------------------*
Epoch 4, batch 1259:
batchly_train_loss:  1.5066609503
cumulative_train_loss:  1.53985259829
*------------------------------------------------------------------------------*
Epoch 4, batch 1279:
batchly_train_loss:  1.66824727987
cumulative_train_loss:  1.54186033373
*------------------------------------------------------------------------------*
Epoch 4, batch 1299:
batchly_train_loss:  1.52126216872
cumulative_train_loss:  1.54154319493
*------------------------------------------------------------------------------*
Epoch 4, batch 1319:
batchly_train_loss:  1.17292192932
cumulative_train_loss:  1.53595378985
*------------------------------------------------------------------------------*
Epoch 4, batch 1339:
batchly_train_loss:  2.04376408052
cumulative_train_loss:  1.5435387083
*------------------------------------------------------------------------------*
Epoch 4, batch 1359:
batchly_train_loss:  1.67831322303
cumulative_train_loss:  1.54552214487
*------------------------------------------------------------------------------*
Epoch 4, batch 1379:
batchly_train_loss:  1.73604696102
cumulative_train_loss:  1.54828537643
*------------------------------------------------------------------------------*
Epoch 4, batch 1399:
batchly_train_loss:  1.3574741623
cumulative_train_loss:  1.5455575535
*------------------------------------------------------------------------------*
Epoch 4, batch 1419:
batchly_train_loss:  1.88729294103
cumulative_train_loss:  1.55037411992
*------------------------------------------------------------------------------*
Epoch 4, batch 1439:
batchly_train_loss:  1.59774018595
cumulative_train_loss:  1.55103243911
*------------------------------------------------------------------------------*
Epoch 4, batch 1459:
batchly_train_loss:  1.30072500837
cumulative_train_loss:  1.54760122005
*------------------------------------------------------------------------------*
Epoch 4, batch 1479:
batchly_train_loss:  1.36740292822
cumulative_train_loss:  1.54516446154
*------------------------------------------------------------------------------*
Epoch 4, batch 1499:
batchly_train_loss:  1.44870205484
cumulative_train_loss:  1.5438774381
*------------------------------------------------------------------------------*
Epoch 4, batch 1519:
batchly_train_loss:  2.10945068594
cumulative_train_loss:  1.55132409047
*------------------------------------------------------------------------------*
Epoch 4, batch 1539:
batchly_train_loss:  1.43374875215
cumulative_train_loss:  1.54979614586
*------------------------------------------------------------------------------*
Epoch 4, batch 1559:
batchly_train_loss:  1.83734968948
cumulative_train_loss:  1.55348509446
*------------------------------------------------------------------------------*
Epoch 4, batch 1579:
batchly_train_loss:  1.62549474581
cumulative_train_loss:  1.55439718631
*------------------------------------------------------------------------------*
Epoch 4, batch 1599:
batchly_train_loss:  1.68101338774
cumulative_train_loss:  1.55598087863
*------------------------------------------------------------------------------*
Epoch 4, batch 1619:
batchly_train_loss:  1.27121408484
cumulative_train_loss:  1.55246306772
*------------------------------------------------------------------------------*
Epoch 4, batch 1639:
batchly_train_loss:  1.16361293694
cumulative_train_loss:  1.54771809968
*------------------------------------------------------------------------------*
Epoch 4, batch 1659:
batchly_train_loss:  1.49714916198
cumulative_train_loss:  1.54710846812
*------------------------------------------------------------------------------*
Epoch 4, batch 1679:
batchly_train_loss:  1.63686654691
cumulative_train_loss:  1.5481776531
*------------------------------------------------------------------------------*
Epoch 4, batch 1699:
batchly_train_loss:  1.75379502024
cumulative_train_loss:  1.55059810474
*------------------------------------------------------------------------------*
Epoch 4, batch 1719:
batchly_train_loss:  1.64470345614
cumulative_train_loss:  1.55169298957
*------------------------------------------------------------------------------*
Epoch 4, batch 1739:
batchly_train_loss:  1.63446327469
cumulative_train_loss:  1.55264491925
*------------------------------------------------------------------------------*
Epoch 4, batch 1759:
batchly_train_loss:  1.55477176701
cumulative_train_loss:  1.55266910171
*------------------------------------------------------------------------------*
Epoch 4, batch 1779:
batchly_train_loss:  1.54339644848
cumulative_train_loss:  1.55256485603
*------------------------------------------------------------------------------*
Epoch 4, batch 1799:
batchly_train_loss:  1.4833200918
cumulative_train_loss:  1.55179504209
*------------------------------------------------------------------------------*
Epoch 4, batch 1819:
batchly_train_loss:  1.52680950892
cumulative_train_loss:  1.55152032484
*------------------------------------------------------------------------------*
Epoch 4, batch 1839:
batchly_train_loss:  1.47590490659
cumulative_train_loss:  1.55069797119
*------------------------------------------------------------------------------*
Epoch 4, batch 1859:
batchly_train_loss:  1.58771962859
cumulative_train_loss:  1.55109626767
*------------------------------------------------------------------------------*
Epoch 4, batch 1879:
batchly_train_loss:  1.33753575921
cumulative_train_loss:  1.54882313825
*------------------------------------------------------------------------------*
Epoch 4, batch 1899:
batchly_train_loss:  1.61462804187
cumulative_train_loss:  1.54951618621
*------------------------------------------------------------------------------*
Epoch 4, batch 1919:
batchly_train_loss:  1.37198359559
cumulative_train_loss:  1.54766592472
*------------------------------------------------------------------------------*
Epoch 4, batch 1939:
batchly_train_loss:  1.86303585829
cumulative_train_loss:  1.5509188379
*------------------------------------------------------------------------------*
Epoch 4, batch 1959:
batchly_train_loss:  2.15737115481
cumulative_train_loss:  1.55711028575
*------------------------------------------------------------------------------*
Epoch 4, batch 1979:
batchly_train_loss:  1.4709286429
cumulative_train_loss:  1.55623932423
*------------------------------------------------------------------------------*
Epoch 4, batch 1999:
batchly_train_loss:  1.40328793187
cumulative_train_loss:  1.55470904517
*------------------------------------------------------------------------------*
Epoch 4, batch 2019:
batchly_train_loss:  1.11338168658
cumulative_train_loss:  1.55033730313
*------------------------------------------------------------------------------*
Epoch 4, batch 2039:
batchly_train_loss:  1.57748722094
cumulative_train_loss:  1.55060360934
*------------------------------------------------------------------------------*
Epoch 4, batch 2059:
batchly_train_loss:  1.45700826527
cumulative_train_loss:  1.54969447535
*------------------------------------------------------------------------------*
Epoch 4, batch 2079:
batchly_train_loss:  1.66538078952
cumulative_train_loss:  1.5508073788
*------------------------------------------------------------------------------*
Epoch 4, batch 2099:
batchly_train_loss:  1.4350542961
cumulative_train_loss:  1.54970444329
*------------------------------------------------------------------------------*
Epoch 4, batch 2119:
batchly_train_loss:  1.5448098915
cumulative_train_loss:  1.54965824648
*------------------------------------------------------------------------------*
Epoch 4, batch 2139:
batchly_train_loss:  1.58828463922
cumulative_train_loss:  1.55001940957
*------------------------------------------------------------------------------*
Epoch 4, batch 2159:
batchly_train_loss:  1.13691969877
cumulative_train_loss:  1.5461926406
*------------------------------------------------------------------------------*
Epoch 4, batch 2179:
batchly_train_loss:  1.52329101586
cumulative_train_loss:  1.54598243752
*------------------------------------------------------------------------------*
Epoch 4, batch 2199:
batchly_train_loss:  1.54833532978
cumulative_train_loss:  1.54600383718
*------------------------------------------------------------------------------*
Epoch 4, batch 2219:
batchly_train_loss:  1.76436610137
cumulative_train_loss:  1.54797195132
*------------------------------------------------------------------------------*
Epoch 4, batch 2239:
batchly_train_loss:  1.50477404229
cumulative_train_loss:  1.54758608344
*------------------------------------------------------------------------------*
Epoch 4, batch 2259:
batchly_train_loss:  1.30076928449
cumulative_train_loss:  1.54540089709
*------------------------------------------------------------------------------*
Epoch 4, batch 2279:
batchly_train_loss:  1.0809787785
cumulative_train_loss:  1.54132523128
*------------------------------------------------------------------------------*
Epoch 4, batch 2299:
batchly_train_loss:  1.42552954112
cumulative_train_loss:  1.54031787426
*------------------------------------------------------------------------------*
Epoch 4, batch 2319:
batchly_train_loss:  1.31477106011
cumulative_train_loss:  1.53837266672
*------------------------------------------------------------------------------*
Epoch 4, batch 2339:
batchly_train_loss:  1.77901170654
cumulative_train_loss:  1.54043028997
*------------------------------------------------------------------------------*
Epoch 4, batch 2359:
batchly_train_loss:  1.54216532339
cumulative_train_loss:  1.54044499988
*------------------------------------------------------------------------------*
Epoch 4, batch 2379:
batchly_train_loss:  1.45378770538
cumulative_train_loss:  1.53971648122
*------------------------------------------------------------------------------*
Epoch 4, batch 2399:
batchly_train_loss:  1.32308804183
cumulative_train_loss:  1.53791049173
*------------------------------------------------------------------------------*
Epoch 4, batch 2419:
batchly_train_loss:  1.03094100662
cumulative_train_loss:  1.53371892922
*------------------------------------------------------------------------------*
Epoch 4, batch 2439:
batchly_train_loss:  2.13096848095
cumulative_train_loss:  1.53861642452
*------------------------------------------------------------------------------*
Epoch 4, batch 2459:
batchly_train_loss:  1.72150654317
cumulative_train_loss:  1.54010394074
*------------------------------------------------------------------------------*
Epoch 4, batch 2479:
batchly_train_loss:  1.4676680542
cumulative_train_loss:  1.53951954472
*------------------------------------------------------------------------------*
Epoch 4, batch 2499:
batchly_train_loss:  1.48345528613
cumulative_train_loss:  1.53907085117
*------------------------------------------------------------------------------*
Epoch 4, batch 2519:
batchly_train_loss:  1.30049472501
cumulative_train_loss:  1.53717663818
*------------------------------------------------------------------------------*
Epoch 4, batch 2539:
batchly_train_loss:  1.28996077674
cumulative_train_loss:  1.53522928992
*------------------------------------------------------------------------------*
Epoch 4, batch 2559:
batchly_train_loss:  1.43187618814
cumulative_train_loss:  1.53442152828
*------------------------------------------------------------------------------*
Epoch 4, batch 2579:
batchly_train_loss:  1.77502575313
cumulative_train_loss:  1.53628740052
*------------------------------------------------------------------------------*
Epoch 4, batch 2599:
batchly_train_loss:  1.67848733314
cumulative_train_loss:  1.53738166703
*------------------------------------------------------------------------------*
Epoch 4, batch 2619:
batchly_train_loss:  1.69485426253
cumulative_train_loss:  1.53858420689
*------------------------------------------------------------------------------*
Epoch 4, batch 2639:
batchly_train_loss:  1.51276416673
cumulative_train_loss:  1.53838852641
*------------------------------------------------------------------------------*
Epoch 4, batch 2659:
batchly_train_loss:  1.63462157652
cumulative_train_loss:  1.53911235529
*------------------------------------------------------------------------------*
Epoch 4, batch 2679:
batchly_train_loss:  1.26299462201
cumulative_train_loss:  1.53705100603
*------------------------------------------------------------------------------*
Epoch 4, batch 2699:
batchly_train_loss:  1.76257700597
cumulative_train_loss:  1.53872218795
*------------------------------------------------------------------------------*
Epoch 4, batch 2719:
batchly_train_loss:  1.47626102688
cumulative_train_loss:  1.53826274579
*------------------------------------------------------------------------------*
Epoch 4, batch 2739:
batchly_train_loss:  1.78190850554
cumulative_train_loss:  1.5400418313
*------------------------------------------------------------------------------*
Epoch 4, batch 2759:
batchly_train_loss:  1.60282609077
cumulative_train_loss:  1.5404969546
*------------------------------------------------------------------------------*
Epoch 4, batch 2779:
batchly_train_loss:  1.45462410222
cumulative_train_loss:  1.53987894199
*------------------------------------------------------------------------------*
Epoch 4, batch 2799:
batchly_train_loss:  1.72162617675
cumulative_train_loss:  1.54117760033
*------------------------------------------------------------------------------*
Epoch 4, batch 2819:
batchly_train_loss:  1.84802571081
cumulative_train_loss:  1.54335460005
*------------------------------------------------------------------------------*
Epoch 4, batch 2839:
batchly_train_loss:  1.52172777266
cumulative_train_loss:  1.5432022448
*------------------------------------------------------------------------------*
Epoch 4, batch 2859:
batchly_train_loss:  1.30997800935
cumulative_train_loss:  1.54157073563
*------------------------------------------------------------------------------*
Epoch 4, batch 2879:
batchly_train_loss:  1.70207400384
cumulative_train_loss:  1.54268572881
*------------------------------------------------------------------------------*
Epoch 4, batch 2899:
batchly_train_loss:  1.36586743881
cumulative_train_loss:  1.54146587169
*------------------------------------------------------------------------------*
Epoch 4, batch 2919:
batchly_train_loss:  1.6345647467
cumulative_train_loss:  1.54210375367
*------------------------------------------------------------------------------*
Epoch 4, batch 2939:
batchly_train_loss:  1.4063165623
cumulative_train_loss:  1.54117971698
*------------------------------------------------------------------------------*
Epoch 4, batch 2959:
batchly_train_loss:  1.42530389818
cumulative_train_loss:  1.54039650766
*------------------------------------------------------------------------------*
Epoch 4, batch 2979:
batchly_train_loss:  1.96303806978
cumulative_train_loss:  1.54323398039
*------------------------------------------------------------------------------*
Epoch 4, batch 2999:
batchly_train_loss:  1.50340160203
cumulative_train_loss:  1.54296834265
*------------------------------------------------------------------------------*
Epoch 4, batch 3019:
batchly_train_loss:  1.67044539633
cumulative_train_loss:  1.54381284119
*------------------------------------------------------------------------------*
Epoch 4, batch 3039:
batchly_train_loss:  1.40356153058
cumulative_train_loss:  1.54288983157
*------------------------------------------------------------------------------*
Epoch 4, batch 3059:
batchly_train_loss:  1.24585626388
cumulative_train_loss:  1.54094780105
*------------------------------------------------------------------------------*
Epoch 4, batch 3079:
batchly_train_loss:  1.64397311086
cumulative_train_loss:  1.54161701385
*------------------------------------------------------------------------------*
Epoch 4, batch 3099:
batchly_train_loss:  1.09607900584
cumulative_train_loss:  1.53874164755
*------------------------------------------------------------------------------*
Epoch 4, batch 3119:
batchly_train_loss:  1.20512238793
cumulative_train_loss:  1.53660237689
*------------------------------------------------------------------------------*
Epoch 4, batch 3139:
batchly_train_loss:  2.60288039385
cumulative_train_loss:  1.54339612023
*------------------------------------------------------------------------------*
Epoch 4, batch 3159:
batchly_train_loss:  1.2506714105
cumulative_train_loss:  1.54154284571
*------------------------------------------------------------------------------*
Epoch 4, batch 3179:
batchly_train_loss:  1.14760139597
cumulative_train_loss:  1.53906444716
*------------------------------------------------------------------------------*
Epoch 4, batch 3199:
batchly_train_loss:  1.31617283021
cumulative_train_loss:  1.53767093908
*------------------------------------------------------------------------------*
Epoch 4, batch 3219:
batchly_train_loss:  1.3818824929
cumulative_train_loss:  1.53670300838
*------------------------------------------------------------------------------*
Epoch 4, batch 3239:
batchly_train_loss:  1.49125951175
cumulative_train_loss:  1.53642240637
*------------------------------------------------------------------------------*
Epoch 4, batch 3259:
batchly_train_loss:  1.5246947677
cumulative_train_loss:  1.53635043559
*------------------------------------------------------------------------------*
Epoch 4, batch 3279:
batchly_train_loss:  1.37987678926
cumulative_train_loss:  1.53539603701
*------------------------------------------------------------------------------*
Epoch 4, batch 3299:
batchly_train_loss:  1.75736228589
cumulative_train_loss:  1.53674169478
*------------------------------------------------------------------------------*
Epoch 4, batch 3319:
batchly_train_loss:  1.41723911103
cumulative_train_loss:  1.5360215828
*------------------------------------------------------------------------------*
Epoch 4, batch 3339:
batchly_train_loss:  1.58075121644
cumulative_train_loss:  1.53628950513
*------------------------------------------------------------------------------*
Epoch 4, batch 3359:
batchly_train_loss:  0.841822391116
cumulative_train_loss:  1.53215454166
*------------------------------------------------------------------------------*
Epoch 4, batch 3379:
batchly_train_loss:  1.53034788062
cumulative_train_loss:  1.5321438482
*------------------------------------------------------------------------------*
Epoch 4, batch 3399:
batchly_train_loss:  1.40192241682
cumulative_train_loss:  1.53137761442
*------------------------------------------------------------------------------*
Epoch 4, batch 3419:
batchly_train_loss:  1.64219530703
cumulative_train_loss:  1.53202586064
*------------------------------------------------------------------------------*
Epoch 4, batch 3439:
batchly_train_loss:  1.75348526282
cumulative_train_loss:  1.53331378971
*------------------------------------------------------------------------------*
Epoch 4, batch 3459:
batchly_train_loss:  1.55464444889
cumulative_train_loss:  1.53343712396
*------------------------------------------------------------------------------*
Epoch 4, batch 3479:
batchly_train_loss:  1.18270564952
cumulative_train_loss:  1.53142084644
*------------------------------------------------------------------------------*
Epoch 4, batch 3499:
batchly_train_loss:  1.30979988794
cumulative_train_loss:  1.53015407903
*------------------------------------------------------------------------------*
Epoch 4, batch 3519:
batchly_train_loss:  1.69561376655
cumulative_train_loss:  1.53109445804
*------------------------------------------------------------------------------*
Epoch 4, batch 3539:
batchly_train_loss:  1.44426893656
cumulative_train_loss:  1.53060377976
*------------------------------------------------------------------------------*
Epoch 4, batch 3559:
batchly_train_loss:  1.86307930023
cumulative_train_loss:  1.53247214459
*------------------------------------------------------------------------------*
Epoch 4, batch 3579:
batchly_train_loss:  1.34942797852
cumulative_train_loss:  1.53144926576
*------------------------------------------------------------------------------*
Epoch 4, batch 3599:
batchly_train_loss:  1.4464691785
cumulative_train_loss:  1.53097702299
*------------------------------------------------------------------------------*
Epoch 4, batch 3619:
batchly_train_loss:  1.51920399628
cumulative_train_loss:  1.53091196067
*------------------------------------------------------------------------------*
Epoch 4, batch 3639:
batchly_train_loss:  1.3347445453
cumulative_train_loss:  1.52983382153
*------------------------------------------------------------------------------*
Epoch 4, batch 3659:
batchly_train_loss:  1.72254862547
cumulative_train_loss:  1.5308871957
*------------------------------------------------------------------------------*
Epoch 4, batch 3679:
batchly_train_loss:  1.26316814677
cumulative_train_loss:  1.52943180538
*------------------------------------------------------------------------------*
Epoch 4, batch 3699:
batchly_train_loss:  1.56120839628
cumulative_train_loss:  1.52960361718
*------------------------------------------------------------------------------*
Epoch 4, batch 3719:
batchly_train_loss:  1.67527379053
cumulative_train_loss:  1.53038700074
*------------------------------------------------------------------------------*
Epoch 4, batch 3739:
batchly_train_loss:  1.59805534826
cumulative_train_loss:  1.53074896034
*------------------------------------------------------------------------------*
Epoch 4, batch 3759:
batchly_train_loss:  1.85388180738
cumulative_train_loss:  1.53246820933
*------------------------------------------------------------------------------*
Epoch 4, batch 3779:
batchly_train_loss:  1.42220127467
cumulative_train_loss:  1.53188463201
*------------------------------------------------------------------------------*
Epoch 4, batch 3799:
batchly_train_loss:  1.58749862088
cumulative_train_loss:  1.53217741426
*------------------------------------------------------------------------------*
Epoch 4, batch 3819:
batchly_train_loss:  1.19550289652
cumulative_train_loss:  1.53041425889
*------------------------------------------------------------------------------*
Epoch 4, batch 3839:
batchly_train_loss:  1.81289099241
cumulative_train_loss:  1.53188587511
*------------------------------------------------------------------------------*
Epoch 4, batch 3859:
batchly_train_loss:  1.4429663831
cumulative_train_loss:  1.53142503296
*------------------------------------------------------------------------------*
Epoch 4, batch 3879:
batchly_train_loss:  1.29100291464
cumulative_train_loss:  1.53018542421
*------------------------------------------------------------------------------*
Epoch 4, batch 3899:
batchly_train_loss:  1.56472366232
cumulative_train_loss:  1.5303625888
*------------------------------------------------------------------------------*
Epoch 4, batch 3919:
batchly_train_loss:  1.39637759558
cumulative_train_loss:  1.52967881747
*------------------------------------------------------------------------------*
Epoch 4, batch 3939:
batchly_train_loss:  1.40694980876
cumulative_train_loss:  1.52905566942
*------------------------------------------------------------------------------*
Epoch 4, batch 3959:
batchly_train_loss:  1.34689716487
cumulative_train_loss:  1.52813544459
*------------------------------------------------------------------------------*
Epoch 4, batch 3979:
batchly_train_loss:  1.33260834175
cumulative_train_loss:  1.5271526494
*------------------------------------------------------------------------------*
Epoch 4, batch 3999:
batchly_train_loss:  1.91151094642
cumulative_train_loss:  1.52907492145
================================================================================
Epoch 4 of 8 took 6370.974s
  training loss:		1.528844
evaluating model...
VALID_LOSS:  1.67717637199
VALID_ACC:  0.46
FULL_TRAIN_LOSS:  1.45310840981
FULL_TRAIN_ACC:  0.516666666667
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=4
*------------------------------------------------------------------------------*
Epoch 5, batch 19:
batchly_train_loss:  1.53013400755
cumulative_train_loss:  1.4982799195
*------------------------------------------------------------------------------*
Epoch 5, batch 39:
batchly_train_loss:  1.22516043084
cumulative_train_loss:  1.35821864326
*------------------------------------------------------------------------------*
Epoch 5, batch 59:
batchly_train_loss:  1.81111871017
cumulative_train_loss:  1.51174408967
*------------------------------------------------------------------------------*
Epoch 5, batch 79:
batchly_train_loss:  1.80008128102
cumulative_train_loss:  1.58474084698
*------------------------------------------------------------------------------*
Epoch 5, batch 99:
batchly_train_loss:  1.45380698324
cumulative_train_loss:  1.55828956137
*------------------------------------------------------------------------------*
Epoch 5, batch 119:
batchly_train_loss:  1.25248438889
cumulative_train_loss:  1.50689373406
*------------------------------------------------------------------------------*
Epoch 5, batch 139:
batchly_train_loss:  1.32998144559
cumulative_train_loss:  1.48143872853
*------------------------------------------------------------------------------*
Epoch 5, batch 159:
batchly_train_loss:  1.55013470677
cumulative_train_loss:  1.49007973208
*------------------------------------------------------------------------------*
Epoch 5, batch 179:
batchly_train_loss:  1.424298911
cumulative_train_loss:  1.48272991967
*------------------------------------------------------------------------------*
Epoch 5, batch 199:
batchly_train_loss:  1.90691357826
cumulative_train_loss:  1.52536144315
*------------------------------------------------------------------------------*
Epoch 5, batch 219:
batchly_train_loss:  1.58297624014
cumulative_train_loss:  1.53062306844
*------------------------------------------------------------------------------*
Epoch 5, batch 239:
batchly_train_loss:  1.70811781419
cumulative_train_loss:  1.54547618524
*------------------------------------------------------------------------------*
Epoch 5, batch 259:
batchly_train_loss:  1.65092296638
cumulative_train_loss:  1.55361879382
*------------------------------------------------------------------------------*
Epoch 5, batch 279:
batchly_train_loss:  1.72306964411
cumulative_train_loss:  1.56576580818
*------------------------------------------------------------------------------*
Epoch 5, batch 299:
batchly_train_loss:  1.60538991014
cumulative_train_loss:  1.56841624978
*------------------------------------------------------------------------------*
Epoch 5, batch 319:
batchly_train_loss:  1.43805900782
cumulative_train_loss:  1.56024338195
*------------------------------------------------------------------------------*
Epoch 5, batch 339:
batchly_train_loss:  1.38501493543
cumulative_train_loss:  1.5499054205
*------------------------------------------------------------------------------*
Epoch 5, batch 359:
batchly_train_loss:  1.44096736571
cumulative_train_loss:  1.54383644809
*------------------------------------------------------------------------------*
Epoch 5, batch 379:
batchly_train_loss:  1.58115072615
cumulative_train_loss:  1.54580553928
*------------------------------------------------------------------------------*
Epoch 5, batch 399:
batchly_train_loss:  1.31925050663
cumulative_train_loss:  1.53444939729
*------------------------------------------------------------------------------*
Epoch 5, batch 419:
batchly_train_loss:  1.361710923
cumulative_train_loss:  1.52620412406
*------------------------------------------------------------------------------*
Epoch 5, batch 439:
batchly_train_loss:  1.48506281397
cumulative_train_loss:  1.52432980469
*------------------------------------------------------------------------------*
Epoch 5, batch 459:
batchly_train_loss:  1.76619625923
cumulative_train_loss:  1.53486864803
*------------------------------------------------------------------------------*
Epoch 5, batch 479:
batchly_train_loss:  1.46726808962
cumulative_train_loss:  1.53204607774
*------------------------------------------------------------------------------*
Epoch 5, batch 499:
batchly_train_loss:  1.2620743369
cumulative_train_loss:  1.52122556708
*------------------------------------------------------------------------------*
Epoch 5, batch 519:
batchly_train_loss:  1.31787636791
cumulative_train_loss:  1.51338937444
*------------------------------------------------------------------------------*
Epoch 5, batch 539:
batchly_train_loss:  1.41169790066
cumulative_train_loss:  1.50961603589
*------------------------------------------------------------------------------*
Epoch 5, batch 559:
batchly_train_loss:  1.29223642091
cumulative_train_loss:  1.50183858992
*------------------------------------------------------------------------------*
Epoch 5, batch 579:
batchly_train_loss:  1.34483673172
cumulative_train_loss:  1.49641538238
*------------------------------------------------------------------------------*
Epoch 5, batch 599:
batchly_train_loss:  1.71079811095
cumulative_train_loss:  1.50357340337
*------------------------------------------------------------------------------*
Epoch 5, batch 619:
batchly_train_loss:  1.2360755004
cumulative_train_loss:  1.49493049859
*------------------------------------------------------------------------------*
Epoch 5, batch 639:
batchly_train_loss:  1.86122721872
cumulative_train_loss:  1.50639518466
*------------------------------------------------------------------------------*
Epoch 5, batch 659:
batchly_train_loss:  1.55319481197
cumulative_train_loss:  1.50781550719
*------------------------------------------------------------------------------*
Epoch 5, batch 679:
batchly_train_loss:  1.75044680563
cumulative_train_loss:  1.51496223174
*------------------------------------------------------------------------------*
Epoch 5, batch 699:
batchly_train_loss:  1.38567765335
cumulative_train_loss:  1.51126310217
*------------------------------------------------------------------------------*
Epoch 5, batch 719:
batchly_train_loss:  1.06473976333
cumulative_train_loss:  1.49884242515
*------------------------------------------------------------------------------*
Epoch 5, batch 739:
batchly_train_loss:  1.47042736063
cumulative_train_loss:  1.49807341123
*------------------------------------------------------------------------------*
Epoch 5, batch 759:
batchly_train_loss:  1.66367390466
cumulative_train_loss:  1.50243706059
*------------------------------------------------------------------------------*
Epoch 5, batch 779:
batchly_train_loss:  1.5239711162
cumulative_train_loss:  1.50298992467
*------------------------------------------------------------------------------*
Epoch 5, batch 799:
batchly_train_loss:  1.39235536141
cumulative_train_loss:  1.50022059893
*------------------------------------------------------------------------------*
Epoch 5, batch 819:
batchly_train_loss:  1.34924346954
cumulative_train_loss:  1.49653373374
*------------------------------------------------------------------------------*
Epoch 5, batch 839:
batchly_train_loss:  1.11890097485
cumulative_train_loss:  1.48753176094
*------------------------------------------------------------------------------*
Epoch 5, batch 859:
batchly_train_loss:  1.37250461651
cumulative_train_loss:  1.48485359693
*------------------------------------------------------------------------------*
Epoch 5, batch 879:
batchly_train_loss:  2.02164545408
cumulative_train_loss:  1.49706729106
*------------------------------------------------------------------------------*
Epoch 5, batch 899:
batchly_train_loss:  1.60980388132
cumulative_train_loss:  1.49957533534
*------------------------------------------------------------------------------*
Epoch 5, batch 919:
batchly_train_loss:  1.50324740601
cumulative_train_loss:  1.49965524983
*------------------------------------------------------------------------------*
Epoch 5, batch 939:
batchly_train_loss:  1.57297732194
cumulative_train_loss:  1.5012169553
*------------------------------------------------------------------------------*
Epoch 5, batch 959:
batchly_train_loss:  1.60840134173
cumulative_train_loss:  1.50345229183
*------------------------------------------------------------------------------*
Epoch 5, batch 979:
batchly_train_loss:  1.77981367767
cumulative_train_loss:  1.50909808112
*------------------------------------------------------------------------------*
Epoch 5, batch 999:
batchly_train_loss:  1.74520929114
cumulative_train_loss:  1.51382503227
*------------------------------------------------------------------------------*
Epoch 5, batch 1019:
batchly_train_loss:  1.58141357977
cumulative_train_loss:  1.51515159846
*------------------------------------------------------------------------------*
Epoch 5, batch 1039:
batchly_train_loss:  1.21551853912
cumulative_train_loss:  1.50938387836
*------------------------------------------------------------------------------*
Epoch 5, batch 1059:
batchly_train_loss:  1.51857945071
cumulative_train_loss:  1.50955754356
*------------------------------------------------------------------------------*
Epoch 5, batch 1079:
batchly_train_loss:  1.7239232386
cumulative_train_loss:  1.51353095774
*------------------------------------------------------------------------------*
Epoch 5, batch 1099:
batchly_train_loss:  1.2656740073
cumulative_train_loss:  1.5090203672
*------------------------------------------------------------------------------*
Epoch 5, batch 1119:
batchly_train_loss:  1.59022308085
cumulative_train_loss:  1.5104717115
*------------------------------------------------------------------------------*
Epoch 5, batch 1139:
batchly_train_loss:  1.55756263039
cumulative_train_loss:  1.5112985933
*------------------------------------------------------------------------------*
Epoch 5, batch 1159:
batchly_train_loss:  1.78952425678
cumulative_train_loss:  1.51609972641
*------------------------------------------------------------------------------*
Epoch 5, batch 1179:
batchly_train_loss:  1.61140958384
cumulative_train_loss:  1.51771651788
*------------------------------------------------------------------------------*
Epoch 5, batch 1199:
batchly_train_loss:  1.7587398969
cumulative_train_loss:  1.52173692454
*------------------------------------------------------------------------------*
Epoch 5, batch 1219:
batchly_train_loss:  1.70224019457
cumulative_train_loss:  1.524698422
*------------------------------------------------------------------------------*
Epoch 5, batch 1239:
batchly_train_loss:  1.41823020396
cumulative_train_loss:  1.52297980669
*------------------------------------------------------------------------------*
Epoch 5, batch 1259:
batchly_train_loss:  1.74653849234
cumulative_train_loss:  1.52653117581
*------------------------------------------------------------------------------*
Epoch 5, batch 1279:
batchly_train_loss:  1.26739821143
cumulative_train_loss:  1.52247905752
*------------------------------------------------------------------------------*
Epoch 5, batch 1299:
batchly_train_loss:  1.18132984015
cumulative_train_loss:  1.51722656765
*------------------------------------------------------------------------------*
Epoch 5, batch 1319:
batchly_train_loss:  1.42514326855
cumulative_train_loss:  1.51583030837
*------------------------------------------------------------------------------*
Epoch 5, batch 1339:
batchly_train_loss:  1.1821914495
cumulative_train_loss:  1.51084690495
*------------------------------------------------------------------------------*
Epoch 5, batch 1359:
batchly_train_loss:  1.66304689921
cumulative_train_loss:  1.51308678714
*------------------------------------------------------------------------------*
Epoch 5, batch 1379:
batchly_train_loss:  1.35771873026
cumulative_train_loss:  1.51083344331
*------------------------------------------------------------------------------*
Epoch 5, batch 1399:
batchly_train_loss:  1.46909063019
cumulative_train_loss:  1.51023669116
*------------------------------------------------------------------------------*
Epoch 5, batch 1419:
batchly_train_loss:  1.47522928389
cumulative_train_loss:  1.50974328161
*------------------------------------------------------------------------------*
Epoch 5, batch 1439:
batchly_train_loss:  1.72545831454
cumulative_train_loss:  1.51274140577
*------------------------------------------------------------------------------*
Epoch 5, batch 1459:
batchly_train_loss:  1.55347756665
cumulative_train_loss:  1.51329981784
*------------------------------------------------------------------------------*
Epoch 5, batch 1479:
batchly_train_loss:  1.80285266767
cumulative_train_loss:  1.51721533981
*------------------------------------------------------------------------------*
Epoch 5, batch 1499:
batchly_train_loss:  1.61298590033
cumulative_train_loss:  1.51849313248
*------------------------------------------------------------------------------*
Epoch 5, batch 1519:
batchly_train_loss:  1.70681370385
cumulative_train_loss:  1.52097266601
*------------------------------------------------------------------------------*
Epoch 5, batch 1539:
batchly_train_loss:  1.39933409914
cumulative_train_loss:  1.5193919179
*------------------------------------------------------------------------------*
Epoch 5, batch 1559:
batchly_train_loss:  1.7725643963
cumulative_train_loss:  1.52263980088
*------------------------------------------------------------------------------*
Epoch 5, batch 1579:
batchly_train_loss:  1.53708026811
cumulative_train_loss:  1.52282270737
*------------------------------------------------------------------------------*
Epoch 5, batch 1599:
batchly_train_loss:  1.62217255883
cumulative_train_loss:  1.52406535717
*------------------------------------------------------------------------------*
Epoch 5, batch 1619:
batchly_train_loss:  1.94222515617
cumulative_train_loss:  1.5292310125
*------------------------------------------------------------------------------*
Epoch 5, batch 1639:
batchly_train_loss:  1.40203898429
cumulative_train_loss:  1.52767894382
*------------------------------------------------------------------------------*
Epoch 5, batch 1659:
batchly_train_loss:  1.39102666907
cumulative_train_loss:  1.52603153846
*------------------------------------------------------------------------------*
Epoch 5, batch 1679:
batchly_train_loss:  1.65499518986
cumulative_train_loss:  1.52756773443
*------------------------------------------------------------------------------*
Epoch 5, batch 1699:
batchly_train_loss:  1.39021561323
cumulative_train_loss:  1.52595087603
*------------------------------------------------------------------------------*
Epoch 5, batch 1719:
batchly_train_loss:  1.53383078095
cumulative_train_loss:  1.52604255613
*------------------------------------------------------------------------------*
Epoch 5, batch 1739:
batchly_train_loss:  1.02570008222
cumulative_train_loss:  1.5202881861
*------------------------------------------------------------------------------*
Epoch 5, batch 1759:
batchly_train_loss:  1.41102062429
cumulative_train_loss:  1.51904580336
*------------------------------------------------------------------------------*
Epoch 5, batch 1779:
batchly_train_loss:  1.72411499408
cumulative_train_loss:  1.52135124677
*------------------------------------------------------------------------------*
Epoch 5, batch 1799:
batchly_train_loss:  1.29872899139
cumulative_train_loss:  1.51887629118
*------------------------------------------------------------------------------*
Epoch 5, batch 1819:
batchly_train_loss:  1.01518643115
cumulative_train_loss:  1.51333819486
*------------------------------------------------------------------------------*
Epoch 5, batch 1839:
batchly_train_loss:  1.38798495053
cumulative_train_loss:  1.51197491868
*------------------------------------------------------------------------------*
Epoch 5, batch 1859:
batchly_train_loss:  1.41148214419
cumulative_train_loss:  1.51089376995
*------------------------------------------------------------------------------*
Epoch 5, batch 1879:
batchly_train_loss:  1.19542472429
cumulative_train_loss:  1.50753593019
*------------------------------------------------------------------------------*
Epoch 5, batch 1899:
batchly_train_loss:  1.73197307542
cumulative_train_loss:  1.50989967053
*------------------------------------------------------------------------------*
Epoch 5, batch 1919:
batchly_train_loss:  1.54216342265
cumulative_train_loss:  1.51023592641
*------------------------------------------------------------------------------*
Epoch 5, batch 1939:
batchly_train_loss:  1.20627736424
cumulative_train_loss:  1.5071007169
*------------------------------------------------------------------------------*
Epoch 5, batch 1959:
batchly_train_loss:  1.61149569841
cumulative_train_loss:  1.50816651559
*------------------------------------------------------------------------------*
Epoch 5, batch 1979:
batchly_train_loss:  1.29118928263
cumulative_train_loss:  1.5059737189
*------------------------------------------------------------------------------*
Epoch 5, batch 1999:
batchly_train_loss:  1.68432631855
cumulative_train_loss:  1.5077581371
*------------------------------------------------------------------------------*
Epoch 5, batch 2019:
batchly_train_loss:  1.40520107549
cumulative_train_loss:  1.50674221772
*------------------------------------------------------------------------------*
Epoch 5, batch 2039:
batchly_train_loss:  1.38595362187
cumulative_train_loss:  1.50555743502
*------------------------------------------------------------------------------*
Epoch 5, batch 2059:
batchly_train_loss:  1.40161700645
cumulative_train_loss:  1.50454781454
*------------------------------------------------------------------------------*
Epoch 5, batch 2079:
batchly_train_loss:  1.56545970307
cumulative_train_loss:  1.5051337875
*------------------------------------------------------------------------------*
Epoch 5, batch 2099:
batchly_train_loss:  1.4579827127
cumulative_train_loss:  1.5046845157
*------------------------------------------------------------------------------*
Epoch 5, batch 2119:
batchly_train_loss:  1.51390060851
cumulative_train_loss:  1.504771501
*------------------------------------------------------------------------------*
Epoch 5, batch 2139:
batchly_train_loss:  1.41390856064
cumulative_train_loss:  1.50392191764
*------------------------------------------------------------------------------*
Epoch 5, batch 2159:
batchly_train_loss:  1.14455441253
cumulative_train_loss:  1.50059289953
*------------------------------------------------------------------------------*
Epoch 5, batch 2179:
batchly_train_loss:  1.44981444524
cumulative_train_loss:  1.50012682836
*------------------------------------------------------------------------------*
Epoch 5, batch 2199:
batchly_train_loss:  1.24577870892
cumulative_train_loss:  1.49781352123
*------------------------------------------------------------------------------*
Epoch 5, batch 2219:
batchly_train_loss:  1.08574977112
cumulative_train_loss:  1.49409956223
*------------------------------------------------------------------------------*
Epoch 5, batch 2239:
batchly_train_loss:  1.98216269531
cumulative_train_loss:  1.49845921505
*------------------------------------------------------------------------------*
Epoch 5, batch 2259:
batchly_train_loss:  1.41428616513
cumulative_train_loss:  1.49771399106
*------------------------------------------------------------------------------*
Epoch 5, batch 2279:
batchly_train_loss:  1.93514278022
cumulative_train_loss:  1.50155276938
*------------------------------------------------------------------------------*
Epoch 5, batch 2299:
batchly_train_loss:  1.62840858012
cumulative_train_loss:  1.5026563432
*------------------------------------------------------------------------------*
Epoch 5, batch 2319:
batchly_train_loss:  1.65630099485
cumulative_train_loss:  1.50398143722
*------------------------------------------------------------------------------*
Epoch 5, batch 2339:
batchly_train_loss:  1.53431655744
cumulative_train_loss:  1.5042408226
*------------------------------------------------------------------------------*
Epoch 5, batch 2359:
batchly_train_loss:  1.46538456777
cumulative_train_loss:  1.50391139271
*------------------------------------------------------------------------------*
Epoch 5, batch 2379:
batchly_train_loss:  1.3714746484
cumulative_train_loss:  1.50279801109
*------------------------------------------------------------------------------*
Epoch 5, batch 2399:
batchly_train_loss:  1.14139287242
cumulative_train_loss:  1.4997850462
*------------------------------------------------------------------------------*
Epoch 5, batch 2419:
batchly_train_loss:  1.12244524429
cumulative_train_loss:  1.49666524627
*------------------------------------------------------------------------------*
Epoch 5, batch 2439:
batchly_train_loss:  1.17066546814
cumulative_train_loss:  1.49399202135
*------------------------------------------------------------------------------*
Epoch 5, batch 2459:
batchly_train_loss:  1.57152128828
cumulative_train_loss:  1.49462259693
*------------------------------------------------------------------------------*
Epoch 5, batch 2479:
batchly_train_loss:  1.63606499421
cumulative_train_loss:  1.49576372155
*------------------------------------------------------------------------------*
Epoch 5, batch 2499:
batchly_train_loss:  1.19290183697
cumulative_train_loss:  1.49333985693
*------------------------------------------------------------------------------*
Epoch 5, batch 2519:
batchly_train_loss:  1.10518485334
cumulative_train_loss:  1.49025803872
*------------------------------------------------------------------------------*
Epoch 5, batch 2539:
batchly_train_loss:  1.25212194684
cumulative_train_loss:  1.48838221287
*------------------------------------------------------------------------------*
Epoch 5, batch 2559:
batchly_train_loss:  1.458853242
cumulative_train_loss:  1.48815142763
*------------------------------------------------------------------------------*
Epoch 5, batch 2579:
batchly_train_loss:  1.42293791936
cumulative_train_loss:  1.48764570054
*------------------------------------------------------------------------------*
Epoch 5, batch 2599:
batchly_train_loss:  1.80923408337
cumulative_train_loss:  1.49012040914
*------------------------------------------------------------------------------*
Epoch 5, batch 2619:
batchly_train_loss:  1.3315504962
cumulative_train_loss:  1.48890948961
*------------------------------------------------------------------------------*
Epoch 5, batch 2639:
batchly_train_loss:  1.70913946646
cumulative_train_loss:  1.49057853074
*------------------------------------------------------------------------------*
Epoch 5, batch 2659:
batchly_train_loss:  1.70157181983
cumulative_train_loss:  1.49216554307
*------------------------------------------------------------------------------*
Epoch 5, batch 2679:
batchly_train_loss:  1.45677524676
cumulative_train_loss:  1.49190133779
*------------------------------------------------------------------------------*
Epoch 5, batch 2699:
batchly_train_loss:  1.47803384642
cumulative_train_loss:  1.49179857758
*------------------------------------------------------------------------------*
Epoch 5, batch 2719:
batchly_train_loss:  1.44608238204
cumulative_train_loss:  1.49146230545
*------------------------------------------------------------------------------*
Epoch 5, batch 2739:
batchly_train_loss:  1.51941909385
cumulative_train_loss:  1.4916664441
*------------------------------------------------------------------------------*
Epoch 5, batch 2759:
batchly_train_loss:  1.68202389194
cumulative_train_loss:  1.49304634586
*------------------------------------------------------------------------------*
Epoch 5, batch 2779:
batchly_train_loss:  1.40783425216
cumulative_train_loss:  1.49243308862
*------------------------------------------------------------------------------*
Epoch 5, batch 2799:
batchly_train_loss:  1.52434415234
cumulative_train_loss:  1.49266110623
*------------------------------------------------------------------------------*
Epoch 5, batch 2819:
batchly_train_loss:  1.41841016167
cumulative_train_loss:  1.49213431698
*------------------------------------------------------------------------------*
Epoch 5, batch 2839:
batchly_train_loss:  1.23882089511
cumulative_train_loss:  1.49034979129
*------------------------------------------------------------------------------*
Epoch 5, batch 2859:
batchly_train_loss:  1.33610407353
cumulative_train_loss:  1.48927077262
*------------------------------------------------------------------------------*
Epoch 5, batch 2879:
batchly_train_loss:  1.3125214255
cumulative_train_loss:  1.48804292026
*------------------------------------------------------------------------------*
Epoch 5, batch 2899:
batchly_train_loss:  1.51836741131
cumulative_train_loss:  1.48825212683
*------------------------------------------------------------------------------*
Epoch 5, batch 2919:
batchly_train_loss:  1.19213058576
cumulative_train_loss:  1.48622320226
*------------------------------------------------------------------------------*
Epoch 5, batch 2939:
batchly_train_loss:  1.14687075264
cumulative_train_loss:  1.48391389671
*------------------------------------------------------------------------------*
Epoch 5, batch 2959:
batchly_train_loss:  1.20007069308
cumulative_train_loss:  1.48199538908
*------------------------------------------------------------------------------*
Epoch 5, batch 2979:
batchly_train_loss:  1.58394196401
cumulative_train_loss:  1.48267982396
*------------------------------------------------------------------------------*
Epoch 5, batch 2999:
batchly_train_loss:  1.24318032377
cumulative_train_loss:  1.48108262823
*------------------------------------------------------------------------------*
Epoch 5, batch 3019:
batchly_train_loss:  1.92062735725
cumulative_train_loss:  1.48399448466
*------------------------------------------------------------------------------*
Epoch 5, batch 3039:
batchly_train_loss:  1.59209831285
cumulative_train_loss:  1.48470592809
*------------------------------------------------------------------------------*
Epoch 5, batch 3059:
batchly_train_loss:  1.40275164596
cumulative_train_loss:  1.48417010408
*------------------------------------------------------------------------------*
Epoch 5, batch 3079:
batchly_train_loss:  1.65126079014
cumulative_train_loss:  1.48525546092
*------------------------------------------------------------------------------*
Epoch 5, batch 3099:
batchly_train_loss:  1.40118557181
cumulative_train_loss:  1.48471289952
*------------------------------------------------------------------------------*
Epoch 5, batch 3119:
batchly_train_loss:  1.59257690461
cumulative_train_loss:  1.48540455714
*------------------------------------------------------------------------------*
Epoch 5, batch 3139:
batchly_train_loss:  1.63145900406
cumulative_train_loss:  1.4863351366
*------------------------------------------------------------------------------*
Epoch 5, batch 3159:
batchly_train_loss:  1.43054616095
cumulative_train_loss:  1.48598193004
*------------------------------------------------------------------------------*
Epoch 5, batch 3179:
batchly_train_loss:  1.41830058234
cumulative_train_loss:  1.48555612729
*------------------------------------------------------------------------------*
Epoch 5, batch 3199:
batchly_train_loss:  1.28039646208
cumulative_train_loss:  1.48427347855
*------------------------------------------------------------------------------*
Epoch 5, batch 3219:
batchly_train_loss:  1.2773606693
cumulative_train_loss:  1.48298790658
*------------------------------------------------------------------------------*
Epoch 5, batch 3239:
batchly_train_loss:  1.57008130648
cumulative_train_loss:  1.48352568614
*------------------------------------------------------------------------------*
Epoch 5, batch 3259:
batchly_train_loss:  1.77222251212
cumulative_train_loss:  1.48529737578
*------------------------------------------------------------------------------*
Epoch 5, batch 3279:
batchly_train_loss:  1.56623153368
cumulative_train_loss:  1.48579102724
*------------------------------------------------------------------------------*
Epoch 5, batch 3299:
batchly_train_loss:  1.15319972281
cumulative_train_loss:  1.48377471136
*------------------------------------------------------------------------------*
Epoch 5, batch 3319:
batchly_train_loss:  1.58178506904
cumulative_train_loss:  1.4843653131
*------------------------------------------------------------------------------*
Epoch 5, batch 3339:
batchly_train_loss:  1.22430648981
cumulative_train_loss:  1.48280760825
*------------------------------------------------------------------------------*
Epoch 5, batch 3359:
batchly_train_loss:  1.57307187584
cumulative_train_loss:  1.48334505552
*------------------------------------------------------------------------------*
Epoch 5, batch 3379:
batchly_train_loss:  1.36267589925
cumulative_train_loss:  1.48263082553
*------------------------------------------------------------------------------*
Epoch 5, batch 3399:
batchly_train_loss:  0.898141231317
cumulative_train_loss:  1.47919163992
*------------------------------------------------------------------------------*
Epoch 5, batch 3419:
batchly_train_loss:  1.56497448804
cumulative_train_loss:  1.47969344073
*------------------------------------------------------------------------------*
Epoch 5, batch 3439:
batchly_train_loss:  1.24095074898
cumulative_train_loss:  1.47830499821
*------------------------------------------------------------------------------*
Epoch 5, batch 3459:
batchly_train_loss:  1.80572561038
cumulative_train_loss:  1.48019815005
*------------------------------------------------------------------------------*
Epoch 5, batch 3479:
batchly_train_loss:  1.51691892006
cumulative_train_loss:  1.48040924962
*------------------------------------------------------------------------------*
Epoch 5, batch 3499:
batchly_train_loss:  1.24196868994
cumulative_train_loss:  1.47904634274
*------------------------------------------------------------------------------*
Epoch 5, batch 3519:
batchly_train_loss:  1.09363652219
cumulative_train_loss:  1.47685589192
*------------------------------------------------------------------------------*
Epoch 5, batch 3539:
batchly_train_loss:  1.69619507329
cumulative_train_loss:  1.47809544649
*------------------------------------------------------------------------------*
Epoch 5, batch 3559:
batchly_train_loss:  1.50106219791
cumulative_train_loss:  1.47822450944
*------------------------------------------------------------------------------*
Epoch 5, batch 3579:
batchly_train_loss:  1.57631667262
cumulative_train_loss:  1.47877266347
*------------------------------------------------------------------------------*
Epoch 5, batch 3599:
batchly_train_loss:  1.79432845656
cumulative_train_loss:  1.48052623831
*------------------------------------------------------------------------------*
Epoch 5, batch 3619:
batchly_train_loss:  1.23858409082
cumulative_train_loss:  1.47918917201
*------------------------------------------------------------------------------*
Epoch 5, batch 3639:
batchly_train_loss:  1.40983690118
cumulative_train_loss:  1.47880801086
*------------------------------------------------------------------------------*
Epoch 5, batch 3659:
batchly_train_loss:  1.43694657055
cumulative_train_loss:  1.4785791973
*------------------------------------------------------------------------------*
Epoch 5, batch 3679:
batchly_train_loss:  1.4261586133
cumulative_train_loss:  1.47829422539
*------------------------------------------------------------------------------*
Epoch 5, batch 3699:
batchly_train_loss:  1.25611802918
cumulative_train_loss:  1.47709294831
*------------------------------------------------------------------------------*
Epoch 5, batch 3719:
batchly_train_loss:  1.8190385799
cumulative_train_loss:  1.47893186001
*------------------------------------------------------------------------------*
Epoch 5, batch 3739:
batchly_train_loss:  1.59817592702
cumulative_train_loss:  1.47956969937
*------------------------------------------------------------------------------*
Epoch 5, batch 3759:
batchly_train_loss:  1.44467864515
cumulative_train_loss:  1.47938405928
*------------------------------------------------------------------------------*
Epoch 5, batch 3779:
batchly_train_loss:  1.10678362414
cumulative_train_loss:  1.47741210672
*------------------------------------------------------------------------------*
Epoch 5, batch 3799:
batchly_train_loss:  2.22844316577
cumulative_train_loss:  1.48136594226
*------------------------------------------------------------------------------*
Epoch 5, batch 3819:
batchly_train_loss:  1.53418206494
cumulative_train_loss:  1.48164253887
*------------------------------------------------------------------------------*
Epoch 5, batch 3839:
batchly_train_loss:  1.85567234561
cumulative_train_loss:  1.48359111822
*------------------------------------------------------------------------------*
Epoch 5, batch 3859:
batchly_train_loss:  1.58363426761
cumulative_train_loss:  1.48410961083
*------------------------------------------------------------------------------*
Epoch 5, batch 3879:
batchly_train_loss:  1.53650838757
cumulative_train_loss:  1.48437977725
*------------------------------------------------------------------------------*
Epoch 5, batch 3899:
batchly_train_loss:  1.62118925907
cumulative_train_loss:  1.48508154427
*------------------------------------------------------------------------------*
Epoch 5, batch 3919:
batchly_train_loss:  1.55721336525
cumulative_train_loss:  1.48544965768
*------------------------------------------------------------------------------*
Epoch 5, batch 3939:
batchly_train_loss:  1.30978476078
cumulative_train_loss:  1.48455773131
*------------------------------------------------------------------------------*
Epoch 5, batch 3959:
batchly_train_loss:  1.4238565173
cumulative_train_loss:  1.48425108209
*------------------------------------------------------------------------------*
Epoch 5, batch 3979:
batchly_train_loss:  1.85755341306
cumulative_train_loss:  1.48612744465
*------------------------------------------------------------------------------*
Epoch 5, batch 3999:
batchly_train_loss:  1.7661343989
cumulative_train_loss:  1.48752782951
================================================================================
Epoch 5 of 8 took 6447.273s
  training loss:		1.487058
evaluating model...
VALID_LOSS:  1.50203809372
VALID_ACC:  0.484545454545
FULL_TRAIN_LOSS:  1.38726261649
FULL_TRAIN_ACC:  0.528009950249
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=5
*------------------------------------------------------------------------------*
Epoch 6, batch 19:
batchly_train_loss:  1.53347695192
cumulative_train_loss:  1.55649339407
*------------------------------------------------------------------------------*
Epoch 6, batch 39:
batchly_train_loss:  1.12246094483
cumulative_train_loss:  1.33391265087
*------------------------------------------------------------------------------*
Epoch 6, batch 59:
batchly_train_loss:  1.65360495199
cumulative_train_loss:  1.44228292244
*------------------------------------------------------------------------------*
Epoch 6, batch 79:
batchly_train_loss:  1.57331980043
cumulative_train_loss:  1.4754568156
*------------------------------------------------------------------------------*
Epoch 6, batch 99:
batchly_train_loss:  2.0839273294
cumulative_train_loss:  1.59838015172
*------------------------------------------------------------------------------*
Epoch 6, batch 119:
batchly_train_loss:  1.49375107785
cumulative_train_loss:  1.58079543342
*------------------------------------------------------------------------------*
Epoch 6, batch 139:
batchly_train_loss:  1.18772596772
cumulative_train_loss:  1.52423867577
*------------------------------------------------------------------------------*
Epoch 6, batch 159:
batchly_train_loss:  1.30746244408
cumulative_train_loss:  1.49697122524
*------------------------------------------------------------------------------*
Epoch 6, batch 179:
batchly_train_loss:  1.45650662996
cumulative_train_loss:  1.49245004141
*------------------------------------------------------------------------------*
Epoch 6, batch 199:
batchly_train_loss:  1.43798105837
cumulative_train_loss:  1.48697577176
*------------------------------------------------------------------------------*
Epoch 6, batch 219:
batchly_train_loss:  1.28875828021
cumulative_train_loss:  1.46887371774
*------------------------------------------------------------------------------*
Epoch 6, batch 239:
batchly_train_loss:  1.40603701519
cumulative_train_loss:  1.46361541627
*------------------------------------------------------------------------------*
Epoch 6, batch 259:
batchly_train_loss:  1.45776884524
cumulative_train_loss:  1.4631639436
*------------------------------------------------------------------------------*
Epoch 6, batch 279:
batchly_train_loss:  1.50711481152
cumulative_train_loss:  1.46631454345
*------------------------------------------------------------------------------*
Epoch 6, batch 299:
batchly_train_loss:  0.984587588739
cumulative_train_loss:  1.43409200468
*------------------------------------------------------------------------------*
Epoch 6, batch 319:
batchly_train_loss:  1.38315139539
cumulative_train_loss:  1.43089823607
*------------------------------------------------------------------------------*
Epoch 6, batch 339:
batchly_train_loss:  1.65477065166
cumulative_train_loss:  1.4441060482
*------------------------------------------------------------------------------*
Epoch 6, batch 359:
batchly_train_loss:  1.73541236129
cumulative_train_loss:  1.46033481216
*------------------------------------------------------------------------------*
Epoch 6, batch 379:
batchly_train_loss:  1.45592561048
cumulative_train_loss:  1.46010213661
*------------------------------------------------------------------------------*
Epoch 6, batch 399:
batchly_train_loss:  1.54868648679
cumulative_train_loss:  1.46454245491
*------------------------------------------------------------------------------*
Epoch 6, batch 419:
batchly_train_loss:  1.68969031842
cumulative_train_loss:  1.47528936964
*------------------------------------------------------------------------------*
Epoch 6, batch 439:
batchly_train_loss:  1.29018171474
cumulative_train_loss:  1.46685621907
*------------------------------------------------------------------------------*
Epoch 6, batch 459:
batchly_train_loss:  1.44884071137
cumulative_train_loss:  1.46607122963
*------------------------------------------------------------------------------*
Epoch 6, batch 479:
batchly_train_loss:  1.61921334974
cumulative_train_loss:  1.47246547264
*------------------------------------------------------------------------------*
Epoch 6, batch 499:
batchly_train_loss:  1.17018021226
cumulative_train_loss:  1.46034983094
*------------------------------------------------------------------------------*
Epoch 6, batch 519:
batchly_train_loss:  1.38409731817
cumulative_train_loss:  1.45741139114
*------------------------------------------------------------------------------*
Epoch 6, batch 539:
batchly_train_loss:  0.875693282776
cumulative_train_loss:  1.43582630364
*------------------------------------------------------------------------------*
Epoch 6, batch 559:
batchly_train_loss:  1.30771941785
cumulative_train_loss:  1.43124287302
*------------------------------------------------------------------------------*
Epoch 6, batch 579:
batchly_train_loss:  0.994394172526
cumulative_train_loss:  1.41615310789
*------------------------------------------------------------------------------*
Epoch 6, batch 599:
batchly_train_loss:  1.25314980642
cumulative_train_loss:  1.41071059365
*------------------------------------------------------------------------------*
Epoch 6, batch 619:
batchly_train_loss:  1.17668316894
cumulative_train_loss:  1.40314912597
*------------------------------------------------------------------------------*
Epoch 6, batch 639:
batchly_train_loss:  1.29478052932
cumulative_train_loss:  1.39975730761
*------------------------------------------------------------------------------*
Epoch 6, batch 659:
batchly_train_loss:  1.75135880578
cumulative_train_loss:  1.41042806628
*------------------------------------------------------------------------------*
Epoch 6, batch 679:
batchly_train_loss:  1.47486189333
cumulative_train_loss:  1.41232596987
*------------------------------------------------------------------------------*
Epoch 6, batch 699:
batchly_train_loss:  1.2799516553
cumulative_train_loss:  1.40853843584
*------------------------------------------------------------------------------*
Epoch 6, batch 719:
batchly_train_loss:  1.49779621545
cumulative_train_loss:  1.41102126698
*------------------------------------------------------------------------------*
Epoch 6, batch 739:
batchly_train_loss:  1.74993088731
cumulative_train_loss:  1.4201933812
*------------------------------------------------------------------------------*
Epoch 6, batch 759:
batchly_train_loss:  1.42988839153
cumulative_train_loss:  1.42044884919
*------------------------------------------------------------------------------*
Epoch 6, batch 779:
batchly_train_loss:  1.14671965862
cumulative_train_loss:  1.41342114211
*------------------------------------------------------------------------------*
Epoch 6, batch 799:
batchly_train_loss:  1.22711408786
cumulative_train_loss:  1.40875763638
*------------------------------------------------------------------------------*
Epoch 6, batch 819:
batchly_train_loss:  1.05230354262
cumulative_train_loss:  1.4000530187
*------------------------------------------------------------------------------*
Epoch 6, batch 839:
batchly_train_loss:  0.887528062759
cumulative_train_loss:  1.38783549889
*------------------------------------------------------------------------------*
Epoch 6, batch 859:
batchly_train_loss:  1.01481707398
cumulative_train_loss:  1.37915055303
*------------------------------------------------------------------------------*
Epoch 6, batch 879:
batchly_train_loss:  1.17003816513
cumulative_train_loss:  1.37439259198
*------------------------------------------------------------------------------*
Epoch 6, batch 899:
batchly_train_loss:  1.05858635292
cumulative_train_loss:  1.3673668692
*------------------------------------------------------------------------------*
Epoch 6, batch 919:
batchly_train_loss:  1.59120297744
cumulative_train_loss:  1.37223816644
*------------------------------------------------------------------------------*
Epoch 6, batch 939:
batchly_train_loss:  1.56360461498
cumulative_train_loss:  1.37631412914
*------------------------------------------------------------------------------*
Epoch 6, batch 959:
batchly_train_loss:  1.37066145236
cumulative_train_loss:  1.37619624224
*------------------------------------------------------------------------------*
Epoch 6, batch 979:
batchly_train_loss:  1.64258818889
cumulative_train_loss:  1.38163836577
*------------------------------------------------------------------------------*
Epoch 6, batch 999:
batchly_train_loss:  1.33711035279
cumulative_train_loss:  1.38074691406
*------------------------------------------------------------------------------*
Epoch 6, batch 1019:
batchly_train_loss:  1.09946594838
cumulative_train_loss:  1.37522618853
*------------------------------------------------------------------------------*
Epoch 6, batch 1039:
batchly_train_loss:  1.50956532767
cumulative_train_loss:  1.37781211998
*------------------------------------------------------------------------------*
Epoch 6, batch 1059:
batchly_train_loss:  1.65714199367
cumulative_train_loss:  1.38308747171
*------------------------------------------------------------------------------*
Epoch 6, batch 1079:
batchly_train_loss:  1.5829254197
cumulative_train_loss:  1.3867916042
*------------------------------------------------------------------------------*
Epoch 6, batch 1099:
batchly_train_loss:  1.38401482152
cumulative_train_loss:  1.3867410713
*------------------------------------------------------------------------------*
Epoch 6, batch 1119:
batchly_train_loss:  1.47115018318
cumulative_train_loss:  1.38824972388
*------------------------------------------------------------------------------*
Epoch 6, batch 1139:
batchly_train_loss:  1.17105188479
cumulative_train_loss:  1.38443589001
*------------------------------------------------------------------------------*
Epoch 6, batch 1159:
batchly_train_loss:  1.47229269312
cumulative_train_loss:  1.38595196944
*------------------------------------------------------------------------------*
Epoch 6, batch 1179:
batchly_train_loss:  1.2868706155
cumulative_train_loss:  1.38427120008
*------------------------------------------------------------------------------*
Epoch 6, batch 1199:
batchly_train_loss:  1.34410371348
cumulative_train_loss:  1.38360118362
*------------------------------------------------------------------------------*
Epoch 6, batch 1219:
batchly_train_loss:  1.40784802265
cumulative_train_loss:  1.38399899886
*------------------------------------------------------------------------------*
Epoch 6, batch 1239:
batchly_train_loss:  1.70860849184
cumulative_train_loss:  1.38923886154
*------------------------------------------------------------------------------*
Epoch 6, batch 1259:
batchly_train_loss:  1.57632652548
cumulative_train_loss:  1.39221086574
*------------------------------------------------------------------------------*
Epoch 6, batch 1279:
batchly_train_loss:  1.47889696986
cumulative_train_loss:  1.39356639512
*------------------------------------------------------------------------------*
Epoch 6, batch 1299:
batchly_train_loss:  0.936523455998
cumulative_train_loss:  1.38652955233
*------------------------------------------------------------------------------*
Epoch 6, batch 1319:
batchly_train_loss:  1.45795298665
cumulative_train_loss:  1.38761254603
*------------------------------------------------------------------------------*
Epoch 6, batch 1339:
batchly_train_loss:  1.26277661433
cumulative_train_loss:  1.38574793166
*------------------------------------------------------------------------------*
Epoch 6, batch 1359:
batchly_train_loss:  1.50656608218
cumulative_train_loss:  1.38752597656
*------------------------------------------------------------------------------*
Epoch 6, batch 1379:
batchly_train_loss:  1.34206588454
cumulative_train_loss:  1.38686665688
*------------------------------------------------------------------------------*
Epoch 6, batch 1399:
batchly_train_loss:  0.994505149481
cumulative_train_loss:  1.38125748593
*------------------------------------------------------------------------------*
Epoch 6, batch 1419:
batchly_train_loss:  1.51676057699
cumulative_train_loss:  1.38316732513
*------------------------------------------------------------------------------*
Epoch 6, batch 1439:
batchly_train_loss:  1.71784442786
cumulative_train_loss:  1.38781884845
*------------------------------------------------------------------------------*
Epoch 6, batch 1459:
batchly_train_loss:  1.10579512289
cumulative_train_loss:  1.38395286181
*------------------------------------------------------------------------------*
Epoch 6, batch 1479:
batchly_train_loss:  1.61254725057
cumulative_train_loss:  1.38704406382
*------------------------------------------------------------------------------*
Epoch 6, batch 1499:
batchly_train_loss:  1.05153218278
cumulative_train_loss:  1.38256758775
*------------------------------------------------------------------------------*
Epoch 6, batch 1519:
batchly_train_loss:  1.97407905663
cumulative_train_loss:  1.39035575719
*------------------------------------------------------------------------------*
Epoch 6, batch 1539:
batchly_train_loss:  1.52484223436
cumulative_train_loss:  1.3921034697
*------------------------------------------------------------------------------*
Epoch 6, batch 1559:
batchly_train_loss:  1.97026298716
cumulative_train_loss:  1.39952052573
*------------------------------------------------------------------------------*
Epoch 6, batch 1579:
batchly_train_loss:  1.38112139496
cumulative_train_loss:  1.39928747784
*------------------------------------------------------------------------------*
Epoch 6, batch 1599:
batchly_train_loss:  1.14242661839
cumulative_train_loss:  1.39607470911
*------------------------------------------------------------------------------*
Epoch 6, batch 1619:
batchly_train_loss:  1.1162052907
cumulative_train_loss:  1.39261739697
*------------------------------------------------------------------------------*
Epoch 6, batch 1639:
batchly_train_loss:  1.40607265848
cumulative_train_loss:  1.39278158564
*------------------------------------------------------------------------------*
Epoch 6, batch 1659:
batchly_train_loss:  1.19361747093
cumulative_train_loss:  1.39038057159
*------------------------------------------------------------------------------*
Epoch 6, batch 1679:
batchly_train_loss:  1.3868802812
cumulative_train_loss:  1.39033887665
*------------------------------------------------------------------------------*
Epoch 6, batch 1699:
batchly_train_loss:  1.05498053534
cumulative_train_loss:  1.38639116222
*------------------------------------------------------------------------------*
Epoch 6, batch 1719:
batchly_train_loss:  1.05421528206
cumulative_train_loss:  1.38252640503
*------------------------------------------------------------------------------*
Epoch 6, batch 1739:
batchly_train_loss:  1.41621111219
cumulative_train_loss:  1.38291380822
*------------------------------------------------------------------------------*
Epoch 6, batch 1759:
batchly_train_loss:  1.63338951945
cumulative_train_loss:  1.38576174126
*------------------------------------------------------------------------------*
Epoch 6, batch 1779:
batchly_train_loss:  1.14301277021
cumulative_train_loss:  1.38303269156
*------------------------------------------------------------------------------*
Epoch 6, batch 1799:
batchly_train_loss:  1.38542746346
cumulative_train_loss:  1.38305931493
*------------------------------------------------------------------------------*
Epoch 6, batch 1819:
batchly_train_loss:  1.5431198087
cumulative_train_loss:  1.38481918842
*------------------------------------------------------------------------------*
Epoch 6, batch 1839:
batchly_train_loss:  1.44226163384
cumulative_train_loss:  1.38544390234
*------------------------------------------------------------------------------*
Epoch 6, batch 1859:
batchly_train_loss:  1.79468962994
cumulative_train_loss:  1.38984676116
*------------------------------------------------------------------------------*
Epoch 6, batch 1879:
batchly_train_loss:  1.49465571299
cumulative_train_loss:  1.39096234341
*------------------------------------------------------------------------------*
Epoch 6, batch 1899:
batchly_train_loss:  1.37061672052
cumulative_train_loss:  1.39074806618
*------------------------------------------------------------------------------*
Epoch 6, batch 1919:
batchly_train_loss:  0.860453526927
cumulative_train_loss:  1.3852212862
*------------------------------------------------------------------------------*
Epoch 6, batch 1939:
batchly_train_loss:  1.34497261003
cumulative_train_loss:  1.3848061374
*------------------------------------------------------------------------------*
Epoch 6, batch 1959:
batchly_train_loss:  1.01614578029
cumulative_train_loss:  1.38104237673
*------------------------------------------------------------------------------*
Epoch 6, batch 1979:
batchly_train_loss:  1.34228372055
cumulative_train_loss:  1.38065067733
*------------------------------------------------------------------------------*
Epoch 6, batch 1999:
batchly_train_loss:  1.71508185435
cumulative_train_loss:  1.38399666209
*------------------------------------------------------------------------------*
Epoch 6, batch 2019:
batchly_train_loss:  1.41897562518
cumulative_train_loss:  1.38434315999
*------------------------------------------------------------------------------*
Epoch 6, batch 2039:
batchly_train_loss:  1.18217212691
cumulative_train_loss:  1.38236011896
*------------------------------------------------------------------------------*
Epoch 6, batch 2059:
batchly_train_loss:  1.52412597745
cumulative_train_loss:  1.38373715498
*------------------------------------------------------------------------------*
Epoch 6, batch 2079:
batchly_train_loss:  1.54349745739
cumulative_train_loss:  1.38527405063
*------------------------------------------------------------------------------*
Epoch 6, batch 2099:
batchly_train_loss:  1.6535913717
cumulative_train_loss:  1.38783067112
*------------------------------------------------------------------------------*
Epoch 6, batch 2119:
batchly_train_loss:  1.10817271506
cumulative_train_loss:  1.38519114346
*------------------------------------------------------------------------------*
Epoch 6, batch 2139:
batchly_train_loss:  1.63292135394
cumulative_train_loss:  1.38750746146
*------------------------------------------------------------------------------*
Epoch 6, batch 2159:
batchly_train_loss:  1.29150587185
cumulative_train_loss:  1.38661814614
*------------------------------------------------------------------------------*
Epoch 6, batch 2179:
batchly_train_loss:  1.69983703066
cumulative_train_loss:  1.38949303264
*------------------------------------------------------------------------------*
Epoch 6, batch 2199:
batchly_train_loss:  1.77755805919
cumulative_train_loss:  1.39302250082
*------------------------------------------------------------------------------*
Epoch 6, batch 2219:
batchly_train_loss:  1.31721452127
cumulative_train_loss:  1.39233923827
*------------------------------------------------------------------------------*
Epoch 6, batch 2239:
batchly_train_loss:  1.58076891018
cumulative_train_loss:  1.39402239747
*------------------------------------------------------------------------------*
Epoch 6, batch 2259:
batchly_train_loss:  1.31714567934
cumulative_train_loss:  1.39334177137
*------------------------------------------------------------------------------*
Epoch 6, batch 2279:
batchly_train_loss:  1.320961076
cumulative_train_loss:  1.39270657439
*------------------------------------------------------------------------------*
Epoch 6, batch 2299:
batchly_train_loss:  1.12804533251
cumulative_train_loss:  1.39040417124
*------------------------------------------------------------------------------*
Epoch 6, batch 2319:
batchly_train_loss:  1.73203835317
cumulative_train_loss:  1.3933505635
*------------------------------------------------------------------------------*
Epoch 6, batch 2339:
batchly_train_loss:  1.54275501847
cumulative_train_loss:  1.3946280706
*------------------------------------------------------------------------------*
Epoch 6, batch 2359:
batchly_train_loss:  1.46305869041
cumulative_train_loss:  1.39520823694
*------------------------------------------------------------------------------*
Epoch 6, batch 2379:
batchly_train_loss:  1.76858509108
cumulative_train_loss:  1.39834717644
*------------------------------------------------------------------------------*
Epoch 6, batch 2399:
batchly_train_loss:  1.67418359099
cumulative_train_loss:  1.40064677139
*------------------------------------------------------------------------------*
Epoch 6, batch 2419:
batchly_train_loss:  1.69281916234
cumulative_train_loss:  1.40306241745
*------------------------------------------------------------------------------*
Epoch 6, batch 2439:
batchly_train_loss:  1.27324147158
cumulative_train_loss:  1.40199787505
*------------------------------------------------------------------------------*
Epoch 6, batch 2459:
batchly_train_loss:  1.05184456623
cumulative_train_loss:  1.39914994249
*------------------------------------------------------------------------------*
Epoch 6, batch 2479:
batchly_train_loss:  0.926404536269
cumulative_train_loss:  1.39533594163
*------------------------------------------------------------------------------*
Epoch 6, batch 2499:
batchly_train_loss:  1.13621368219
cumulative_train_loss:  1.39326213403
*------------------------------------------------------------------------------*
Epoch 6, batch 2519:
batchly_train_loss:  1.5673131194
cumulative_train_loss:  1.39464403943
*------------------------------------------------------------------------------*
Epoch 6, batch 2539:
batchly_train_loss:  1.15021250927
cumulative_train_loss:  1.39271862368
*------------------------------------------------------------------------------*
Epoch 6, batch 2559:
batchly_train_loss:  1.07591338858
cumulative_train_loss:  1.39024261559
*------------------------------------------------------------------------------*
Epoch 6, batch 2579:
batchly_train_loss:  1.16790610778
cumulative_train_loss:  1.38851840847
*------------------------------------------------------------------------------*
Epoch 6, batch 2599:
batchly_train_loss:  1.28687561606
cumulative_train_loss:  1.38773624
*------------------------------------------------------------------------------*
Epoch 6, batch 2619:
batchly_train_loss:  1.30590379202
cumulative_train_loss:  1.38711132631
*------------------------------------------------------------------------------*
Epoch 6, batch 2639:
batchly_train_loss:  1.33800214574
cumulative_train_loss:  1.38673914609
*------------------------------------------------------------------------------*
Epoch 6, batch 2659:
batchly_train_loss:  1.70987651215
cumulative_train_loss:  1.38916966407
*------------------------------------------------------------------------------*
Epoch 6, batch 2679:
batchly_train_loss:  1.72824082151
cumulative_train_loss:  1.39170099037
*------------------------------------------------------------------------------*
Epoch 6, batch 2699:
batchly_train_loss:  1.10666870638
cumulative_train_loss:  1.38958885784
*------------------------------------------------------------------------------*
Epoch 6, batch 2719:
batchly_train_loss:  1.24482196607
cumulative_train_loss:  1.38852400391
*------------------------------------------------------------------------------*
Epoch 6, batch 2739:
batchly_train_loss:  1.21146951744
cumulative_train_loss:  1.38723116356
*------------------------------------------------------------------------------*
Epoch 6, batch 2759:
batchly_train_loss:  0.91270314561
cumulative_train_loss:  1.38379130841
*------------------------------------------------------------------------------*
Epoch 6, batch 2779:
batchly_train_loss:  1.38590738783
cumulative_train_loss:  1.38380653748
*------------------------------------------------------------------------------*
Epoch 6, batch 2799:
batchly_train_loss:  1.41011578685
cumulative_train_loss:  1.38399452783
*------------------------------------------------------------------------------*
Epoch 6, batch 2819:
batchly_train_loss:  1.41933058162
cumulative_train_loss:  1.38424522704
*------------------------------------------------------------------------------*
Epoch 6, batch 2839:
batchly_train_loss:  1.39521054384
cumulative_train_loss:  1.38432247478
*------------------------------------------------------------------------------*
Epoch 6, batch 2859:
batchly_train_loss:  1.46615023567
cumulative_train_loss:  1.38489489703
*------------------------------------------------------------------------------*
Epoch 6, batch 2879:
batchly_train_loss:  1.18132074187
cumulative_train_loss:  1.38348069658
*------------------------------------------------------------------------------*
Epoch 6, batch 2899:
batchly_train_loss:  1.50131625668
cumulative_train_loss:  1.38429363594
*------------------------------------------------------------------------------*
Epoch 6, batch 2919:
batchly_train_loss:  1.05025920759
cumulative_train_loss:  1.3820049451
*------------------------------------------------------------------------------*
Epoch 6, batch 2939:
batchly_train_loss:  1.2864509299
cumulative_train_loss:  1.38135469661
*------------------------------------------------------------------------------*
Epoch 6, batch 2959:
batchly_train_loss:  1.72368536245
cumulative_train_loss:  1.38366852335
*------------------------------------------------------------------------------*
Epoch 6, batch 2979:
batchly_train_loss:  1.29864109184
cumulative_train_loss:  1.38309767789
*------------------------------------------------------------------------------*
Epoch 6, batch 2999:
batchly_train_loss:  1.66333276614
cumulative_train_loss:  1.38496653476
*------------------------------------------------------------------------------*
Epoch 6, batch 3019:
batchly_train_loss:  1.53677766179
cumulative_train_loss:  1.38597223948
*------------------------------------------------------------------------------*
Epoch 6, batch 3039:
batchly_train_loss:  1.35838561283
cumulative_train_loss:  1.38579068879
*------------------------------------------------------------------------------*
Epoch 6, batch 3059:
batchly_train_loss:  1.14073022293
cumulative_train_loss:  1.3841884628
*------------------------------------------------------------------------------*
Epoch 6, batch 3079:
batchly_train_loss:  1.56193139064
cumulative_train_loss:  1.38534301251
*------------------------------------------------------------------------------*
Epoch 6, batch 3099:
batchly_train_loss:  1.48751475012
cumulative_train_loss:  1.38600239771
*------------------------------------------------------------------------------*
Epoch 6, batch 3119:
batchly_train_loss:  1.11497788263
cumulative_train_loss:  1.38426450406
*------------------------------------------------------------------------------*
Epoch 6, batch 3139:
batchly_train_loss:  1.35709585413
cumulative_train_loss:  1.38409140021
*------------------------------------------------------------------------------*
Epoch 6, batch 3159:
batchly_train_loss:  1.15881973205
cumulative_train_loss:  1.38266517882
*------------------------------------------------------------------------------*
Epoch 6, batch 3179:
batchly_train_loss:  1.35148840216
cumulative_train_loss:  1.38246903678
*------------------------------------------------------------------------------*
Epoch 6, batch 3199:
batchly_train_loss:  1.47990064551
cumulative_train_loss:  1.38307817469
*------------------------------------------------------------------------------*
Epoch 6, batch 3219:
batchly_train_loss:  1.12780964718
cumulative_train_loss:  1.38149216334
*------------------------------------------------------------------------------*
Epoch 6, batch 3239:
batchly_train_loss:  1.19802224434
cumulative_train_loss:  1.38035928332
*------------------------------------------------------------------------------*
Epoch 6, batch 3259:
batchly_train_loss:  1.37276055972
cumulative_train_loss:  1.38031265108
*------------------------------------------------------------------------------*
Epoch 6, batch 3279:
batchly_train_loss:  1.30330901118
cumulative_train_loss:  1.3798429735
*------------------------------------------------------------------------------*
Epoch 6, batch 3299:
batchly_train_loss:  1.02326230737
cumulative_train_loss:  1.37768122347
*------------------------------------------------------------------------------*
Epoch 6, batch 3319:
batchly_train_loss:  1.39538374278
cumulative_train_loss:  1.37778789729
*------------------------------------------------------------------------------*
Epoch 6, batch 3339:
batchly_train_loss:  1.38185937114
cumulative_train_loss:  1.37781228467
*------------------------------------------------------------------------------*
Epoch 6, batch 3359:
batchly_train_loss:  1.59243505718
cumulative_train_loss:  1.3790901815
*------------------------------------------------------------------------------*
Epoch 6, batch 3379:
batchly_train_loss:  1.45528505941
cumulative_train_loss:  1.3795411722
*------------------------------------------------------------------------------*
Epoch 6, batch 3399:
batchly_train_loss:  1.6971319323
cumulative_train_loss:  1.38140990276
*------------------------------------------------------------------------------*
Epoch 6, batch 3419:
batchly_train_loss:  1.36704561044
cumulative_train_loss:  1.38132587649
*------------------------------------------------------------------------------*
Epoch 6, batch 3439:
batchly_train_loss:  1.63098202636
cumulative_train_loss:  1.3827777878
*------------------------------------------------------------------------------*
Epoch 6, batch 3459:
batchly_train_loss:  1.10677508996
cumulative_train_loss:  1.38118193525
*------------------------------------------------------------------------------*
Epoch 6, batch 3479:
batchly_train_loss:  1.17959250015
cumulative_train_loss:  1.38002304226
*------------------------------------------------------------------------------*
Epoch 6, batch 3499:
batchly_train_loss:  1.48377566049
cumulative_train_loss:  1.38061608381
*------------------------------------------------------------------------------*
Epoch 6, batch 3519:
batchly_train_loss:  1.01643041977
cumulative_train_loss:  1.37854625906
*------------------------------------------------------------------------------*
Epoch 6, batch 3539:
batchly_train_loss:  1.75975588789
cumulative_train_loss:  1.38070059435
*------------------------------------------------------------------------------*
Epoch 6, batch 3559:
batchly_train_loss:  1.63918613225
cumulative_train_loss:  1.38215316832
*------------------------------------------------------------------------------*
Epoch 6, batch 3579:
batchly_train_loss:  1.05031998263
cumulative_train_loss:  1.38029883367
*------------------------------------------------------------------------------*
Epoch 6, batch 3599:
batchly_train_loss:  1.14654854681
cumulative_train_loss:  1.37899986014
*------------------------------------------------------------------------------*
Epoch 6, batch 3619:
batchly_train_loss:  1.17823125727
cumulative_train_loss:  1.37789033484
*------------------------------------------------------------------------------*
Epoch 6, batch 3639:
batchly_train_loss:  1.62220793568
cumulative_train_loss:  1.37923310813
*------------------------------------------------------------------------------*
Epoch 6, batch 3659:
batchly_train_loss:  1.3365485582
cumulative_train_loss:  1.37899979548
*------------------------------------------------------------------------------*
Epoch 6, batch 3679:
batchly_train_loss:  1.61467655892
cumulative_train_loss:  1.38028099561
*------------------------------------------------------------------------------*
Epoch 6, batch 3699:
batchly_train_loss:  1.38351314014
cumulative_train_loss:  1.38029847138
*------------------------------------------------------------------------------*
Epoch 6, batch 3719:
batchly_train_loss:  1.46802775751
cumulative_train_loss:  1.38077026103
*------------------------------------------------------------------------------*
Epoch 6, batch 3739:
batchly_train_loss:  1.40369626458
cumulative_train_loss:  1.38089289277
*------------------------------------------------------------------------------*
Epoch 6, batch 3759:
batchly_train_loss:  1.28356795424
cumulative_train_loss:  1.38037506921
*------------------------------------------------------------------------------*
Epoch 6, batch 3779:
batchly_train_loss:  1.1952821398
cumulative_train_loss:  1.37939548239
*------------------------------------------------------------------------------*
Epoch 6, batch 3799:
batchly_train_loss:  1.36170754735
cumulative_train_loss:  1.37930236349
*------------------------------------------------------------------------------*
Epoch 6, batch 3819:
batchly_train_loss:  1.73866299086
cumulative_train_loss:  1.38118432541
*------------------------------------------------------------------------------*
Epoch 6, batch 3839:
batchly_train_loss:  1.57688666448
cumulative_train_loss:  1.38220387393
*------------------------------------------------------------------------------*
Epoch 6, batch 3859:
batchly_train_loss:  1.3901106106
cumulative_train_loss:  1.38224485209
*------------------------------------------------------------------------------*
Epoch 6, batch 3879:
batchly_train_loss:  1.1713924559
cumulative_train_loss:  1.38115770388
*------------------------------------------------------------------------------*
Epoch 6, batch 3899:
batchly_train_loss:  1.57183214463
cumulative_train_loss:  1.38213577231
*------------------------------------------------------------------------------*
Epoch 6, batch 3919:
batchly_train_loss:  1.34111451355
cumulative_train_loss:  1.38192642677
*------------------------------------------------------------------------------*
Epoch 6, batch 3939:
batchly_train_loss:  1.35575749799
cumulative_train_loss:  1.38179355584
*------------------------------------------------------------------------------*
Epoch 6, batch 3959:
batchly_train_loss:  1.32776442381
cumulative_train_loss:  1.38152061251
*------------------------------------------------------------------------------*
Epoch 6, batch 3979:
batchly_train_loss:  1.52196210892
cumulative_train_loss:  1.38222652604
*------------------------------------------------------------------------------*
Epoch 6, batch 3999:
batchly_train_loss:  1.62292432228
cumulative_train_loss:  1.38343031597
================================================================================
Epoch 6 of 8 took 6432.736s
  training loss:		1.383256
evaluating model...
VALID_LOSS:  1.43717328269
VALID_ACC:  0.5
FULL_TRAIN_LOSS:  1.33622209829
FULL_TRAIN_ACC:  0.524975124378
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=6
*------------------------------------------------------------------------------*
Epoch 7, batch 19:
batchly_train_loss:  1.62858173083
cumulative_train_loss:  1.6053768915
*------------------------------------------------------------------------------*
Epoch 7, batch 39:
batchly_train_loss:  1.11289396544
cumulative_train_loss:  1.3528215448
*------------------------------------------------------------------------------*
Epoch 7, batch 59:
batchly_train_loss:  1.22717943601
cumulative_train_loss:  1.31023099945
*------------------------------------------------------------------------------*
Epoch 7, batch 79:
batchly_train_loss:  1.22874617423
cumulative_train_loss:  1.28960192977
*------------------------------------------------------------------------------*
Epoch 7, batch 99:
batchly_train_loss:  1.03766075035
cumulative_train_loss:  1.23870472181
*------------------------------------------------------------------------------*
Epoch 7, batch 119:
batchly_train_loss:  1.84095602388
cumulative_train_loss:  1.33992342804
*------------------------------------------------------------------------------*
Epoch 7, batch 139:
batchly_train_loss:  1.37097452117
cumulative_train_loss:  1.34439121122
*------------------------------------------------------------------------------*
Epoch 7, batch 159:
batchly_train_loss:  1.308387321
cumulative_train_loss:  1.33986242
*------------------------------------------------------------------------------*
Epoch 7, batch 179:
batchly_train_loss:  1.23708204048
cumulative_train_loss:  1.32837857871
*------------------------------------------------------------------------------*
Epoch 7, batch 199:
batchly_train_loss:  0.916126608752
cumulative_train_loss:  1.28694621992
*------------------------------------------------------------------------------*
Epoch 7, batch 219:
batchly_train_loss:  1.27818822767
cumulative_train_loss:  1.28614640328
*------------------------------------------------------------------------------*
Epoch 7, batch 239:
batchly_train_loss:  1.21920364006
cumulative_train_loss:  1.28054449841
*------------------------------------------------------------------------------*
Epoch 7, batch 259:
batchly_train_loss:  1.36407976038
cumulative_train_loss:  1.28699509779
*------------------------------------------------------------------------------*
Epoch 7, batch 279:
batchly_train_loss:  1.3314709603
cumulative_train_loss:  1.29018333166
*------------------------------------------------------------------------------*
Epoch 7, batch 299:
batchly_train_loss:  1.62557032216
cumulative_train_loss:  1.31261724407
*------------------------------------------------------------------------------*
Epoch 7, batch 319:
batchly_train_loss:  1.58299368272
cumulative_train_loss:  1.32956874492
*------------------------------------------------------------------------------*
Epoch 7, batch 339:
batchly_train_loss:  1.66869462984
cumulative_train_loss:  1.34957617176
*------------------------------------------------------------------------------*
Epoch 7, batch 359:
batchly_train_loss:  1.07380898359
cumulative_train_loss:  1.33421309721
*------------------------------------------------------------------------------*
Epoch 7, batch 379:
batchly_train_loss:  1.09765924304
cumulative_train_loss:  1.32173004422
*------------------------------------------------------------------------------*
Epoch 7, batch 399:
batchly_train_loss:  1.07529647274
cumulative_train_loss:  1.30937748425
*------------------------------------------------------------------------------*
Epoch 7, batch 419:
batchly_train_loss:  1.7207049334
cumulative_train_loss:  1.3290112527
*------------------------------------------------------------------------------*
Epoch 7, batch 439:
batchly_train_loss:  1.48817123955
cumulative_train_loss:  1.33626227716
*------------------------------------------------------------------------------*
Epoch 7, batch 459:
batchly_train_loss:  1.24366660621
cumulative_train_loss:  1.3322276074
*------------------------------------------------------------------------------*
Epoch 7, batch 479:
batchly_train_loss:  1.33846262639
cumulative_train_loss:  1.33248794222
*------------------------------------------------------------------------------*
Epoch 7, batch 499:
batchly_train_loss:  1.096483475
cumulative_train_loss:  1.32302884534
*------------------------------------------------------------------------------*
Epoch 7, batch 519:
batchly_train_loss:  0.839231140242
cumulative_train_loss:  1.3043853885
*------------------------------------------------------------------------------*
Epoch 7, batch 539:
batchly_train_loss:  1.55506835565
cumulative_train_loss:  1.31368716836
*------------------------------------------------------------------------------*
Epoch 7, batch 559:
batchly_train_loss:  1.31817541377
cumulative_train_loss:  1.31384774959
*------------------------------------------------------------------------------*
Epoch 7, batch 579:
batchly_train_loss:  1.26550125135
cumulative_train_loss:  1.31217774965
*------------------------------------------------------------------------------*
Epoch 7, batch 599:
batchly_train_loss:  1.11343734224
cumulative_train_loss:  1.30554200983
*------------------------------------------------------------------------------*
Epoch 7, batch 619:
batchly_train_loss:  1.14373676956
cumulative_train_loss:  1.30031405377
*------------------------------------------------------------------------------*
Epoch 7, batch 639:
batchly_train_loss:  1.54246350845
cumulative_train_loss:  1.30789306643
*------------------------------------------------------------------------------*
Epoch 7, batch 659:
batchly_train_loss:  1.7123792735
cumulative_train_loss:  1.32016882386
*------------------------------------------------------------------------------*
Epoch 7, batch 679:
batchly_train_loss:  1.74366982209
cumulative_train_loss:  1.33264308006
*------------------------------------------------------------------------------*
Epoch 7, batch 699:
batchly_train_loss:  1.36923061124
cumulative_train_loss:  1.3336899336
*------------------------------------------------------------------------------*
Epoch 7, batch 719:
batchly_train_loss:  1.90337848848
cumulative_train_loss:  1.34953662498
*------------------------------------------------------------------------------*
Epoch 7, batch 739:
batchly_train_loss:  1.6023612489
cumulative_train_loss:  1.35637896933
*------------------------------------------------------------------------------*
Epoch 7, batch 759:
batchly_train_loss:  1.77486659693
cumulative_train_loss:  1.3674063113
*------------------------------------------------------------------------------*
Epoch 7, batch 779:
batchly_train_loss:  1.47480074208
cumulative_train_loss:  1.37016354957
*------------------------------------------------------------------------------*
Epoch 7, batch 799:
batchly_train_loss:  1.13386604581
cumulative_train_loss:  1.36424871844
*------------------------------------------------------------------------------*
Epoch 7, batch 819:
batchly_train_loss:  1.07566433761
cumulative_train_loss:  1.35720148081
*------------------------------------------------------------------------------*
Epoch 7, batch 839:
batchly_train_loss:  1.31829505035
cumulative_train_loss:  1.35627403312
*------------------------------------------------------------------------------*
Epoch 7, batch 859:
batchly_train_loss:  1.52295677619
cumulative_train_loss:  1.36015488861
*------------------------------------------------------------------------------*
Epoch 7, batch 879:
batchly_train_loss:  1.27877870868
cumulative_train_loss:  1.35830332593
*------------------------------------------------------------------------------*
Epoch 7, batch 899:
batchly_train_loss:  1.13351186239
cumulative_train_loss:  1.35330240349
*------------------------------------------------------------------------------*
Epoch 7, batch 919:
batchly_train_loss:  1.26522669329
cumulative_train_loss:  1.35138563069
*------------------------------------------------------------------------------*
Epoch 7, batch 939:
batchly_train_loss:  1.32403626739
cumulative_train_loss:  1.35080310964
*------------------------------------------------------------------------------*
Epoch 7, batch 959:
batchly_train_loss:  1.53611508871
cumulative_train_loss:  1.35466780159
*------------------------------------------------------------------------------*
Epoch 7, batch 979:
batchly_train_loss:  1.25700186629
cumulative_train_loss:  1.3526725833
*------------------------------------------------------------------------------*
Epoch 7, batch 999:
batchly_train_loss:  1.37830983815
cumulative_train_loss:  1.35318584165
*------------------------------------------------------------------------------*
Epoch 7, batch 1019:
batchly_train_loss:  0.861330345389
cumulative_train_loss:  1.34353215184
*------------------------------------------------------------------------------*
Epoch 7, batch 1039:
batchly_train_loss:  0.831769305004
cumulative_train_loss:  1.33368108645
*------------------------------------------------------------------------------*
Epoch 7, batch 1059:
batchly_train_loss:  1.53623467982
cumulative_train_loss:  1.33750646121
*------------------------------------------------------------------------------*
Epoch 7, batch 1079:
batchly_train_loss:  1.44260014648
cumulative_train_loss:  1.33945444425
*------------------------------------------------------------------------------*
Epoch 7, batch 1099:
batchly_train_loss:  1.34059247255
cumulative_train_loss:  1.3394751545
*------------------------------------------------------------------------------*
Epoch 7, batch 1119:
batchly_train_loss:  1.03450252974
cumulative_train_loss:  1.33402434798
*------------------------------------------------------------------------------*
Epoch 7, batch 1139:
batchly_train_loss:  1.4661323977
cumulative_train_loss:  1.33634406791
*------------------------------------------------------------------------------*
Epoch 7, batch 1159:
batchly_train_loss:  1.31067273019
cumulative_train_loss:  1.33590107675
*------------------------------------------------------------------------------*
Epoch 7, batch 1179:
batchly_train_loss:  1.11900168607
cumulative_train_loss:  1.33222169777
*------------------------------------------------------------------------------*
Epoch 7, batch 1199:
batchly_train_loss:  0.990601782561
cumulative_train_loss:  1.32652328384
*------------------------------------------------------------------------------*
Epoch 7, batch 1219:
batchly_train_loss:  1.49749444014
cumulative_train_loss:  1.32932838895
*------------------------------------------------------------------------------*
Epoch 7, batch 1239:
batchly_train_loss:  1.36993808254
cumulative_train_loss:  1.32998391265
*------------------------------------------------------------------------------*
Epoch 7, batch 1259:
batchly_train_loss:  1.24266467644
cumulative_train_loss:  1.32859679214
*------------------------------------------------------------------------------*
Epoch 7, batch 1279:
batchly_train_loss:  1.27238360718
cumulative_train_loss:  1.32771777439
*------------------------------------------------------------------------------*
Epoch 7, batch 1299:
batchly_train_loss:  1.12234811287
cumulative_train_loss:  1.32455580886
*------------------------------------------------------------------------------*
Epoch 7, batch 1319:
batchly_train_loss:  1.70923837765
cumulative_train_loss:  1.33038875152
*------------------------------------------------------------------------------*
Epoch 7, batch 1339:
batchly_train_loss:  1.07189148615
cumulative_train_loss:  1.326527702
*------------------------------------------------------------------------------*
Epoch 7, batch 1359:
batchly_train_loss:  1.3176681973
cumulative_train_loss:  1.3263973193
*------------------------------------------------------------------------------*
Epoch 7, batch 1379:
batchly_train_loss:  1.40511247606
cumulative_train_loss:  1.32753894594
*------------------------------------------------------------------------------*
Epoch 7, batch 1399:
batchly_train_loss:  0.831452937315
cumulative_train_loss:  1.32044693724
*------------------------------------------------------------------------------*
Epoch 7, batch 1419:
batchly_train_loss:  1.38450346217
cumulative_train_loss:  1.32134977762
*------------------------------------------------------------------------------*
Epoch 7, batch 1439:
batchly_train_loss:  1.25899867907
cumulative_train_loss:  1.32048318834
*------------------------------------------------------------------------------*
Epoch 7, batch 1459:
batchly_train_loss:  1.26646054055
cumulative_train_loss:  1.31974264485
*------------------------------------------------------------------------------*
Epoch 7, batch 1479:
batchly_train_loss:  1.49562694377
cumulative_train_loss:  1.32212106674
*------------------------------------------------------------------------------*
Epoch 7, batch 1499:
batchly_train_loss:  1.62205617118
cumulative_train_loss:  1.32612286933
*------------------------------------------------------------------------------*
Epoch 7, batch 1519:
batchly_train_loss:  1.44205710658
cumulative_train_loss:  1.32764932407
*------------------------------------------------------------------------------*
Epoch 7, batch 1539:
batchly_train_loss:  1.39767108654
cumulative_train_loss:  1.32855928849
*------------------------------------------------------------------------------*
Epoch 7, batch 1559:
batchly_train_loss:  1.11342155298
cumulative_train_loss:  1.3257993432
*------------------------------------------------------------------------------*
Epoch 7, batch 1579:
batchly_train_loss:  1.34065564656
cumulative_train_loss:  1.32598751677
*------------------------------------------------------------------------------*
Epoch 7, batch 1599:
batchly_train_loss:  1.50880483176
cumulative_train_loss:  1.32827416236
*------------------------------------------------------------------------------*
Epoch 7, batch 1619:
batchly_train_loss:  1.61756885133
cumulative_train_loss:  1.33184790775
*------------------------------------------------------------------------------*
Epoch 7, batch 1639:
batchly_train_loss:  1.69701329817
cumulative_train_loss:  1.33630386126
*------------------------------------------------------------------------------*
Epoch 7, batch 1659:
batchly_train_loss:  1.71512253199
cumulative_train_loss:  1.34087069274
*------------------------------------------------------------------------------*
Epoch 7, batch 1679:
batchly_train_loss:  1.67971630703
cumulative_train_loss:  1.34490697164
*------------------------------------------------------------------------------*
Epoch 7, batch 1699:
batchly_train_loss:  1.58075367144
cumulative_train_loss:  1.34768327182
*------------------------------------------------------------------------------*
Epoch 7, batch 1719:
batchly_train_loss:  0.919012807584
cumulative_train_loss:  1.34269583186
*------------------------------------------------------------------------------*
Epoch 7, batch 1739:
batchly_train_loss:  1.56661298906
cumulative_train_loss:  1.34527107231
*------------------------------------------------------------------------------*
Epoch 7, batch 1759:
batchly_train_loss:  1.81679379726
cumulative_train_loss:  1.35063233127
*------------------------------------------------------------------------------*
Epoch 7, batch 1779:
batchly_train_loss:  1.58813699154
cumulative_train_loss:  1.35330242301
*------------------------------------------------------------------------------*
Epoch 7, batch 1799:
batchly_train_loss:  1.08859699241
cumulative_train_loss:  1.35035961666
*------------------------------------------------------------------------------*
Epoch 7, batch 1819:
batchly_train_loss:  1.19332863449
cumulative_train_loss:  1.34863305281
*------------------------------------------------------------------------------*
Epoch 7, batch 1839:
batchly_train_loss:  0.810647000558
cumulative_train_loss:  1.34278219852
*------------------------------------------------------------------------------*
Epoch 7, batch 1859:
batchly_train_loss:  1.42954453372
cumulative_train_loss:  1.3437156287
*------------------------------------------------------------------------------*
Epoch 7, batch 1879:
batchly_train_loss:  1.80163505318
cumulative_train_loss:  1.34858970453
*------------------------------------------------------------------------------*
Epoch 7, batch 1899:
batchly_train_loss:  1.49882537792
cumulative_train_loss:  1.35017196544
*------------------------------------------------------------------------------*
Epoch 7, batch 1919:
batchly_train_loss:  1.51056481205
cumulative_train_loss:  1.3518435949
*------------------------------------------------------------------------------*
Epoch 7, batch 1939:
batchly_train_loss:  1.55860676944
cumulative_train_loss:  1.35397627334
*------------------------------------------------------------------------------*
Epoch 7, batch 1959:
batchly_train_loss:  1.268316879
cumulative_train_loss:  1.3531017517
*------------------------------------------------------------------------------*
Epoch 7, batch 1979:
batchly_train_loss:  1.10832992003
cumulative_train_loss:  1.35062805962
*------------------------------------------------------------------------------*
Epoch 7, batch 1999:
batchly_train_loss:  1.40742155964
cumulative_train_loss:  1.35119627873
*------------------------------------------------------------------------------*
Epoch 7, batch 2019:
batchly_train_loss:  1.18983462038
cumulative_train_loss:  1.34959784724
*------------------------------------------------------------------------------*
Epoch 7, batch 2039:
batchly_train_loss:  1.63665748112
cumulative_train_loss:  1.35241353762
*------------------------------------------------------------------------------*
Epoch 7, batch 2059:
batchly_train_loss:  1.04637229704
cumulative_train_loss:  1.34944082037
*------------------------------------------------------------------------------*
Epoch 7, batch 2079:
batchly_train_loss:  1.38883581887
cumulative_train_loss:  1.34981980064
*------------------------------------------------------------------------------*
Epoch 7, batch 2099:
batchly_train_loss:  1.33225176704
cumulative_train_loss:  1.34965240632
*------------------------------------------------------------------------------*
Epoch 7, batch 2119:
batchly_train_loss:  1.27548868065
cumulative_train_loss:  1.34895241835
*------------------------------------------------------------------------------*
Epoch 7, batch 2139:
batchly_train_loss:  1.33022593527
cumulative_train_loss:  1.34877732267
*------------------------------------------------------------------------------*
Epoch 7, batch 2159:
batchly_train_loss:  1.38995936408
cumulative_train_loss:  1.34915881448
*------------------------------------------------------------------------------*
Epoch 7, batch 2179:
batchly_train_loss:  1.80442182416
cumulative_train_loss:  1.35333745615
*------------------------------------------------------------------------------*
Epoch 7, batch 2199:
batchly_train_loss:  1.79634150708
cumulative_train_loss:  1.35736659713
*------------------------------------------------------------------------------*
Epoch 7, batch 2219:
batchly_train_loss:  1.51071583798
cumulative_train_loss:  1.35874874441
*------------------------------------------------------------------------------*
Epoch 7, batch 2239:
batchly_train_loss:  1.5770639886
cumulative_train_loss:  1.36069885825
*------------------------------------------------------------------------------*
Epoch 7, batch 2259:
batchly_train_loss:  1.37786397665
cumulative_train_loss:  1.3608508292
*------------------------------------------------------------------------------*
Epoch 7, batch 2279:
batchly_train_loss:  1.55260041595
cumulative_train_loss:  1.36253358116
*------------------------------------------------------------------------------*
Epoch 7, batch 2299:
batchly_train_loss:  1.29812860023
cumulative_train_loss:  1.36197329425
*------------------------------------------------------------------------------*
Epoch 7, batch 2319:
batchly_train_loss:  1.35410541087
cumulative_train_loss:  1.36190543842
*------------------------------------------------------------------------------*
Epoch 7, batch 2339:
batchly_train_loss:  1.29390891652
cumulative_train_loss:  1.3613240231
*------------------------------------------------------------------------------*
Epoch 7, batch 2359:
batchly_train_loss:  2.25012407415
cumulative_train_loss:  1.36885941988
*------------------------------------------------------------------------------*
Epoch 7, batch 2379:
batchly_train_loss:  1.32156568441
cumulative_train_loss:  1.36846182648
*------------------------------------------------------------------------------*
Epoch 7, batch 2399:
batchly_train_loss:  1.32309045835
cumulative_train_loss:  1.36808357414
*------------------------------------------------------------------------------*
Epoch 7, batch 2419:
batchly_train_loss:  1.29200821509
cumulative_train_loss:  1.36745459226
*------------------------------------------------------------------------------*
Epoch 7, batch 2439:
batchly_train_loss:  1.47644098862
cumulative_train_loss:  1.36834828964
*------------------------------------------------------------------------------*
Epoch 7, batch 2459:
batchly_train_loss:  1.79968947807
cumulative_train_loss:  1.3718565547
*------------------------------------------------------------------------------*
Epoch 7, batch 2479:
batchly_train_loss:  1.45392062592
cumulative_train_loss:  1.37251862869
*------------------------------------------------------------------------------*
Epoch 7, batch 2499:
batchly_train_loss:  1.27077880875
cumulative_train_loss:  1.37170438443
*------------------------------------------------------------------------------*
Epoch 7, batch 2519:
batchly_train_loss:  1.01711135644
cumulative_train_loss:  1.36888903685
*------------------------------------------------------------------------------*
Epoch 7, batch 2539:
batchly_train_loss:  1.73839455691
cumulative_train_loss:  1.37179967505
*------------------------------------------------------------------------------*
Epoch 7, batch 2559:
batchly_train_loss:  1.63598544575
cumulative_train_loss:  1.37386443293
*------------------------------------------------------------------------------*
Epoch 7, batch 2579:
batchly_train_loss:  1.21575622143
cumulative_train_loss:  1.37263831264
*------------------------------------------------------------------------------*
Epoch 7, batch 2599:
batchly_train_loss:  1.56624516373
cumulative_train_loss:  1.37412816913
*------------------------------------------------------------------------------*
Epoch 7, batch 2619:
batchly_train_loss:  1.44145557291
cumulative_train_loss:  1.37464231502
*------------------------------------------------------------------------------*
Epoch 7, batch 2639:
batchly_train_loss:  1.02362750244
cumulative_train_loss:  1.37198210424
*------------------------------------------------------------------------------*
Epoch 7, batch 2659:
batchly_train_loss:  1.1683643451
cumulative_train_loss:  1.37045056788
*------------------------------------------------------------------------------*
Epoch 7, batch 2679:
batchly_train_loss:  1.2020198188
cumulative_train_loss:  1.3691931528
*------------------------------------------------------------------------------*
Epoch 7, batch 2699:
batchly_train_loss:  1.19824916938
cumulative_train_loss:  1.36792643192
*------------------------------------------------------------------------------*
Epoch 7, batch 2719:
batchly_train_loss:  1.16996163782
cumulative_train_loss:  1.36647027308
*------------------------------------------------------------------------------*
Epoch 7, batch 2739:
batchly_train_loss:  1.16752108316
cumulative_train_loss:  1.36501755902
*------------------------------------------------------------------------------*
Epoch 7, batch 2759:
batchly_train_loss:  1.39929098724
cumulative_train_loss:  1.36526600722
*------------------------------------------------------------------------------*
Epoch 7, batch 2779:
batchly_train_loss:  1.16323666918
cumulative_train_loss:  1.36381203573
*------------------------------------------------------------------------------*
Epoch 7, batch 2799:
batchly_train_loss:  1.43049810339
cumulative_train_loss:  1.36428853496
*------------------------------------------------------------------------------*
Epoch 7, batch 2819:
batchly_train_loss:  1.09984970786
cumulative_train_loss:  1.362412417
*------------------------------------------------------------------------------*
Epoch 7, batch 2839:
batchly_train_loss:  1.35979041291
cumulative_train_loss:  1.36239394568
*------------------------------------------------------------------------------*
Epoch 7, batch 2859:
batchly_train_loss:  1.16150073217
cumulative_train_loss:  1.36098860665
*------------------------------------------------------------------------------*
Epoch 7, batch 2879:
batchly_train_loss:  1.20931192654
cumulative_train_loss:  1.35993493052
*------------------------------------------------------------------------------*
Epoch 7, batch 2899:
batchly_train_loss:  1.23707106549
cumulative_train_loss:  1.35908730123
*------------------------------------------------------------------------------*
Epoch 7, batch 2919:
batchly_train_loss:  1.2298420098
cumulative_train_loss:  1.35820175624
*------------------------------------------------------------------------------*
Epoch 7, batch 2939:
batchly_train_loss:  1.56113832335
cumulative_train_loss:  1.35958274683
*------------------------------------------------------------------------------*
Epoch 7, batch 2959:
batchly_train_loss:  1.53396089259
cumulative_train_loss:  1.36076137573
*------------------------------------------------------------------------------*
Epoch 7, batch 2979:
batchly_train_loss:  1.69940158895
cumulative_train_loss:  1.36303489176
*------------------------------------------------------------------------------*
Epoch 7, batch 2999:
batchly_train_loss:  1.13491126423
cumulative_train_loss:  1.36151356047
*------------------------------------------------------------------------------*
Epoch 7, batch 3019:
batchly_train_loss:  1.03461973058
cumulative_train_loss:  1.35934798359
*------------------------------------------------------------------------------*
Epoch 7, batch 3039:
batchly_train_loss:  1.49382976988
cumulative_train_loss:  1.36023302331
*------------------------------------------------------------------------------*
Epoch 7, batch 3059:
batchly_train_loss:  1.13822394719
cumulative_train_loss:  1.35878150925
*------------------------------------------------------------------------------*
Epoch 7, batch 3079:
batchly_train_loss:  0.945660584797
cumulative_train_loss:  1.35609803459
*------------------------------------------------------------------------------*
Epoch 7, batch 3099:
batchly_train_loss:  1.43846268067
cumulative_train_loss:  1.35662959087
*------------------------------------------------------------------------------*
Epoch 7, batch 3119:
batchly_train_loss:  1.18903636005
cumulative_train_loss:  1.35555493085
*------------------------------------------------------------------------------*
Epoch 7, batch 3139:
batchly_train_loss:  1.03089793528
cumulative_train_loss:  1.35348639312
*------------------------------------------------------------------------------*
Epoch 7, batch 3159:
batchly_train_loss:  1.58567286057
cumulative_train_loss:  1.35495639292
*------------------------------------------------------------------------------*
Epoch 7, batch 3179:
batchly_train_loss:  1.20793934916
cumulative_train_loss:  1.35403146656
*------------------------------------------------------------------------------*
Epoch 7, batch 3199:
batchly_train_loss:  0.911599133556
cumulative_train_loss:  1.35126540009
*------------------------------------------------------------------------------*
Epoch 7, batch 3219:
batchly_train_loss:  1.23469092826
cumulative_train_loss:  1.35054111011
*------------------------------------------------------------------------------*
Epoch 7, batch 3239:
batchly_train_loss:  1.32035406844
cumulative_train_loss:  1.35035471282
*------------------------------------------------------------------------------*
Epoch 7, batch 3259:
batchly_train_loss:  1.05163931496
cumulative_train_loss:  1.34852154069
*------------------------------------------------------------------------------*
Epoch 7, batch 3279:
batchly_train_loss:  1.16708914808
cumulative_train_loss:  1.34741490823
*------------------------------------------------------------------------------*
Epoch 7, batch 3299:
batchly_train_loss:  1.79837816998
cumulative_train_loss:  1.35014884737
*------------------------------------------------------------------------------*
Epoch 7, batch 3319:
batchly_train_loss:  1.42725998078
cumulative_train_loss:  1.35061351223
*------------------------------------------------------------------------------*
Epoch 7, batch 3339:
batchly_train_loss:  1.54538694741
cumulative_train_loss:  1.35178016952
*------------------------------------------------------------------------------*
Epoch 7, batch 3359:
batchly_train_loss:  1.81934532743
cumulative_train_loss:  1.35456412402
*------------------------------------------------------------------------------*
Epoch 7, batch 3379:
batchly_train_loss:  1.56296785129
cumulative_train_loss:  1.35579764712
*------------------------------------------------------------------------------*
Epoch 7, batch 3399:
batchly_train_loss:  1.5882812549
cumulative_train_loss:  1.35716560009
*------------------------------------------------------------------------------*
Epoch 7, batch 3419:
batchly_train_loss:  1.22877214799
cumulative_train_loss:  1.35641454158
*------------------------------------------------------------------------------*
Epoch 7, batch 3439:
batchly_train_loss:  1.27243987834
cumulative_train_loss:  1.35592617483
*------------------------------------------------------------------------------*
Epoch 7, batch 3459:
batchly_train_loss:  1.51300524466
cumulative_train_loss:  1.35683440883
*------------------------------------------------------------------------------*
Epoch 7, batch 3479:
batchly_train_loss:  1.2701615313
cumulative_train_loss:  1.35633614566
*------------------------------------------------------------------------------*
Epoch 7, batch 3499:
batchly_train_loss:  1.35162046959
cumulative_train_loss:  1.35630919124
*------------------------------------------------------------------------------*
Epoch 7, batch 3519:
batchly_train_loss:  1.3574943549
cumulative_train_loss:  1.35631592704
*------------------------------------------------------------------------------*
Epoch 7, batch 3539:
batchly_train_loss:  1.43336199178
cumulative_train_loss:  1.35675133854
*------------------------------------------------------------------------------*
Epoch 7, batch 3559:
batchly_train_loss:  1.46398740064
cumulative_train_loss:  1.3573539576
*------------------------------------------------------------------------------*
Epoch 7, batch 3579:
batchly_train_loss:  1.35985370865
cumulative_train_loss:  1.35736792659
*------------------------------------------------------------------------------*
Epoch 7, batch 3599:
batchly_train_loss:  1.72545890142
cumulative_train_loss:  1.35941344465
*------------------------------------------------------------------------------*
Epoch 7, batch 3619:
batchly_train_loss:  1.34818357205
cumulative_train_loss:  1.35935138401
*------------------------------------------------------------------------------*
Epoch 7, batch 3639:
batchly_train_loss:  1.32605305705
cumulative_train_loss:  1.35916837589
*------------------------------------------------------------------------------*
Epoch 7, batch 3659:
batchly_train_loss:  1.29444710959
cumulative_train_loss:  1.35881461111
*------------------------------------------------------------------------------*
Epoch 7, batch 3679:
batchly_train_loss:  1.27313029433
cumulative_train_loss:  1.3583488089
*------------------------------------------------------------------------------*
Epoch 7, batch 3699:
batchly_train_loss:  1.23771992412
cumulative_train_loss:  1.3576965846
*------------------------------------------------------------------------------*
Epoch 7, batch 3719:
batchly_train_loss:  1.29818263758
cumulative_train_loss:  1.35737653111
*------------------------------------------------------------------------------*
Epoch 7, batch 3739:
batchly_train_loss:  1.51642769061
cumulative_train_loss:  1.35822729955
*------------------------------------------------------------------------------*
Epoch 7, batch 3759:
batchly_train_loss:  1.10578648075
cumulative_train_loss:  1.35688417202
*------------------------------------------------------------------------------*
Epoch 7, batch 3779:
batchly_train_loss:  1.18697535381
cumulative_train_loss:  1.35598494567
*------------------------------------------------------------------------------*
Epoch 7, batch 3799:
batchly_train_loss:  1.893977196
cumulative_train_loss:  1.35881722917
*------------------------------------------------------------------------------*
Epoch 7, batch 3819:
batchly_train_loss:  1.3231493446
cumulative_train_loss:  1.35863043742
*------------------------------------------------------------------------------*
Epoch 7, batch 3839:
batchly_train_loss:  1.35043628843
cumulative_train_loss:  1.35858774844
*------------------------------------------------------------------------------*
Epoch 7, batch 3859:
batchly_train_loss:  1.47297613752
cumulative_train_loss:  1.35918058798
*------------------------------------------------------------------------------*
Epoch 7, batch 3879:
batchly_train_loss:  1.04600731834
cumulative_train_loss:  1.35756587662
*------------------------------------------------------------------------------*
Epoch 7, batch 3899:
batchly_train_loss:  1.2309649634
cumulative_train_loss:  1.35691647465
*------------------------------------------------------------------------------*
Epoch 7, batch 3919:
batchly_train_loss:  1.15995286634
cumulative_train_loss:  1.35591130186
*------------------------------------------------------------------------------*
Epoch 7, batch 3939:
batchly_train_loss:  1.22451717714
cumulative_train_loss:  1.35524415728
*------------------------------------------------------------------------------*
Epoch 7, batch 3959:
batchly_train_loss:  1.78710697821
cumulative_train_loss:  1.35742583357
*------------------------------------------------------------------------------*
Epoch 7, batch 3979:
batchly_train_loss:  1.0575143714
cumulative_train_loss:  1.35591836203
*------------------------------------------------------------------------------*
Epoch 7, batch 3999:
batchly_train_loss:  1.22944398899
cumulative_train_loss:  1.35528583203
================================================================================
Epoch 7 of 8 took 6462.812s
  training loss:		1.354949
evaluating model...
VALID_LOSS:  1.51638973472
VALID_ACC:  0.531818181818
FULL_TRAIN_LOSS:  1.26999700875
FULL_TRAIN_ACC:  0.555671641791
saving model to ../saved_models/cifar_scq_tight_Mar--7-2016_epoch=7
{'valid_loss': [2.5578855144758936, 2.0372587635432819, 1.6374688046822348, 1.8492976824895537, 1.6771763719915782, 1.5020380937235276, 1.4371732826928776, 1.5163897347204283], 'full_train_acc': [0.34915422885572173, 0.28253731343283567, 0.41338308457711509, 0.34378109452736239, 0.51666666666666661, 0.52800995024875652, 0.52497512437811034, 0.55567164179104489], 'valid_acc': [0.3227272727272727, 0.25818181818181818, 0.48909090909090908, 0.38545454545454549, 0.46000000000000002, 0.48454545454545456, 0.5, 0.53181818181818186], 'batchly_train_loss': [2.4007152464911972, 1.9302073062834491, 2.2117319524638912, 2.49051843666077, 2.4821500945157564, 2.6014089764056694, 2.9267168403718484, 2.8587967397620551, 2.7590100294929294, 2.0274611919633223, 2.2262959936744311, 2.1820790898602054, 2.4678872826968887, 2.1231556056298508, 2.1439957476722982, 2.2377797651074105, 2.0694605696372941, 2.4792764215965506, 2.3275274269376314, 2.1446139434923794, 2.1502223056758489, 2.0423226761422701, 2.114610790570965, 2.0797091042163567, 2.1557124322887433, 2.1264435212829276, 2.1237463585707741, 2.0886781597994726, 2.2927441068171839, 2.236688867183259, 2.0309608924339502, 1.699750911284734, 1.9361478925047209, 2.1665061048872478, 1.97401510066159, 2.1333163709964507, 2.3360284138412681, 2.1162283787113578, 2.0377270033723733, 2.290504086078144, 2.2764325234441376, 2.2139999779789354, 2.2157586903511897, 2.1259915584415072, 2.1130045908519826, 2.0480920689268922, 2.1288238025375685, 2.1129250191629874, 2.3166095967296232, 2.290726100474326, 2.1750849460498491, 2.1037718051205627, 2.1596958032352704, 2.0234935576326492, 2.0848749670410283, 2.0945948403747936, 2.0729501753872488, 2.3122274158694545, 1.9210142317102676, 2.0794909620966524, 1.9593860786470607, 2.033642762159634, 2.1122635695294738, 2.1294364556362053, 2.0724557042290508, 2.2279887749226708, 2.0090644076632964, 1.5015726997337531, 2.0480428678928431, 1.725517950242335, 1.8599178959065683, 2.1090278361715429, 2.1249989224386843, 2.0668872396228402, 2.1742082348000276, 2.0100569052970654, 2.0462374141021673, 2.0596968145058456, 2.0087355996737419, 2.0959721179461495, 1.8656556123714361, 1.8028047056918954, 1.8836984888838946, 2.013512529043608, 2.1307523909078121, 1.8801440495017583, 1.6196282164021834, 1.9004510917542237, 2.1510502768911532, 1.7700814198119119, 2.1747255232590521, 2.2004654169001645, 2.0065686182318947, 1.8388876743902594, 1.8590068495140444, 1.9359607644260675, 2.0417840857167655, 1.6056497687493494, 2.0476542676992446, 2.1748111677045352, 2.305943749826203, 2.086864811771707, 2.0771304704700269, 1.9088904631109727, 2.2353239323311804, 2.0552718324735402, 1.9938432481053827, 1.9538875673717366, 2.0630985933648267, 1.853774043037038, 1.9610357016352147, 1.9905492310367154, 1.9535297222134367, 1.6445122428036267, 2.0176801222247782, 1.9365742563947628, 2.07947462261484, 2.0456013958528443, 2.2481443822241438, 1.9626490301543469, 1.8420001160841608, 2.3989134180560456, 2.0059273239472422, 1.9788710034331056, 1.7322492138609236, 2.0413460820200076, 1.8065846833856547, 2.1187089149354548, 2.2362176620073937, 1.84109082453603, 1.9864677065012135, 2.282154361657033, 2.128717699019413, 2.1810696862139971, 1.7498190937557847, 2.2783508098358496, 2.07638644286197, 2.0592343142926186, 2.0708311283844978, 1.9681268223533543, 2.0798486228137283, 1.9076739691691067, 1.9273400506460401, 1.8425410940688027, 2.0209117609358183, 1.9578736985766412, 1.8927396370570642, 1.8594488633756729, 1.4081964794041588, 1.7640838918545776, 2.0324728959739113, 1.9705377733089022, 2.1169922423590917, 1.981538877107895, 1.8721927806268552, 1.926427577112662, 2.0111525271641404, 1.517460634258686, 1.8142760785803778, 1.7999021358538407, 1.7763413865725366, 1.8952823233354561, 2.0123789884313181, 2.174344146095919, 2.0640659163043802, 1.9633375584319985, 1.8899270073374397, 1.8911569177861705, 2.288583453721599, 2.1288281139223182, 1.871467677615227, 1.7125074965131231, 1.7951584186065737, 1.8113622311945416, 2.4094806854038922, 1.9698889683172962, 1.704024695463584, 1.5993525242664544, 1.893059146306594, 1.678452033547345, 1.6048857769220741, 2.007909393741214, 2.1577514079222189, 1.9409669337388931, 1.762542890055792, 1.9393954725877554, 2.0908044265334742, 1.8132868955078909, 2.1829732371479316, 2.2255656651064788, 2.0452887752156319, 1.96365018469534, 2.0071368426893677, 1.8918969030939004, 1.8047216257816205, 1.9681016010910952, 2.0282208880904644, 1.5870256682709651, 1.6723313893915304, 1.3042728034422848, 1.9390579197959554, 1.703062297089889, 2.2532710514928649, 1.8513640276188965, 1.7816228613683704, 1.8193313521642971, 2.0578172178654697, 1.8559307011338966, 2.1555534509816909, 1.8722347422517207, 1.8040974183469658, 2.128037561934899, 1.7200670106225437, 1.89031087095284, 1.4901782298963753, 1.7038894405774243, 2.2201269092741844, 1.786389336253523, 1.7504873004028916, 1.940700317740959, 1.7133726280971111, 1.7361897199092957, 1.6762538525171116, 1.6792034990593785, 2.1591100906001097, 1.666754315359779, 1.7716983160106277, 1.9171965303771099, 1.8650254132377149, 1.5129634498493734, 1.526847225437405, 1.7805398529975456, 1.8047584563376458, 2.1574457854924529, 1.9489425110047827, 1.9350437856502793, 1.6957330545418803, 2.0581088874863598, 1.828648590058495, 1.7578102517163932, 1.8294802063024669, 1.95084703160138, 1.800601492698243, 1.6343858645257243, 1.4487729548158843, 1.7050375529775614, 1.5604741557695072, 1.7990837976118066, 1.9253880678463673, 1.9315618597887987, 1.8366574568696481, 1.5850996940642443, 1.7673944813505778, 1.8411321787125736, 1.8967792151024845, 1.7286750664598074, 2.1704119939804314, 1.881554018552396, 1.9346562720539566, 2.0230213448629826, 1.8154850591476528, 1.6699040768022715, 1.8771030205281813, 2.2331607486765597, 1.9792658002234649, 1.7045184263571742, 1.7138887269175818, 1.5381185930845094, 2.4394902363859439, 2.1615284212250829, 2.003534840758018, 1.6633037088452252, 1.7299366030777605, 1.5794327588759256, 1.8190159727919333, 1.9512044103593666, 1.8559960256435264, 1.9118341468132845, 1.8801635948734625, 1.7502024112181398, 2.1027789900624247, 1.5742599771680461, 1.4841177509528556, 1.7285449060205189, 2.059580720729278, 1.925302433950878, 2.1012453958261421, 1.5765347724958345, 2.1027784787825921, 1.7142038577332284, 1.7080560280705619, 1.7708537417821912, 1.6012523152446427, 2.0629343551370671, 1.6264675606494907, 1.7816381429414681, 1.6675600500043095, 1.6459323582052634, 1.8558383495994082, 1.877302403221929, 1.7780389874936475, 1.9814828605867287, 2.0437861833958872, 1.8579298807734805, 1.7523470122762568, 2.076262295270566, 1.8965893201355886, 1.5815708190704085, 1.6728273451489055, 1.5630977118999996, 1.8405348890024738, 1.5770490993386428, 1.8084127385319575, 1.9216784610120281, 1.6768810974019306, 1.4173612541599681, 1.9225951966050221, 1.8104907002985862, 1.7534271415563893, 1.7901464974394945, 1.8256506662881549, 2.1254142176533759, 1.8320795472829972, 1.8799971307395669, 1.7594877306720431, 1.953258890589932, 2.0220539227807626, 1.4726257470465065, 1.88665497657979, 1.8317594215673547, 1.5844169075736536, 1.64638115098692, 1.4858195352366803, 1.603547858534228, 1.4058421062578197, 1.8220005608846743, 1.8730335031528518, 1.7496146557348808, 1.942669337508115, 1.8975420332774662, 1.7882083342377606, 1.8113442072207326, 1.6378240232520831, 1.7173201327161109, 1.9376720733421517, 1.7711844531722511, 1.8899670099545542, 2.1122586873451672, 1.8397791216671926, 1.8322388093848481, 1.9162946457645293, 1.8176913914280846, 1.4754245869319236, 2.1068495803021241, 1.8506463117902683, 1.6932790632596812, 1.8060217964080247, 1.898962758428236, 2.078679716154773, 1.8978669688973564, 2.1622093686127646, 1.7522028106258638, 1.5358157096210188, 1.6030782753537434, 1.7999555388455923, 2.0369462867631158, 2.0335835948903327, 1.7504828517189277, 1.6701894781156459, 1.7776155359540013, 1.9788503856868398, 1.9101054287278476, 1.5654289059605422, 1.4495627332726189, 1.9641534371737503, 1.3827965214252127, 1.7461975502164937, 1.8916088942621923, 1.473659312136177, 1.7371019684572921, 1.7614611322414533, 1.5253350833724815, 1.9228708225023623, 1.4318272025769552, 1.7505398368594569, 1.8807354527422688, 1.8771425013254723, 1.9776132106551174, 1.5796966260985308, 2.0952180680470738, 1.7841375048628376, 1.5974613837310809, 1.8400296025606189, 1.9870823371594835, 1.8354604426439871, 1.804426440830482, 2.1749727137386627, 2.0144962255899088, 1.7158323693663042, 2.1876352910537848, 2.0554049676608606, 1.7994212755298418, 1.6667308114191948, 1.8210934525237903, 1.5143438015151696, 1.4629999970482532, 2.1206068979887234, 1.6675591436342194, 2.0520255287537905, 1.9895063913747701, 1.7281141355146044, 1.5842291377930431, 1.4807907989696423, 1.6360284680843449, 1.3688887040535056, 1.7926266268564519, 1.4749284673288829, 1.8783373649859911, 1.4026837825126564, 1.7297718736695404, 2.0259232951846768, 2.0367743488244328, 1.8413856718294852, 1.9556230297424133, 1.6144959725942414, 1.3522632732832653, 2.4081573734895043, 1.910784669690532, 1.6227378420718572, 1.78262423285382, 1.600186332808375, 1.793845262174838, 1.6960305608359163, 1.4971198371930752, 1.8811237146552973, 2.1658199227443076, 1.8816043554308979, 1.7874571624514899, 2.0003783275350688, 1.9139582868080756, 1.8191065825307806, 1.7405219525819238, 1.628093810706891, 1.9989487716206078, 2.0723626109142161, 1.619010731163447, 1.6979822552744341, 1.7450757974740345, 1.8957156569239846, 1.864972651132399, 1.9171285328255319, 1.9246191066084442, 1.5895434809860638, 1.9249375111140286, 1.6863634604439022, 1.7441077132697571, 1.6872642050788127, 1.7029812048716102, 1.5557602910896293, 1.4835165600722062, 1.3230540929207197, 1.7020137746703081, 1.7929545860780873, 2.1303799790796001, 1.6618696143514957, 1.8565696881924034, 1.8206490389313223, 1.6816393225286983, 1.3701684402539029, 1.2679517224905974, 1.1987390626938927, 1.4940851130887844, 1.7874494451740226, 1.7861063814610798, 1.7509684486746639, 1.5772006957317142, 1.5083981220443896, 1.9998072808892211, 1.7175532513179832, 1.7770733665949152, 1.7221568304684058, 1.8218441933841825, 1.9248353969623366, 2.0335601087206983, 1.7583835763319233, 1.5155934688456199, 1.9281070248099781, 1.6660429655951674, 1.782942430928578, 1.5640690531567478, 1.4235725814403715, 1.8603940071765677, 1.8848382964920969, 1.630634543713843, 1.6967731757315785, 1.7177841073827236, 1.7738958018211601, 1.8974449641425326, 1.7102502004347873, 1.5590615479298755, 1.3806115863172705, 1.6761048423195166, 1.4914350934224267, 1.7406134759097269, 1.7149155365681921, 1.3629289397426252, 1.4882273069060648, 1.6446887345322101, 1.8540385123235985, 1.6726774538922846, 1.7429635582368743, 1.5770124833398795, 1.9647914671930196, 1.7105315351426793, 1.7687130654004466, 1.6563436504168432, 1.6558635279875253, 1.8308527566640425, 1.9188451846324401, 1.6173342243721378, 1.5764651333778059, 1.8074156864379336, 2.0514308062981601, 1.8106313017302849, 1.9455882398740507, 1.4821174972034827, 1.5052186467380864, 1.9363679596271695, 1.7836718455250284, 1.5468428336257545, 1.8262521364359121, 1.7252519842815517, 1.8339341112999132, 1.6852957521887155, 1.6040784896203899, 1.7249809975448638, 1.5443490147181371, 1.8837494391519076, 1.4962244145275119, 1.5681664376942552, 1.9044263997964794, 1.4204431284069341, 1.4743178222983597, 2.0443247085214389, 1.762479577346538, 1.6542913523504077, 1.8724633076708035, 1.6304889754312977, 2.0704674480839809, 1.77074553113366, 1.9416238487483302, 1.8102126010214832, 1.6690261681881331, 1.9063798750404781, 1.6542954244607126, 1.7663990032625634, 1.7189718069934574, 1.700574347930117, 1.4781477518373101, 1.7732414456337064, 1.647306817966109, 1.6776663366314548, 1.4981431811780308, 1.7808901809752622, 1.6323931694192513, 1.7625990028505121, 1.8397051000329641, 1.7350376182646507, 1.8007002042371316, 1.6984660128546794, 1.5362056680004603, 1.6707298126042265, 1.6194044615786876, 1.5777822305597018, 1.7559757266946132, 1.674645656141998, 1.3883676627012322, 1.5677424980602401, 1.6196069590339461, 1.8604922708777241, 1.8206631258468466, 1.7725990968029617, 1.5321689218906767, 1.7303251007672036, 1.7927939011513068, 1.5060284223068388, 1.7748306280211685, 2.0107059622173411, 1.5152502908522547, 1.919742377855675, 1.3842000133501642, 1.5959809254181017, 1.851111974473681, 1.8331452659096996, 1.5129097513610081, 2.3789073248967538, 1.9494189351353548, 1.8181408314354417, 1.7912391279773494, 1.3386424927232801, 1.480237073140116, 1.544590419910659, 1.742176018924694, 1.9018300574877951, 1.4447798222682802, 1.6323590057187825, 1.4557723819720845, 1.6846076647419088, 1.6696090192905113, 1.4503432482942642, 1.4742984573929943, 1.7570761628236462, 2.1322985671241028, 1.8272648355358005, 1.7378965750479984, 1.7815980010866006, 1.9117746237188435, 1.3472740020356795, 1.4787520501059381, 1.9449897843777664, 1.7105887809745663, 1.6810947595256576, 1.7120040966388317, 1.2605626219943229, 1.5834872895576531, 1.1394115745052291, 1.5088562831931047, 1.5454960699902824, 1.5722846156266095, 1.3564431930137455, 1.9194441741098653, 1.4513643531638063, 1.6106388696901757, 1.5413393202255268, 2.3731263684314259, 1.7822257150560028, 1.6935383829393345, 1.7186755479139457, 1.3697137625100204, 1.5612307730671806, 1.4611959200894997, 1.7156750132845047, 1.4857583615239462, 1.3008527150902514, 1.4921838806216958, 1.4831086633147579, 1.7043853232177386, 1.6408204069086765, 1.8762993744100318, 1.3507085816567237, 1.5251146266924351, 1.670585296812066, 1.4874390277460894, 1.8493843930267666, 1.7581384531462974, 1.2514543532749478, 1.4901060402978787, 1.5612694163866461, 1.8863358309183056, 1.7209636231374699, 1.3560401357119187, 1.7803847424706163, 1.631405262897512, 1.8741755110018097, 1.6159953958195099, 1.8886064478612954, 1.3566776652894228, 1.4062956559713142, 1.6532994147059217, 1.4822257489104131, 1.3569190926834667, 1.4707108487267715, 1.547665582915241, 1.8508191827750662, 1.4123585096559594, 2.1412642294383755, 1.6382211221936589, 1.6454795082519353, 1.8251546710082664, 2.1354661355068765, 1.4344264323134837, 1.8261253301340354, 1.7669142789990666, 1.8978324714785597, 1.6794001134347947, 1.6225543370994129, 1.6062891968477857, 1.6077251699265482, 1.691811459176537, 1.5014704768715958, 1.8852717392424967, 1.5019588914674782, 1.3880134992553825, 1.9269313921152551, 1.4820548414841288, 1.7909419689699593, 1.5775885551113233, 1.7910927724382408, 1.5527463713950578, 1.4713738339232101, 1.6805282898741034, 1.5501833754387466, 1.3721326864717407, 1.9543360948082646, 1.529087073498226, 1.4994107204084879, 1.8390952542858219, 1.4726080976252145, 1.5007464815668956, 1.5033539728108345, 1.6500780929447552, 1.327326656279046, 1.3865801643797231, 1.3495334498520495, 1.7193008893974528, 1.3776455823668516, 1.6060769523090705, 1.6301737510288283, 1.685536695561447, 1.8644970016391649, 1.5914785137106289, 1.7570127834386136, 1.4838437686966355, 1.750035900928538, 2.2509211167786631, 1.5650129857482158, 1.6935028023214396, 1.88575993204121, 1.7055828317929105, 1.006038395390191, 1.7557220122787562, 1.7469675204176198, 1.5678976905307018, 1.6726841190891755, 1.5694633805920655, 1.4440596906749388, 2.0178475861792591, 1.8080656508603332, 1.3392481893867114, 1.3135902040581748, 1.6902227680428077, 1.6815999673415667, 1.5680020711372191, 1.7363029000247276, 1.4673709730545443, 1.546070486641868, 1.0626559465979302, 1.5520507273074471, 1.8036404379729549, 1.7279509341429482, 1.9741312575849572, 1.5121468287217941, 1.6595698988829575, 1.8521124620373417, 1.5380496080911896, 1.5886380192134051, 1.7445875019102073, 1.6588379200561199, 1.713317420943508, 1.7844113255941756, 1.7868499945320295, 1.5942216892198631, 1.7107779648942791, 1.7243272969742365, 1.7351650477745371, 1.348011870929807, 1.9060455023741969, 1.7692775206550091, 1.8381259473692357, 1.8214308492869478, 1.5356426053344026, 1.5343443611154766, 1.9688739741461272, 1.7342217510102962, 1.6215429561509989, 1.2613853450434767, 1.4201062964575866, 1.8131548606676897, 1.980351268460852, 1.689464370947108, 1.6645038442105673, 1.5650216084726518, 1.939113901567179, 1.6730550866715486, 1.4723428087636994, 1.5311050072221943, 1.7628829251577052, 1.6731261259239081, 1.4577145784167831, 1.6394381568307312, 1.4752072467486215, 1.4909318946366121, 1.1184355789030771, 1.7255063203520458, 1.3270979500027784, 1.1727926881058532, 1.7013703998970744, 1.5139185270900628, 1.4544187422726891, 1.9368538804028987, 1.6502945604330486, 1.3535321135115361, 1.4886252437360732, 1.6602664210785374, 1.8166251973500454, 1.6804864015607588, 1.821733450535423, 1.5756962854125789, 1.4614665493846375, 1.405337039937061, 1.199038434721128, 1.6105516249949356, 1.426556668900139, 1.4017461122852595, 1.4234869756721946, 1.0605018458446718, 1.6725610327111273, 1.9537152183350226, 1.3566091922033447, 1.9472391932151392, 1.4838491917160421, 1.9293344809218134, 1.5401660448106356, 1.3841586239432124, 1.9049514682547279, 1.5380867570664145, 1.5765194244826639, 1.4178925290083073, 1.3909164506131166, 1.7349291677746463, 1.474693341981262, 1.6374968416177169, 1.3915161958274251, 1.6636882574337732, 1.1842834422397208, 1.8751129841998453, 1.6803457336804766, 1.5235256858524009, 1.7526397960795734, 1.5437690358393958, 1.6404409273799327, 1.410008662722765, 1.6805434702294044, 1.5175735202845901, 1.5989977153820059, 1.2440233059205235, 1.6111268870339761, 1.8130132799086667, 1.0903697924247404, 1.752568257291421, 1.802574717369178, 1.397041405005524, 1.4974117021814251, 1.4530593653647337, 1.6935257615258141, 1.5716584389186372, 1.566392874091362, 0.96173252480420657, 1.4394911559908059, 1.3417874765887241, 1.6738445412593059, 1.2072755098570445, 1.5066609503007056, 1.6682472798666121, 1.5212621687241379, 1.1729219293194704, 2.0437640805240669, 1.6783132230341373, 1.736046961015089, 1.3574741623039406, 1.8872929410305836, 1.5977401859536671, 1.3007250083665007, 1.3674029282188387, 1.4487020548423002, 2.1094506859364959, 1.4337487521511276, 1.8373496894810628, 1.6254947458078779, 1.6810133877391382, 1.2712140848375018, 1.1636129369432102, 1.4971491619768227, 1.636866546914213, 1.7537950202361068, 1.6447034561408824, 1.6344632746880445, 1.5547717670132282, 1.5433964484820044, 1.48332009179581, 1.5268095089182208, 1.4759049065910139, 1.5877196285860606, 1.3375357592097621, 1.6146280418683037, 1.3719835955910402, 1.8630358582941067, 2.1573711548076133, 1.4709286429006998, 1.4032879318704194, 1.1133816865793549, 1.5774872209362041, 1.4570082652700012, 1.6653807895209884, 1.4350542961037582, 1.5448098915014892, 1.5882846392172563, 1.1369196987718972, 1.5232910158608222, 1.5483353297755005, 1.7643661013705874, 1.5047740422926996, 1.3007692844865979, 1.0809787785014131, 1.4255295411239601, 1.3147710601117544, 1.7790117065377984, 1.5421653233945316, 1.4537877053796002, 1.3230880418259008, 1.0309410066158835, 2.1309684809511054, 1.7215065431728962, 1.4676680542036151, 1.4834552861326313, 1.3004947250147645, 1.2899607767445744, 1.4318761881447351, 1.7750257531341957, 1.6784873331396806, 1.6948542625255656, 1.5127641667270331, 1.634621576521549, 1.2629946220081418, 1.762577005966413, 1.4762610268834686, 1.7819085055409052, 1.6028260907689926, 1.4546241022205595, 1.7216261767516035, 1.8480257108130611, 1.5217277726577385, 1.3099780093503723, 1.7020740038443776, 1.3658674388103873, 1.6345647467001476, 1.406316562296908, 1.4253038981825812, 1.9630380697759375, 1.5034016020256087, 1.6704453963347663, 1.4035615305787448, 1.2458562638842157, 1.6439731108622986, 1.0960790058399632, 1.2051223879306519, 2.6028803938539413, 1.2506714105015979, 1.1476013959694455, 1.3161728302107454, 1.3818824928954867, 1.49125951175423, 1.5246947676960885, 1.3798767892592192, 1.7573622858947684, 1.4172391110252405, 1.5807512164409208, 0.84182239111600943, 1.5303478806151847, 1.4019224168197355, 1.642195307026491, 1.7534852628215325, 1.5546444488948954, 1.1827056495174109, 1.3097998879354105, 1.6956137665508326, 1.4442689365606962, 1.8630793002298929, 1.349427978520459, 1.4464691785022807, 1.5192039962808512, 1.3347445453002191, 1.7225486254692963, 1.2631681467650684, 1.5612083962781531, 1.6752737905279136, 1.5980553482606257, 1.8538818073847849, 1.42220127466991, 1.5874986208827804, 1.1955028965247425, 1.8128909924086443, 1.4429663831042263, 1.2910029146414363, 1.5647236623164296, 1.396377595578699, 1.4069498087560686, 1.3468971648688328, 1.332608341747763, 1.9115109464201321, 1.5301340075505485, 1.2251604308404096, 1.8111187101712769, 1.8000812810205875, 1.4538069832387766, 1.2524843888875048, 1.3299814455947141, 1.5501347067674087, 1.4242989109963469, 1.9069135782563653, 1.5829762401374103, 1.7081178141909157, 1.6509229663815332, 1.7230696441106257, 1.6053899101350217, 1.4380590078189279, 1.3850149354261607, 1.4409673657085222, 1.5811507261490061, 1.3192505066328759, 1.3617109230011799, 1.4850628139721969, 1.7661962592250298, 1.4672680896190875, 1.2620743369047238, 1.3178763679114784, 1.4116979006573149, 1.2922364209122625, 1.344836731715336, 1.7107981109481583, 1.236075500400124, 1.8612272187169476, 1.5531948119734984, 1.7504468056298812, 1.385677653352257, 1.0647397633322238, 1.4704273606324583, 1.6636739046613989, 1.5239711162047429, 1.3923553614061892, 1.3492434695415061, 1.118900974850332, 1.3725046165140253, 2.0216454540803754, 1.6098038813218474, 1.5032474060082937, 1.5729773219448595, 1.6084013417267087, 1.779813677673761, 1.7452092911386246, 1.5814135797665716, 1.2155185391199508, 1.5185794507075843, 1.7239232386019605, 1.2656740072964192, 1.5902230808487592, 1.5575626303874313, 1.7895242567754921, 1.6114095838384124, 1.758739896902151, 1.7022401945727119, 1.4182302039644372, 1.74653849234479, 1.2673982114297027, 1.1813298401488155, 1.4251432685503314, 1.1821914495030312, 1.6630468992145193, 1.3577187302622509, 1.4690906301900746, 1.4752292838883472, 1.7254583145429525, 1.5534775666512994, 1.8028526676699577, 1.6129859003317368, 1.7068137038512643, 1.3993340991378935, 1.7725643963034377, 1.5370802681062519, 1.6221725588333051, 1.9422251561740835, 1.4020389842932881, 1.3910266690714292, 1.654995189861814, 1.3902156132284211, 1.5338307809484315, 1.0257000822233997, 1.4110206242886991, 1.7241149940810128, 1.2987289913850362, 1.0151864311506382, 1.3879849505285757, 1.4114821441945311, 1.195424724288894, 1.7319730754209455, 1.5421634226515568, 1.2062773642350508, 1.6114956984090014, 1.2911892826324778, 1.6843263185464263, 1.4052010754914606, 1.3859536218726394, 1.4016170064456526, 1.5654597030656838, 1.4579827127004177, 1.5139006085133033, 1.4139085606374278, 1.1445544125346512, 1.4498144452379635, 1.2457787089231604, 1.0857497711245492, 1.9821626953093276, 1.4142861651295235, 1.9351427802189682, 1.6284085801247343, 1.6563009948525693, 1.5343165574363375, 1.4653845677718764, 1.3714746484044145, 1.1413928724183002, 1.1224452442904835, 1.170665468138028, 1.5715212882833718, 1.6360649942080847, 1.1929018369651836, 1.1051848533408408, 1.2521219468367133, 1.45885324200083, 1.4229379193589091, 1.8092340833715355, 1.331550496197687, 1.7091394664629445, 1.7015718198259371, 1.4567752467572781, 1.478033846417464, 1.4460823820394082, 1.5194190938450831, 1.6820238919377513, 1.407834252159313, 1.5243441523398245, 1.4184101616733988, 1.2388208951121311, 1.3361040735319951, 1.3125214255020436, 1.5183674113111321, 1.1921305857560924, 1.146870752637259, 1.2000706930782008, 1.5839419640073296, 1.2431803237734267, 1.9206273572510892, 1.5920983128530302, 1.4027516459623324, 1.651260790137534, 1.4011855718120594, 1.5925769046095695, 1.6314590040551633, 1.4305461609494352, 1.4183005823373178, 1.2803964620844284, 1.2773606693027921, 1.570081306484099, 1.7722225121193595, 1.5662315336766834, 1.1531997228079702, 1.5817850690412167, 1.2243064898068596, 1.5730718758407296, 1.3626758992477133, 0.89814123131749335, 1.564974488040844, 1.2409507489763594, 1.8057256103783399, 1.516918920064803, 1.241968689938084, 1.0936365221946704, 1.6961950732886504, 1.5010621979113146, 1.5763166726196154, 1.7943284565591668, 1.238584090821421, 1.4098369011791125, 1.436946570552837, 1.4261586132977835, 1.256118029181827, 1.8190385798974653, 1.5981759270202875, 1.4446786451498133, 1.1067836241389393, 2.2284431657661403, 1.5341820649436977, 1.8556723456085347, 1.5836342676111679, 1.5365083875715009, 1.6211892590651549, 1.5572133652510458, 1.3097847607798161, 1.423856517302609, 1.857553413060516, 1.7661343988980569, 1.5334769519248348, 1.1224609448334739, 1.6536049519943443, 1.5733198004258599, 2.0839273293952725, 1.4937510778535359, 1.1877259677190541, 1.3074624440795355, 1.4565066299558493, 1.4379810583716002, 1.2887582802059865, 1.4060370151930819, 1.4577688452414286, 1.5071148115221953, 0.98458758873937513, 1.3831513953936707, 1.6547706516618206, 1.7354123612899126, 1.4559256104785268, 1.5486864867928909, 1.6896903184237015, 1.2901817147355148, 1.4488407113688511, 1.6192133497387715, 1.1701802122630129, 1.3840973181678964, 0.87569328277600467, 1.307719417854075, 0.99439417252579898, 1.2531498064193216, 1.1766831689416717, 1.2947805293166001, 1.7513588057766551, 1.474861893325121, 1.2799516552994437, 1.4977962154514632, 1.7499308873114756, 1.4298883915251297, 1.1467196586200934, 1.2271140878564846, 1.052303542620169, 0.88752806275861063, 1.0148170739769142, 1.1700381651339695, 1.0585863529203801, 1.5912029774391692, 1.5636046149807192, 1.3706614523551588, 1.6425881888881679, 1.3371103527858856, 1.0994659483822546, 1.5095653276664645, 1.6571419936672824, 1.582925419700254, 1.3840148215231918, 1.4711501831751648, 1.1710518847931308, 1.4722926931154023, 1.2868706154960581, 1.3441037134848393, 1.4078480226454355, 1.7086084918432209, 1.5763265254812902, 1.4788969698593915, 0.93652345599766895, 1.4579529866501511, 1.2627766143293524, 1.506566082183814, 1.3420658845383611, 0.99450514948095736, 1.5167605769896819, 1.7178444278586333, 1.105795122892572, 1.6125472505710561, 1.0515321827834796, 1.9740790566307627, 1.5248422343622283, 1.9702629871635569, 1.3811213949553567, 1.1424266183860543, 1.1162052907001911, 1.4060726584775385, 1.193617470929393, 1.386880281203585, 1.0549805353363881, 1.0542152820591533, 1.4162111121884622, 1.6333895194531707, 1.1430127702073771, 1.3854274634558463, 1.5431198087043676, 1.4422616338378689, 1.7946896299351913, 1.4946557129895688, 1.3706167205151432, 0.86045352692672272, 1.3449726100255439, 1.0161457802943195, 1.3422837205490894, 1.715081854347837, 1.4189756251842283, 1.1821721269102421, 1.5241259774530564, 1.5434974573865863, 1.6535913717028827, 1.1081727150647684, 1.632921353935392, 1.2915058718458807, 1.6998370306572501, 1.777558059189019, 1.3172145212696618, 1.5807689101825477, 1.3171456793400664, 1.3209610760039685, 1.1280453325077695, 1.7320383531740959, 1.5427550184685945, 1.4630586904123519, 1.7685850910848639, 1.6741835909896394, 1.6928191623406044, 1.2732414715793317, 1.0518445662279192, 0.92640453626943309, 1.1362136821924318, 1.5673131193997782, 1.1502125092745934, 1.0759133885751369, 1.1679061077758122, 1.2868756160592998, 1.3059037920237317, 1.3380021457390263, 1.7098765121471544, 1.7282408215079559, 1.1066687063773215, 1.2448219660661737, 1.211469517437215, 0.91270314561027011, 1.3859073878280479, 1.4101157868490557, 1.4193305816241024, 1.3952105438381683, 1.4661502356653535, 1.1813207418715428, 1.5013162566784142, 1.0502592075860802, 1.286450929897246, 1.7236853624478734, 1.2986410918424649, 1.6633327661419994, 1.5367776617895867, 1.3583856128274954, 1.1407302229278973, 1.5619313906399683, 1.4875147501172059, 1.1149778826329688, 1.3570958541279041, 1.158819732050697, 1.3514884021568045, 1.4799006455059205, 1.127809647179514, 1.1980222443404609, 1.3727605597164976, 1.3033090111849295, 1.0232623073739526, 1.3953837427788005, 1.3818593711355249, 1.5924350571783621, 1.4552850594082529, 1.6971319322968341, 1.36704561043574, 1.6309820263647048, 1.1067750899570838, 1.1795925001535434, 1.4837756604860843, 1.0164304197655907, 1.7597558878942465, 1.6391861322518566, 1.0503199826295151, 1.1465485468126801, 1.1782312572678932, 1.6222079356839472, 1.3365485581960175, 1.6146765589240855, 1.3835131401431942, 1.4680277575118432, 1.4036962645803999, 1.2835679542428018, 1.1952821398034577, 1.3617075473519991, 1.7386629908556102, 1.5768866644804751, 1.3901106105987628, 1.1713924559049063, 1.5718321446348753, 1.3411145135457354, 1.3557574979875771, 1.3277644238067139, 1.5219621089156612, 1.6229243222773966, 1.6285817308258053, 1.1128939654434897, 1.2271794360111452, 1.2287461742331913, 1.0376607503506639, 1.8409560238778746, 1.370974521165041, 1.3083873209993973, 1.2370820404753478, 0.91612660875227514, 1.2781882276719283, 1.2192036400633459, 1.3640797603789732, 1.3314709602998864, 1.625570322160351, 1.5829936827245377, 1.6686946298361274, 1.0738089835886009, 1.097659243044715, 1.0752964727354608, 1.7207049334041353, 1.4881712395458684, 1.2436666062075319, 1.3384626263887822, 1.0964834750020873, 0.83923114024172807, 1.5550683556521085, 1.3181754137653039, 1.2655012513546198, 1.1134373422401709, 1.143736769559567, 1.5424635084503913, 1.7123792735043881, 1.7436698220916249, 1.3692306112440922, 1.9033784884829263, 1.6023612489040666, 1.774866596931941, 1.474800742075141, 1.1338660458062384, 1.0756643376143136, 1.3182950503542528, 1.522956776192947, 1.2787787086819724, 1.1335118623871001, 1.2652266932919662, 1.3240362673891046, 1.5361150887116231, 1.2570018662876752, 1.3783098381502579, 0.8613303453889195, 0.83176930500370483, 1.536234679821127, 1.4426001464781211, 1.3405924725466793, 1.0345025297370154, 1.4661323976989062, 1.3106727301855969, 1.1190016860677274, 0.99060178256099118, 1.4974944401372683, 1.3699380825382579, 1.2426646764394857, 1.272383607184469, 1.1223481128652446, 1.7092383776472417, 1.0718914861482389, 1.3176681973005213, 1.4051124760566176, 0.83145293731450809, 1.3845034621700798, 1.2589986790715486, 1.2664605405537448, 1.4956269437702461, 1.6220561711838346, 1.4420571065762764, 1.3976710865377191, 1.1134215529756619, 1.340655646556149, 1.5088048317553604, 1.6175688513330595, 1.6970132981711441, 1.7151225319928076, 1.6797163070293348, 1.5807536714377841, 0.91901280758427006, 1.5666129890591809, 1.816793797264884, 1.5881369915398893, 1.0885969924137298, 1.1933286344914205, 0.81064700055783923, 1.4295445337169144, 1.8016350531750436, 1.4988253779163994, 1.510564812050132, 1.5586067694385839, 1.2683168789996402, 1.108329920034604, 1.4074215596406361, 1.18983462037983, 1.6366574811155423, 1.0463722970427272, 1.3888358188690417, 1.33225176704314, 1.2754886806477477, 1.3302259352733399, 1.3899593640755381, 1.8044218241551868, 1.7963415070783426, 1.510715837980463, 1.5770639885975517, 1.3778639766477814, 1.5526004159527491, 1.2981286002324337, 1.3541054108708832, 1.2939089165183058, 2.2501240741458552, 1.321565684411047, 1.3230904583530791, 1.2920082150917849, 1.4764409886168359, 1.7996894780704527, 1.4539206259227959, 1.270778808753239, 1.017111356436899, 1.7383945569053587, 1.6359854457450882, 1.2157562214321824, 1.5662451637301884, 1.4414555729057195, 1.0236275024404971, 1.1683643451028551, 1.2020198188002655, 1.1982491693789572, 1.1699616378163733, 1.1675210831568272, 1.3992909872446242, 1.1632366691776266, 1.4304981033858841, 1.0998497078550047, 1.3597904129052227, 1.1615007321743427, 1.2093119265434569, 1.2370710654869286, 1.2298420098029093, 1.5611383233513916, 1.533960892587654, 1.699401588953745, 1.1349112642290879, 1.0346197305762295, 1.4938297698770377, 1.1382239471930273, 0.94566058479657877, 1.4384626806733161, 1.1890363600451346, 1.0308979352818159, 1.585672860572779, 1.2079393491568791, 0.91159913355565136, 1.2346909282649607, 1.3203540684433768, 1.0516393149628755, 1.1670891480786121, 1.7983781699817754, 1.427259980780144, 1.5453869474130879, 1.8193453274289386, 1.5629678512875573, 1.5882812549046459, 1.2287721479900102, 1.2724398783423885, 1.5130052446577227, 1.2701615312955878, 1.3516204695865564, 1.357494354899264, 1.4333619917775688, 1.4639874006394891, 1.359853708654696, 1.7254589014248229, 1.3481835720500686, 1.3260530570466225, 1.2944471095861478, 1.2731302943272482, 1.2377199241223691, 1.2981826375781524, 1.5164276906084806, 1.1057864807516555, 1.1869753538116861, 1.893977196000673, 1.3231493446049145, 1.3504362884298025, 1.4729761375243013, 1.0460073183396432, 1.2309649633960826, 1.1599528663358838, 1.2245171771356445, 1.7871069782059521, 1.0575143714032278, 1.2294439889892277, 1.3621589507041052], 'cumulative_train_loss': [2.5270681542012601, 2.2209856680895617, 2.2178488153350973, 2.2868790992150134, 2.3263277851343553, 2.3725599181211305, 2.4522947270780677, 2.5034270557175633, 2.5319838125639733, 2.4812780215488335, 2.4579919916059656, 2.4349030458531824, 2.437450091169298, 2.4149200205213091, 2.3967979955815761, 2.3868282005675225, 2.3681044465303418, 2.3742978713251164, 2.3718297739959615, 2.3604405093090643, 2.3504062275127295, 2.3363705304115694, 2.3267077966494525, 2.3163946988443129, 2.3099545278400804, 2.3028828127511725, 2.2962358200172055, 2.2888097856623677, 2.2889456862203921, 2.2872008842492026, 2.2789217245782747, 2.260794312581607, 2.2509416139449794, 2.2484545591862832, 2.2406022141641171, 2.2376179069828188, 2.2402812495229667, 2.2370123991721997, 2.2318959576882498, 2.2333629946942546, 2.2344147536380854, 2.2339281082111695, 2.2335050717068619, 2.2310588029181169, 2.228432457822096, 2.2245077485969555, 2.2224697518757757, 2.2201851901925056, 2.2221550452800876, 2.2235278391778697, 2.2225770463784986, 2.2202901312436008, 2.2191457624426878, 2.2155192155509353, 2.2131417042040762, 2.2110229041356355, 2.2085984488459358, 2.2103866967669621, 2.2054779187337696, 2.2033763848449102, 2.1993732625118856, 2.1966980324819865, 2.1953567383921926, 2.1943259286540231, 2.1924495587629527, 2.1929884399784139, 2.1902412550297194, 2.1801063241202865, 2.1781909730509978, 2.1717195931681013, 2.1673249251305888, 2.1665146806697262, 2.1659455818591566, 2.1646060505239797, 2.1647341650573497, 2.1626975981085632, 2.1611841454249197, 2.1598821912117185, 2.1579677315342263, 2.1571923017207415, 2.1535908602216765, 2.1493103702335157, 2.1461082981256236, 2.1445288369096382, 2.1443666656794811, 2.1412925223848012, 2.1352929328967889, 2.1326227584665154, 2.1328299256213739, 2.1287971462349429, 2.129302131139001, 2.1300760657312918, 2.1287473142789048, 2.125662059995896, 2.1228536849513264, 2.1209058692084883, 2.1200897600440558, 2.1148376927516148, 2.1141587293857484, 2.1147655571828348, 2.1166593480955966, 2.1163671015401881, 2.1159859783632071, 2.1139937175142207, 2.1151497938821762, 2.114584640872184, 2.1134556890931591, 2.1119775221480799, 2.1115288858123007, 2.1091845943818757, 2.1078493181966875, 2.1068015282265229, 2.1054445401254762, 2.101399500219185, 2.100671188970864, 2.099255950225059, 2.0990868067653721, 2.0986333484278346, 2.0998902717888788, 2.0987461180445308, 2.0966233730923984, 2.0991021762491324, 2.0983443490648952, 2.0973804656794037, 2.0944582467772945, 2.0940365543218982, 2.0917722623097958, 2.0919827871446963, 2.0931013204898896, 2.0911620323332607, 2.0903625338542065, 2.0918160528220193, 2.0920936131544563, 2.0927578615535567, 2.0902166331889935, 2.0916004814909201, 2.0914893895695701, 2.0912555724236692, 2.0911085811027688, 2.0902298261277821, 2.0901561744547488, 2.0888706358475941, 2.0877406562379295, 2.0860372900540525, 2.0855879935440949, 2.0847129384227006, 2.083406553248385, 2.0818928142157884, 2.077369844529239, 2.0752805750882608, 2.0749969866211235, 2.0743095288171602, 2.0745885919982121, 2.073984176831662, 2.0726818767593498, 2.0717440486115675, 2.0713579924061043, 2.0678512031807328, 2.0662558894053298, 2.0645906580608377, 2.0627997337272665, 2.0617653563861622, 2.0614622795653288, 2.0621507935423375, 2.0621624038652357, 2.0615668941006491, 2.0605388025357292, 2.0595302768748205, 2.0608860222246088, 2.0612857991689917, 2.0601754270043009, 2.0581535140616363, 2.0566328717057236, 2.0552228651491773, 2.0572477740960466, 2.0567512761944906, 2.0547579075551523, 2.05219873147598, 2.0513094351073335, 2.0492374295415656, 2.0467817696763015, 2.0465681264999609, 2.0471758514599077, 2.0465984721844466, 2.0450626215105965, 2.0444943658024877, 2.0447420794196636, 2.0435106073052087, 2.0442487000802432, 2.0452032510938061, 2.0452036989813256, 2.0447788304515728, 2.0445837437049432, 2.0437964952356942, 2.0425701558181304, 2.0421901172637691, 2.0421191894690325, 2.0398201567779584, 2.0379730154490492, 2.0343035970344117, 1.9280554482140217, 1.8126743450734411, 1.9620291608088389, 1.9340126713936636, 1.9032268511865338, 1.88912676731725, 1.9133987745903756, 1.9061700861052839, 1.934034149219966, 1.9278231535447654, 1.9165239996454237, 1.9342238793349196, 1.9176866693957397, 1.9157242465682929, 1.8872596300684996, 1.8757630664640441, 1.8960795173672966, 1.8899686437676437, 1.8826081507140948, 1.8855200387856168, 1.8773029786095548, 1.870874128554874, 1.8623939422351463, 1.8547450719564087, 1.8669440706996427, 1.8592296292607271, 1.8559817141123003, 1.8581718685403796, 1.8584086058356248, 1.846874543866134, 1.8365344043369343, 1.8347818362198953, 1.8338706562538181, 1.843401587895604, 1.8464213567971541, 1.84888651476247, 1.8447416308593416, 1.8503639564621617, 1.8498064374274079, 1.8475036543057308, 1.847063521265359, 1.8495375024414258, 1.8483981308525275, 1.8435286822444095, 1.8347465748488918, 1.8319237452107777, 1.8261420713142644, 1.8255777694643696, 1.8276167949675763, 1.829697777246279, 1.8298343754724491, 1.8251234095165647, 1.8240331559156964, 1.8243500979508558, 1.8256681892547977, 1.823934621376424, 1.83001850851609, 1.8309078184390635, 1.8326677582798596, 1.8358429640610632, 1.8355089541363152, 1.8328357519194627, 1.833538965082429, 1.8397879374607577, 1.8419354026303141, 1.839851748706536, 1.8379702995386653, 1.8335574708932774, 1.8423454732934603, 1.8469084889894096, 1.8491160485633158, 1.8465335282058717, 1.8449352153185783, 1.8413449184092798, 1.8410470005224573, 1.842497394332028, 1.8426728151417948, 1.843560067632769, 1.8440236968568435, 1.8428501973491669, 1.8460611768761994, 1.842744505738821, 1.8384210970011963, 1.8371122680437144, 1.839731084437894, 1.8407266789639325, 1.8437228689220948, 1.8406849144431159, 1.8436314413047181, 1.8421925576630118, 1.8407177195146618, 1.8399579155153962, 1.83738980792776, 1.839790494965645, 1.8375438079270341, 1.8369611537844017, 1.8352138499805846, 1.8332814202534249, 1.8335093831573759, 1.8339475324326591, 1.833393708312411, 1.8348462747888634, 1.8368758027986449, 1.8370783432313031, 1.8362709937224415, 1.8385361310659825, 1.8390789378829029, 1.8366934990796377, 1.8351894499384649, 1.8327147638262462, 1.8327852471536572, 1.8305008688792939, 1.8303053121697113, 1.8311071827168135, 1.8297655029837563, 1.826208760863671, 1.8270329287622709, 1.8268926809584252, 1.8262750639815264, 1.8259738670949726, 1.8259711949097159, 1.8284266522507873, 1.8284563626617854, 1.8288721816862126, 1.8283168839590083, 1.8293088808357918, 1.8308271560775802, 1.8280276139984004, 1.8284822658989925, 1.8285074844882059, 1.8266434861917986, 1.8252773449625077, 1.8227240707261336, 1.8210878168090612, 1.8180107830146843, 1.8180401304061513, 1.8184416884400811, 1.817942761055483, 1.8188403974459304, 1.8194027528287928, 1.8191814373368385, 1.8191262261348931, 1.8178579351038837, 1.8171595134131036, 1.8179909212084056, 1.8176702191321046, 1.8181622013765579, 1.8201500113526887, 1.8202817945706442, 1.8203615345827433, 1.820997063639926, 1.820975308640177, 1.8187160688774555, 1.8205876733686841, 1.8207816626453648, 1.8199640762434048, 1.8198752436225998, 1.8203759559670483, 1.8220010189440079, 1.8224753293532194, 1.8245861341948444, 1.8241391856084352, 1.8223697871672726, 1.8210322360125701, 1.8209044597338979, 1.8222063086463967, 1.8234724199746031, 1.8230378289162179, 1.8221331331435013, 1.8218711878820157, 1.822789463388333, 1.8232972619654746, 1.8218062625089559, 1.8196663169542775, 1.8204921936060037, 1.8180045796691986, 1.8175987756033456, 1.818014679613791, 1.8160903690942176, 1.8156514227166849, 1.8153519461183136, 1.8137580089776391, 1.8143544168132488, 1.8122748994757316, 1.8119411062201691, 1.812311067750269, 1.8126578526316557, 1.8135355081678275, 1.812297938006042, 1.8137873832813307, 1.8136321076677229, 1.8125059252038698, 1.8126485718862058, 1.81354794680383, 1.8136603473980342, 1.8136132236084579, 1.8154480014207466, 1.816453549408467, 1.8159477882622383, 1.8178066905017556, 2.0639565201596639, 1.9282974203494985, 1.8396307732544803, 1.8349377806644314, 1.7701713202302372, 1.7185458877626769, 1.7763963928311735, 1.7627061728063995, 1.7950323578284537, 1.8145774868280831, 1.8066812903610989, 1.7880660474683745, 1.7643382290514842, 1.7551403967240911, 1.7293041631006405, 1.733274223524202, 1.7180325859905548, 1.7269632143468467, 1.7098508432738015, 1.7108493911633122, 1.7258887183242364, 1.7400520727889381, 1.7444674801545392, 1.7532839957949515, 1.7477213495744823, 1.7324821173474605, 1.7575535554232318, 1.7630358850929029, 1.7581896659902763, 1.7590055112945684, 1.7538740354145621, 1.7551250910252123, 1.7533316303214408, 1.7457848912013123, 1.749657246664946, 1.7612333989898243, 1.7644910703414092, 1.7650962374589345, 1.7711368559461271, 1.7747118604733352, 1.7757959806700989, 1.7749551218360551, 1.7715357665129077, 1.7767101238532419, 1.7832874873028739, 1.7797123674739419, 1.7779715770117579, 1.7772855336428794, 1.7797049437201233, 1.7814120049295783, 1.7840757248097741, 1.7867810834584492, 1.7830561051303584, 1.7856859736379338, 1.7838784665734384, 1.7831676398834708, 1.7814836462960311, 1.7801289881178701, 1.7763229033506396, 1.7714387274827759, 1.764082129704891, 1.7630802192119999, 1.7635547921566559, 1.7692909170498987, 1.7676370093871061, 1.7689854957981033, 1.7697571693325802, 1.7684603651117727, 1.7626838324814917, 1.7556111790148592, 1.747762382449376, 1.7442366386083676, 1.7448290005900762, 1.7453871801826522, 1.7454616467402502, 1.7432462293471154, 1.7401942721372032, 1.7435247789845674, 1.743195817266181, 1.7436195514666657, 1.7433544159385825, 1.7443121923564664, 1.7464884817429154, 1.7499080365609951, 1.7500078074823711, 1.7472804736995122, 1.749360135011881, 1.7484128107433565, 1.74880100208889, 1.7467472839239968, 1.7431939611919065, 1.7444685674560136, 1.7459787312971762, 1.7447510124298709, 1.7442457166247287, 1.7439699312235621, 1.7442786044633518, 1.7458423243171464, 1.745482626248603, 1.7436174828937381, 1.7400215849583593, 1.7393946429020686, 1.7369860994394204, 1.7370209948359607, 1.736810366362709, 1.7332815185418498, 1.7309902215653581, 1.7301907636030309, 1.7313275029212554, 1.7307940781915696, 1.7309037625543038, 1.7295291195957112, 1.7316120089060014, 1.7314270113301931, 1.7317513789167112, 1.7311010319697524, 1.7304577014525895, 1.7313088676688799, 1.7328854655416288, 1.7319221371450508, 1.7306368374032792, 1.7312664302612919, 1.7338704512131984, 1.7344897400435098, 1.7361792038276675, 1.7341620406150902, 1.7323586267208244, 1.7339530724645231, 1.7343386387542514, 1.7328958083954333, 1.7336087242262121, 1.7335453916006374, 1.7343004778714102, 1.7339346344545927, 1.7329723806951693, 1.7329135989139981, 1.7315367125744883, 1.7326401031259735, 1.7309386588035665, 1.7297755846977478, 1.7310146823571926, 1.7288267883526123, 1.7270463828538065, 1.7292504698678224, 1.7294797151764023, 1.7289645499634798, 1.7299410641363775, 1.7292688634692259, 1.7315595555445178, 1.7318208824907615, 1.7332107663348002, 1.7337175240490925, 1.733294566508321, 1.7344188621142467, 1.7339017699060926, 1.7341101522937583, 1.7340136989946171, 1.7338019905358364, 1.7321935964578343, 1.7324502257118255, 1.7319212203825576, 1.7315862102945609, 1.7301536050222908, 1.7304630687365508, 1.7298685255457824, 1.7300657565027258, 1.7307224761405229, 1.730748169157041, 1.7311622090213803, 1.7309698218712379, 1.7298305170811199, 1.7294868084188526, 1.728850310316278, 1.727981853404772, 1.7281418641123443, 1.7278378220096431, 1.7259193695693582, 1.7250304857732965, 1.7244413629639124, 1.7251974113546535, 1.7257249919818556, 1.7259826127832909, 1.7249232321279611, 1.7249525980895775, 1.725319406973393, 1.7241401061685182, 1.7244112509765022, 1.7259345003047322, 1.7248194740572991, 1.7258456541246769, 1.724056465118265, 1.7233892312568422, 1.7240511786173855, 1.7246136642440533, 1.7235277247063101, 1.7268723513977642, 1.7280023162809206, 1.7284576762968566, 1.7287732402660976, 1.7268220987430025, 1.467302580379467, 1.506937369882642, 1.5866792848121511, 1.6664642905528206, 1.6216795494852363, 1.6234744160791093, 1.5993446269989615, 1.6100695374068794, 1.6167219934832631, 1.6000005115547209, 1.5885208719052477, 1.602625917170386, 1.6435272801011751, 1.6566984310283885, 1.6621297450096331, 1.6696199178044271, 1.6839063311327114, 1.6651524409323191, 1.6553160086987371, 1.669835997454578, 1.6717812377180614, 1.6722055439507533, 1.6739396856800817, 1.6566796830209689, 1.6537461201567083, 1.6339259064514489, 1.6292851041042005, 1.6262872853523611, 1.6244219081597617, 1.6154743717608961, 1.6252956900920419, 1.6198518297812985, 1.6195722255296703, 1.617267868966956, 1.638894721598271, 1.6428816755192091, 1.6442526283587255, 1.6462137066078757, 1.6391148633704464, 1.6371653241888877, 1.6328681470436039, 1.6348420890278923, 1.6313709894352513, 1.6238506646492445, 1.6209214814673192, 1.6179222906478943, 1.6197638887856969, 1.6202030236787723, 1.625434818382169, 1.6199347936229, 1.6180737501110167, 1.619084559479661, 1.6165983360286027, 1.6209131841657325, 1.6234104593064167, 1.6167624502620657, 1.614538457110807, 1.6136192329395533, 1.618245468698311, 1.6199588657698565, 1.6156287799608664, 1.6182882789521458, 1.6184966504206981, 1.6224947561373686, 1.6223946890039145, 1.6264312585089538, 1.6224020786251674, 1.619221704487509, 1.6197159424892265, 1.6177503929026822, 1.6140741236959277, 1.6120815833905886, 1.6111985676198501, 1.614438873436689, 1.6117426711180667, 1.618714646869486, 1.6189681423252913, 1.6193082496495579, 1.6219155507433982, 1.6283389476760244, 1.6259434873256529, 1.6283862187815206, 1.6300562375906538, 1.6332459485363109, 1.6337892583055689, 1.6336585436900233, 1.6333437725935056, 1.6330524866052512, 1.6337130708949785, 1.6322428919730956, 1.6350249573636331, 1.6335778005839034, 1.6309359038509446, 1.6340864678558871, 1.6324852922226927, 1.6341367427359519, 1.6335534710740163, 1.6351618355596134, 1.6343289354669956, 1.6326985692684581, 1.6331723654111587, 1.6323583488346762, 1.6298306590594167, 1.6329524044730663, 1.6319627395757357, 1.6307116586963848, 1.632660079412509, 1.6311774302065127, 1.6299802668413026, 1.6288285952266559, 1.6290201184147417, 1.6263252237105368, 1.6242026468240316, 1.6217922106943963, 1.6226404810615389, 1.6205275453246291, 1.6204039831782795, 1.620486812918428, 1.6210336803639345, 1.6230633870856959, 1.6228022471652737, 1.6239027845680896, 1.6227636303113064, 1.6237904336240714, 1.6288094867105427, 1.6283029642733664, 1.6288165510244346, 1.6308247056240184, 1.6314044506893066, 1.626592091664304, 1.6275781926235586, 1.6284829999581101, 1.6280273000000252, 1.6283606842410796, 1.627924246274062, 1.6265718037908024, 1.6294288741258764, 1.6307238127031465, 1.6286261112039275, 1.6263750507741621, 1.6268280320956852, 1.627213885813515, 1.6267996723495324, 1.6275603755636703, 1.6264552399823717, 1.6259044708604771, 1.6220715445300073, 1.621598271010422, 1.6228204406442757, 1.6235215443021525, 1.6258442320350628, 1.6250959766661039, 1.6253213700771327, 1.626794517800161, 1.6262217852431491, 1.6259807864228233, 1.6267364838773462, 1.6269397218398576, 1.6274831486980126, 1.6284642563997702, 1.6294483243595848, 1.6292308088601117, 1.6297312516709996, 1.6303082327341485, 1.630943921215751, 1.6292389977431037, 1.6308970181362219, 1.6317209568234432, 1.6329426495760075, 1.6340517298920472, 1.6334760696138506, 1.6328995548799266, 1.6348421649942146, 1.635413476210174, 1.6353341934433308, 1.6332088802952784, 1.6320045706946134, 1.6330225549035098, 1.6349634809362412, 1.6352663477882046, 1.6354279255523512, 1.6350409713502096, 1.6367030261751181, 1.6369006454221766, 1.636010903131512, 1.6354467412820399, 1.6361284004629741, 1.6363252492283953, 1.6353799691500062, 1.6354013336547741, 1.6345624015421472, 1.6338141311232595, 1.6311430839492755, 1.6316296177796585, 1.6300675163804443, 1.6277338862284945, 1.6281077705324727, 1.6275309115102832, 1.6266607824867214, 1.6282121358146349, 1.5988930166523161, 1.4730669124775566, 1.4783409230736639, 1.5243980111761635, 1.5834338063628064, 1.5997451669002776, 1.6316859271355504, 1.624643204906246, 1.6064111763563458, 1.5862027204348093, 1.5508452514198607, 1.5558416006730045, 1.545858208258112, 1.5355275920593414, 1.528033236448161, 1.49872092355766, 1.5089769771950328, 1.5337534808797118, 1.5244054973083996, 1.5456001687824215, 1.5426526281109951, 1.5602691134326727, 1.5593931627301874, 1.5520764805261382, 1.5662197666074444, 1.5651356429257088, 1.5655580466940555, 1.5602748439146013, 1.5544248130579006, 1.5604516696427673, 1.5576808028362563, 1.5601789574147056, 1.5550602089598564, 1.5582598569266879, 1.5475595303262029, 1.5566708920473056, 1.5600179784243871, 1.5590563896873131, 1.5640264386319147, 1.5635193697259691, 1.5653977960423053, 1.5616936450692531, 1.5644608121276966, 1.5633939795487866, 1.5641860537608716, 1.5572184205108099, 1.558366630660398, 1.5636773011348148, 1.5540080976882349, 1.5579832760586689, 1.5627838931599549, 1.5595934699038547, 1.5584191211272274, 1.5564662062845489, 1.5589604657065916, 1.5591874178640901, 1.5593139403614957, 1.5490019228367797, 1.5471442338317591, 1.5437187666550616, 1.5458537260415139, 1.54038837953329, 1.5398525982905167, 1.541860333733458, 1.5415431949342382, 1.5359537898453106, 1.5435387083020506, 1.5455221448691161, 1.5482853764303341, 1.5455575534978632, 1.5503741199183381, 1.5510324391127144, 1.5476012200483382, 1.5451644615381348, 1.543877438099899, 1.5513240904743109, 1.5497961458567262, 1.5534850944599894, 1.5543971863073349, 1.5559808786329365, 1.5524630677151423, 1.5477180996764364, 1.547108468118876, 1.5481776530955924, 1.5505981047393893, 1.5516929895724492, 1.5526449192460037, 1.5526691017106689, 1.552564856030751, 1.551795042087061, 1.5515203248449629, 1.5506979711934794, 1.5510962676689239, 1.5488231382547766, 1.5495161862127913, 1.5476659247159521, 1.5509188379039678, 1.557110285753929, 1.5562393242293895, 1.5547090451662677, 1.5503373031297456, 1.5506036093367732, 1.5496944753487523, 1.5508073788039931, 1.5497044432851723, 1.549658246477398, 1.5500194095698698, 1.5461926405953637, 1.5459824375229951, 1.54600383717968, 1.547971951322906, 1.5475860834441191, 1.5454008970877, 1.5413252312817651, 1.540317874255599, 1.5383726667166266, 1.5404302899729005, 1.5404449998789764, 1.5397164812198811, 1.53791049172931, 1.5337189292232047, 1.5386164245223266, 1.5401039407374595, 1.5395195447186305, 1.5390708511725242, 1.53717663818199, 1.5352292899233257, 1.5344215282837903, 1.5362874005199316, 1.5373816670272016, 1.5385842068935507, 1.5383885264072563, 1.5391123552911552, 1.5370510060318567, 1.5387221879506014, 1.5382627457949034, 1.5400418312987074, 1.5404969546004132, 1.5398789419888272, 1.5411776003293971, 1.5433546000490399, 1.5432022448014788, 1.5415707356342798, 1.5426857288139255, 1.5414658716907552, 1.5421037536709501, 1.541179716982457, 1.5403965076630932, 1.5432339803862405, 1.5429683426512588, 1.5438128411851011, 1.5428898315726873, 1.5409478010549467, 1.541617013850058, 1.5387416475511873, 1.5366023768899464, 1.5433961202283595, 1.5415428457128371, 1.5390644471614476, 1.5376709390842318, 1.5367030083840838, 1.5364224063672278, 1.5363504355867972, 1.5353960370120634, 1.5367416947803729, 1.5360215827963108, 1.5362895051302103, 1.5321545416648088, 1.5321438481989929, 1.5313776144162372, 1.5320258606438493, 1.5333137897056563, 1.5334371239594249, 1.5314208464403565, 1.5301540790296395, 1.531094458043684, 1.5306037797646055, 1.5324721445888001, 1.5314492657619303, 1.5309770229874955, 1.5309119606680337, 1.5298338215343823, 1.5308871957018317, 1.5294318053841545, 1.5296036171759573, 1.530387000737947, 1.5307489603395661, 1.5324682093262394, 1.5318846320060153, 1.5321774142585916, 1.5304142588894698, 1.5318858751099393, 1.5314250329642758, 1.5301854242077775, 1.5303625888043852, 1.5296788174687093, 1.5290556694173632, 1.5281354445901418, 1.5271526494011876, 1.5290749214542956, 1.4982799195006451, 1.3582186432646268, 1.5117440896736614, 1.5847408469766804, 1.5582895613730632, 1.5068937340645667, 1.4814387285293358, 1.4900797320812946, 1.4827299196695687, 1.5253614431456286, 1.5306230684416826, 1.5454761852407817, 1.5536187938230792, 1.5657658081806094, 1.5684162497829113, 1.5602433819481789, 1.549905420501452, 1.5438364480895896, 1.5458055392800603, 1.5344493972927329, 1.5262041240568593, 1.5243298046908151, 1.534868648025639, 1.5320460777372644, 1.5212255670826531, 1.5133893744363651, 1.5096160358916879, 1.5018385899174691, 1.4964153823802622, 1.5035734033675041, 1.4949304985866514, 1.5063951846627166, 1.5078155071911168, 1.514962231740123, 1.5112631021725165, 1.4988424251533152, 1.4980734112285286, 1.5024370605943485, 1.5029899246665026, 1.5002205989278212, 1.496533733741342, 1.4875317609429863, 1.4848535969283418, 1.4970672910614939, 1.4995753353386982, 1.4996552498255225, 1.5012169553019723, 1.5034522918280355, 1.5090980811200823, 1.5138250322716047, 1.5151515984638513, 1.5093838783609854, 1.5095575435611099, 1.5135309577416634, 1.5090203671967091, 1.5104717114979072, 1.5112985933045711, 1.516099726410195, 1.5177165178848042, 1.5217369245406402, 1.5246984219980984, 1.5229798066948912, 1.5265311758076772, 1.5224790575218607, 1.5172265676469872, 1.5158303083733455, 1.5108469049548199, 1.513086787136714, 1.5108334433096733, 1.5102366911564269, 1.5097432816107181, 1.5127414057654403, 1.5132998178406407, 1.5172153398126398, 1.5184931324813402, 1.5209726660082645, 1.5193919179007875, 1.5226398008822202, 1.5228227073701757, 1.524065357169589, 1.5292310125000959, 1.5276789438215497, 1.5260315384598848, 1.527567734426555, 1.5259508760251643, 1.5260425561289837, 1.5202881861013176, 1.519045803363255, 1.5213512467664896, 1.518876291175812, 1.5133381948588776, 1.511974918683453, 1.5108937699530718, 1.5075359301908136, 1.5098996705302563, 1.5102359264147927, 1.5071007169028818, 1.5081665155910506, 1.5059737188961679, 1.5077581371017734, 1.5067422177197993, 1.5055574350238978, 1.5045478145423214, 1.5051337874958894, 1.504684515701745, 1.5047715010043554, 1.5039219176442158, 1.500592899532964, 1.5001268283599949, 1.4978135212255079, 1.4940995622340616, 1.4984592150529563, 1.4977139910607167, 1.5015527693771562, 1.5026563431983617, 1.5039814372186653, 1.504240822598893, 1.5039113927148147, 1.502798011089675, 1.4997850461987092, 1.4966652462656111, 1.4939920213527154, 1.4946225969275886, 1.4957637215526833, 1.4933398569301344, 1.4902580387198181, 1.4883822128680408, 1.4881514276326584, 1.4876457005425168, 1.4901204091445095, 1.4889094896107424, 1.4905785307388377, 1.4921655430674357, 1.4919013377944972, 1.4917985775768086, 1.4914623054507516, 1.4916664441027734, 1.4930463458630845, 1.492433088621604, 1.4926611062258792, 1.4921343169775458, 1.4903497912863486, 1.4892707726241985, 1.4880429202648919, 1.488252126826094, 1.4862232022555559, 1.48391389671205, 1.4819953890835682, 1.4826798239605317, 1.4810826282273735, 1.4839944846634368, 1.4847059280868629, 1.4841701040782034, 1.4852554609217195, 1.4847128995205603, 1.485404557135751, 1.4863351366000346, 1.4859819300432089, 1.4855561272894755, 1.484273478554214, 1.4829879065799894, 1.48352568614099, 1.4852973757757135, 1.4857910272420201, 1.4837747113618494, 1.4843653130953804, 1.4828076082538801, 1.4833450555154866, 1.4826308255286991, 1.4791916399199245, 1.4796934407278852, 1.4783049982053409, 1.480198150053696, 1.4804092496226013, 1.4790463427367224, 1.4768558919237529, 1.4780954464949025, 1.4782245094418898, 1.4787726634691472, 1.4805262383126594, 1.4791891720098613, 1.4788080108621242, 1.4785791973048177, 1.4782942253884976, 1.4770929483070883, 1.478931860012334, 1.4795696993651446, 1.4793840592788698, 1.477412106724544, 1.4813659422551653, 1.4816425388652128, 1.4835911182178745, 1.4841096108293965, 1.4843797772472473, 1.4850815442737562, 1.4854496576750178, 1.4845577313135294, 1.4842510820889228, 1.4861274446472112, 1.487527829514683, 1.5564933940700942, 1.3339126508718275, 1.4422829224387821, 1.4754568156000674, 1.5983801517203109, 1.580795433423374, 1.5242386757680768, 1.4969712252412164, 1.4924500414104487, 1.486975771758303, 1.4688737177352611, 1.4636154162672967, 1.4631639436012063, 1.4663145434521734, 1.4340920046753982, 1.4308982360683931, 1.4441060481977988, 1.4603348121583624, 1.4601021366079754, 1.4645424549129835, 1.4752893696390323, 1.4668562190739516, 1.4660712296314631, 1.4724654726422071, 1.4603498309436427, 1.4574113911449627, 1.4358263036359107, 1.4312428730175977, 1.4161531078883471, 1.410710593648981, 1.4031491259686155, 1.3997573076070502, 1.4104280662768409, 1.4123259698717829, 1.4085384358353787, 1.4110212669790807, 1.4201933811964662, 1.4204488491893159, 1.4134211421143685, 1.4087576363757481, 1.4000530187016189, 1.3878354988936812, 1.3791505530283319, 1.3743925919840911, 1.3673668692018059, 1.3722381664430978, 1.3763141291382543, 1.3761962422397533, 1.3816383657667899, 1.3807469140554598, 1.3752261885270356, 1.3778121199830402, 1.3830874717051223, 1.3867916041980815, 1.3867410713013593, 1.388249723881767, 1.3844358900083933, 1.3859519694407831, 1.384271200077853, 1.3836011836209214, 1.3839989988633257, 1.3892388615425817, 1.3922108657354124, 1.3935663951196813, 1.3865295523310439, 1.3876125460280735, 1.3857479316636419, 1.3875259765572425, 1.3868666568760402, 1.3812574859340092, 1.3831673251314116, 1.3878188484493712, 1.3839528618070573, 1.3870440638187418, 1.3825675877542283, 1.390355757193025, 1.3921034696968484, 1.3995205257259278, 1.3992874778377642, 1.3960747091141661, 1.3926173969657536, 1.3927815856358181, 1.3903805715947524, 1.3903388766526299, 1.3863911622168885, 1.3825264050306436, 1.3829138082181982, 1.3857617412623708, 1.3830326915596727, 1.3830593149270565, 1.3848191884155374, 1.3854439023407392, 1.3898467611637026, 1.3909623434077241, 1.3907480661787344, 1.3852212861969524, 1.3848061373968348, 1.3810423767321844, 1.3806506773265945, 1.3839966620891886, 1.3843431599900806, 1.3823601189593813, 1.3837371549816608, 1.3852740506276915, 1.3878306711238819, 1.3851911434593316, 1.3875074614628473, 1.3866181461352238, 1.389493032638409, 1.3930225008198605, 1.392339238273216, 1.3940223974684753, 1.3933417713672938, 1.3927065743917488, 1.3904041712435622, 1.393350563498245, 1.3946280705950416, 1.3952082369351626, 1.3983471764404147, 1.4006467713928887, 1.4030624174528121, 1.4019978750512254, 1.3991499424865788, 1.3953359416296434, 1.3932621340311058, 1.3946440394330011, 1.3927186236775191, 1.3902426155876217, 1.3885184084700428, 1.3877362400020878, 1.3871113263100043, 1.3867391460858962, 1.389169664070562, 1.3917009903672204, 1.3895888578441382, 1.3885240039141791, 1.3872311635602037, 1.3837913084101496, 1.383806537481167, 1.383994527830348, 1.3842452270413721, 1.3843224747821037, 1.3848948970338228, 1.3834806965811499, 1.3842936359402209, 1.3820049450984662, 1.3813546966112173, 1.3836685233488764, 1.3830976778872688, 1.3849665347612581, 1.3859722394782394, 1.3857906887928111, 1.384188462798271, 1.3853430125081878, 1.3860023977137961, 1.3842645040614661, 1.3840914002071587, 1.3826651788196533, 1.3824690367833981, 1.383078174693511, 1.3814921633389663, 1.3803592833204517, 1.3803126510798627, 1.3798429734958739, 1.3776812234739166, 1.3777878972871427, 1.3778122846716796, 1.3790901815011329, 1.3795411721960549, 1.3814099027644617, 1.3813258764858494, 1.3827777877965735, 1.3811819352505226, 1.3800230422634749, 1.3806160838080455, 1.3785462590621373, 1.3807005943479935, 1.3821531683176693, 1.3802988336672748, 1.3789998601365463, 1.3778903348374658, 1.3792331081314837, 1.3789997954781061, 1.3802809956055646, 1.3802984713803021, 1.3807702610341421, 1.3808928927728223, 1.380375069210545, 1.379395482391772, 1.3793023634918524, 1.3811843254052527, 1.3822038739286975, 1.3822448520923152, 1.3811577038778919, 1.3821357723095764, 1.3819264267685509, 1.3817935558430319, 1.3815206125137249, 1.3822265260417572, 1.3834303159704175, 1.605376891495395, 1.3528215448021095, 1.31023099944924, 1.2896019297742911, 1.2387047218099219, 1.3399234280398298, 1.3443912112233134, 1.3398624200001794, 1.3283785787124887, 1.2869462199225177, 1.2861464032786283, 1.2805444984070564, 1.286995097787127, 1.290183331659009, 1.3126172440671255, 1.3295687449233899, 1.3495761717618993, 1.3342130972118549, 1.3217300442215045, 1.3093774842472667, 1.3290112527034417, 1.3362622771609551, 1.3322276074026362, 1.3324879422246048, 1.3230288453419388, 1.3043853884979999, 1.3136871683552949, 1.3138477495864225, 1.3121777496475004, 1.3055420098342339, 1.3003140537672011, 1.3078930664333415, 1.3201688238558313, 1.3326430800630713, 1.3336899336018704, 1.3495366249754739, 1.3563789693307813, 1.367406311296556, 1.370163549570717, 1.3642487184376881, 1.3572014808107438, 1.3562740331240577, 1.3601548886087818, 1.3583033259255777, 1.3533024034886816, 1.3513856306878818, 1.3508031096378548, 1.3546678015893407, 1.3526725832992155, 1.3531858416545917, 1.3435321518358343, 1.3336810864492674, 1.3375064612060541, 1.3394544442509488, 1.3394751545020089, 1.3340243479825273, 1.3363440679073104, 1.3359010767473154, 1.3322216977705625, 1.3265232838387933, 1.3293283889462335, 1.3299839126523196, 1.3285967921405986, 1.3277177743930433, 1.3245558088575884, 1.3303887515230872, 1.3265277020029251, 1.3263973192994314, 1.3275389459384044, 1.3204469372375627, 1.3213497776171608, 1.3204831883392514, 1.3197426448466469, 1.3221210667387848, 1.3261228693331151, 1.3276493240696945, 1.3285592884942303, 1.3257993432021384, 1.3259875167721706, 1.328274162362955, 1.3318479077486265, 1.3363038612620191, 1.3408706927355671, 1.3449069716431754, 1.3476832718173317, 1.3426958318611586, 1.3452710723119703, 1.3506323312653863, 1.3533024230054034, 1.3503596166619716, 1.3486330528118275, 1.3427821985186903, 1.3437156286983376, 1.3485897045309796, 1.3501719654407784, 1.351843594899969, 1.3539762733377061, 1.3531017517007682, 1.3506280596172295, 1.3511962787270184, 1.349597847242648, 1.3524135376190372, 1.34944082037206, 1.3498198006365814, 1.34965240631935, 1.3489524183469894, 1.3487773226660762, 1.3491588144808926, 1.3533374561483942, 1.3573665971300215, 1.3587487444112334, 1.3606988582494324, 1.3608508291958541, 1.3625335811638835, 1.3619732942484297, 1.3619054384193867, 1.361324023097445, 1.3688594198846298, 1.3684618264800599, 1.3680835741405268, 1.3674545922550472, 1.3683482896421877, 1.3718565546965045, 1.3725186286878417, 1.3717043844306624, 1.3688890368483382, 1.3717996750528052, 1.3738644329323852, 1.3726383126415729, 1.3741281691332132, 1.3746423150192189, 1.3719821042380236, 1.3704505678774732, 1.3691931528041084, 1.3679264319191493, 1.3664702730805856, 1.3650175590249172, 1.3652660072178835, 1.3638120357314478, 1.3642885349644196, 1.3624124169998266, 1.3623939456782728, 1.360988606654111, 1.3599349305157944, 1.3590873012296347, 1.3582017562387014, 1.3595827468281041, 1.3607613757281349, 1.3630348917618751, 1.3615135604678921, 1.359347983588848, 1.3602330233143378, 1.3587815092501252, 1.3560980345865747, 1.3566295908698069, 1.3555549308452826, 1.3534863931226735, 1.354956392916596, 1.3540314665639082, 1.3512654000868329, 1.3505411101096862, 1.3503547128162854, 1.3485215406907665, 1.3474149082259164, 1.3501488473696315, 1.3506135122289897, 1.3517801695226948, 1.3545641240204991, 1.3557976471176698, 1.3571656000908205, 1.3564145415818947, 1.3559261748285394, 1.3568344088258177, 1.3563361456609413, 1.3563091912392526, 1.3563159270372642, 1.3567513385362202, 1.3573539575983344, 1.3573679265899878, 1.3594134446496422, 1.359351384010794, 1.3591683758933764, 1.3588146111144352, 1.358348808903034, 1.3576965846003541, 1.3573765311073602, 1.3582272995454516, 1.3568841720179512, 1.3559849456712654, 1.3588172291686567, 1.3586304374191738, 1.3585877484429334, 1.3591805879820955, 1.3575658766150298, 1.3569164746493005, 1.3559113018587243, 1.3552441572802882, 1.3574258335668536, 1.3559183620304693, 1.3552858320327636], 'full_train_loss': [2.137572922832701, 2.0050192763021855, 1.6107231143483351, 1.6643982349456532, 1.4531084098136811, 1.3872626164896318, 1.3362220982941744, 1.2699970087530943]}

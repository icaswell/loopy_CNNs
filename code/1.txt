Good job.  You have followed directions.  Asserter passes.
[45mALN> [0m [45mloop outputs: [0m 
[45mALN> [0m [91m[repeating section] adding layer conv_1_unroll=0 with input input[0m 
{'nonlinearity': <function rectify at 0x7fbbabf90e60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fbba9a58450>, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_2_unroll=0 with input conv_1_unroll=0[0m 
{'nonlinearity': <function rectify at 0x7fbbabf90e60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fbba9a74490>, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_3_unroll=0 with input conv_2_unroll=0[0m 
{'nonlinearity': <function rectify at 0x7fbbabf90e60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fbba9a74cd0>, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_4_unroll=0 with input conv_3_unroll=0[0m 
{'nonlinearity': <function rectify at 0x7fbbabf90e60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fbba9a04290>, 'num_filters': 48}
[45mALN> [0m [91m[repeating section] adding layer conv_5_unroll=0 with input conv_4_unroll=0[0m 
{'nonlinearity': <function rectify at 0x7fbbabf90e60>, 'filter_size': 3, 'pad': 1, 'W': <lasagne.init.GlorotUniform object at 0x7fbba9a04810>, 'num_filters': 3}
[45mALN> [0m [94m[after repeating section] adding layer fc_1 with input conv_5_unroll=0[0m 
[45mALN> [0m [45mmarking layer fc_1 as output[0m 
[91mModel has 96505 total parameters[0m 
LoopyCNN instance with the following hyperparameters, layers and loops:[95m
HYPERPARAMETERS:[0m
	n_unrolls=1
	use_batchnorm=True[95m

ARCHITECTURE:[0m
main_stack:
	[93minput [input layer; output_dim=(3, 32, 32)][0m
	[96mconv_1 [conv2d layer; num_filters=48][0m
	[96mconv_2 [conv2d layer; num_filters=48][0m
	[96mconv_3 [conv2d layer; num_filters=48][0m
	[96mconv_4 [conv2d layer; num_filters=48][0m
	[96mconv_5 [conv2d layer; num_filters=3][0m
	[97mfc_1 [dense layer; output_dim=10][0m
loop:
	[96mconv_5 [conv2d layer; num_filters=3][0m
	[96mconv_1 [conv2d layer; num_filters=48][0m
(20000, 3, 32, 32) (20000,)
(1000, 3, 32, 32) (1000,)
*------------------------------------------------------------------------------*
Epoch 0, batch 499:
batchly_train_loss:  2.27977560827
cumulative_train_loss:  2.28434427682
*------------------------------------------------------------------------------*
Epoch 0, batch 999:
batchly_train_loss:  2.03357536422
cumulative_train_loss:  2.15883431055
*------------------------------------------------------------------------------*
Epoch 0, batch 1499:
batchly_train_loss:  1.98829551752
cumulative_train_loss:  2.10195012342
*------------------------------------------------------------------------------*
Epoch 0, batch 1999:
batchly_train_loss:  1.95574155177
cumulative_train_loss:  2.06537969529
*------------------------------------------------------------------------------*
Epoch 0, batch 2499:
batchly_train_loss:  1.95029548229
cumulative_train_loss:  2.04235364227
*------------------------------------------------------------------------------*
Epoch 0, batch 2999:
batchly_train_loss:  1.90369713046
cumulative_train_loss:  2.01923651792
*------------------------------------------------------------------------------*
Epoch 0, batch 3499:
batchly_train_loss:  1.88410787733
cumulative_train_loss:  1.99992690938
*------------------------------------------------------------------------------*
Epoch 0, batch 3999:
batchly_train_loss:  1.83950438223
cumulative_train_loss:  1.97986907903
================================================================================
Epoch 0 of 8 took 1420.338s
  training loss:		1.979396
evaluating model...
VALID_LOSS:  1.91443825841
VALID_ACC:  0.349090909091
FULL_TRAIN_LOSS:  1.89185549159
FULL_TRAIN_ACC:  0.310149253731
*------------------------------------------------------------------------------*
Epoch 1, batch 499:
batchly_train_loss:  1.8537159301
cumulative_train_loss:  1.85329159147
*------------------------------------------------------------------------------*
Epoch 1, batch 999:
batchly_train_loss:  1.79602775103
cumulative_train_loss:  1.82463101067
*------------------------------------------------------------------------------*
Epoch 1, batch 1499:
batchly_train_loss:  1.81758802198
cumulative_train_loss:  1.82228178162
*------------------------------------------------------------------------------*
Epoch 1, batch 1999:
batchly_train_loss:  1.75218538042
cumulative_train_loss:  1.80474891489
*------------------------------------------------------------------------------*
Epoch 1, batch 2499:
batchly_train_loss:  1.79422015973
cumulative_train_loss:  1.80264232122
*------------------------------------------------------------------------------*
Epoch 1, batch 2999:
batchly_train_loss:  1.88229265524
cumulative_train_loss:  1.81592180338
*------------------------------------------------------------------------------*
Epoch 1, batch 3499:
batchly_train_loss:  1.76659755355
cumulative_train_loss:  1.80887346817
*------------------------------------------------------------------------------*
Epoch 1, batch 3999:
batchly_train_loss:  1.66232424037
cumulative_train_loss:  1.79055023388
================================================================================
Epoch 1 of 8 took 1422.153s
  training loss:		1.790118
evaluating model...
VALID_LOSS:  1.73511436345
VALID_ACC:  0.427272727273
FULL_TRAIN_LOSS:  1.6915605695
FULL_TRAIN_ACC:  0.387213930348
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-01:36:16-2016_epoch=1
*------------------------------------------------------------------------------*
Epoch 2, batch 499:
batchly_train_loss:  1.71448656146
cumulative_train_loss:  1.71421283765
*------------------------------------------------------------------------------*
Epoch 2, batch 999:
batchly_train_loss:  1.69651659537
cumulative_train_loss:  1.70535585953
*------------------------------------------------------------------------------*
Epoch 2, batch 1499:
batchly_train_loss:  1.71255288386
cumulative_train_loss:  1.70775646804
*------------------------------------------------------------------------------*
Epoch 2, batch 1999:
batchly_train_loss:  1.64475617463
cumulative_train_loss:  1.69199851572
*------------------------------------------------------------------------------*
Epoch 2, batch 2499:
batchly_train_loss:  1.60754474026
cumulative_train_loss:  1.67510100162
*------------------------------------------------------------------------------*
Epoch 2, batch 2999:
batchly_train_loss:  1.61175763467
cumulative_train_loss:  1.66454025355
*------------------------------------------------------------------------------*
Epoch 2, batch 3499:
batchly_train_loss:  1.61951400841
cumulative_train_loss:  1.65810609448
*------------------------------------------------------------------------------*
Epoch 2, batch 3999:
batchly_train_loss:  1.52300710112
cumulative_train_loss:  1.64121449741
================================================================================
Epoch 2 of 8 took 1425.938s
  training loss:		1.640732
evaluating model...
VALID_LOSS:  1.75136926977
VALID_ACC:  0.337272727273
FULL_TRAIN_LOSS:  1.59009256779
FULL_TRAIN_ACC:  0.455771144279
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-02:07:50-2016_epoch=2
*------------------------------------------------------------------------------*
Epoch 3, batch 499:
batchly_train_loss:  1.54756493759
cumulative_train_loss:  1.54796033015
*------------------------------------------------------------------------------*
Epoch 3, batch 999:
batchly_train_loss:  1.68644120811
cumulative_train_loss:  1.61727007888
*------------------------------------------------------------------------------*
Epoch 3, batch 1499:
batchly_train_loss:  1.51831540479
cumulative_train_loss:  1.58426318292
*------------------------------------------------------------------------------*
Epoch 3, batch 1999:
batchly_train_loss:  1.46326619451
cumulative_train_loss:  1.55399880363
*------------------------------------------------------------------------------*
Epoch 3, batch 2499:
batchly_train_loss:  1.5865060441
cumulative_train_loss:  1.56050285334
*------------------------------------------------------------------------------*
Epoch 3, batch 2999:
batchly_train_loss:  1.56930801702
cumulative_train_loss:  1.56197086996
*------------------------------------------------------------------------------*
Epoch 3, batch 3499:
batchly_train_loss:  1.62692968998
cumulative_train_loss:  1.57125335353
*------------------------------------------------------------------------------*
Epoch 3, batch 3999:
batchly_train_loss:  1.58119961052
cumulative_train_loss:  1.57249694655
================================================================================
Epoch 3 of 8 took 1425.903s
  training loss:		1.571844
evaluating model...
VALID_LOSS:  1.75343385532
VALID_ACC:  0.395454545455
FULL_TRAIN_LOSS:  1.48477699504
FULL_TRAIN_ACC:  0.487164179104
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-02:39:26-2016_epoch=3
*------------------------------------------------------------------------------*
Epoch 4, batch 499:
batchly_train_loss:  1.47577712076
cumulative_train_loss:  1.47766789963
*------------------------------------------------------------------------------*
Epoch 4, batch 999:
batchly_train_loss:  1.42982116065
cumulative_train_loss:  1.45372058283
*------------------------------------------------------------------------------*
Epoch 4, batch 1499:
batchly_train_loss:  1.51790792747
cumulative_train_loss:  1.47513063774
*------------------------------------------------------------------------------*
Epoch 4, batch 1999:
batchly_train_loss:  1.51660321854
cumulative_train_loss:  1.48550396961
*------------------------------------------------------------------------------*
Epoch 4, batch 2499:
batchly_train_loss:  1.47775974974
cumulative_train_loss:  1.48395450585
*------------------------------------------------------------------------------*
Epoch 4, batch 2999:
batchly_train_loss:  1.42390908838
cumulative_train_loss:  1.4739435993
*------------------------------------------------------------------------------*
Epoch 4, batch 3499:
batchly_train_loss:  1.45564864826
cumulative_train_loss:  1.47132928792
*------------------------------------------------------------------------------*
Epoch 4, batch 3999:
batchly_train_loss:  1.51455669588
cumulative_train_loss:  1.47673406511
================================================================================
Epoch 4 of 8 took 1432.790s
  training loss:		1.476235
evaluating model...
VALID_LOSS:  1.63903696832
VALID_ACC:  0.409090909091
FULL_TRAIN_LOSS:  1.44185472423
FULL_TRAIN_ACC:  0.47184079602
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-03:11:09-2016_epoch=4
*------------------------------------------------------------------------------*
Epoch 5, batch 499:
batchly_train_loss:  1.36828841478
cumulative_train_loss:  1.36911144246
*------------------------------------------------------------------------------*
Epoch 5, batch 999:
batchly_train_loss:  1.3936531558
cumulative_train_loss:  1.38139458227
*------------------------------------------------------------------------------*
Epoch 5, batch 1499:
batchly_train_loss:  1.40463066465
cumulative_train_loss:  1.38914511008
*------------------------------------------------------------------------------*
Epoch 5, batch 1999:
batchly_train_loss:  1.41821668058
cumulative_train_loss:  1.39641663847
*------------------------------------------------------------------------------*
Epoch 5, batch 2499:
batchly_train_loss:  1.29296103763
cumulative_train_loss:  1.37571723854
*------------------------------------------------------------------------------*
Epoch 5, batch 2999:
batchly_train_loss:  1.379962123
cumulative_train_loss:  1.37642495519
*------------------------------------------------------------------------------*
Epoch 5, batch 3499:
batchly_train_loss:  1.34578929424
cumulative_train_loss:  1.37204718141
*------------------------------------------------------------------------------*
Epoch 5, batch 3999:
batchly_train_loss:  1.38652973886
cumulative_train_loss:  1.37385795378
================================================================================
Epoch 5 of 8 took 1425.063s
  training loss:		1.373474
evaluating model...
VALID_LOSS:  1.60932868446
VALID_ACC:  0.398181818182
FULL_TRAIN_LOSS:  1.38430636499
FULL_TRAIN_ACC:  0.482835820896
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-03:42:45-2016_epoch=5
*------------------------------------------------------------------------------*
Epoch 6, batch 499:
batchly_train_loss:  1.29864113892
cumulative_train_loss:  1.29881805638
*------------------------------------------------------------------------------*
Epoch 6, batch 999:
batchly_train_loss:  1.36367225395
cumulative_train_loss:  1.33127761472
*------------------------------------------------------------------------------*
Epoch 6, batch 1499:
batchly_train_loss:  1.29523305047
cumulative_train_loss:  1.31925474472
*------------------------------------------------------------------------------*
Epoch 6, batch 1999:
batchly_train_loss:  1.37116832886
cumulative_train_loss:  1.3322396332
*------------------------------------------------------------------------------*
Epoch 6, batch 2499:
batchly_train_loss:  1.36616296069
cumulative_train_loss:  1.33902701365
*------------------------------------------------------------------------------*
Epoch 6, batch 2999:
batchly_train_loss:  1.33678863064
cumulative_train_loss:  1.33865382542
*------------------------------------------------------------------------------*
Epoch 6, batch 3499:
batchly_train_loss:  1.33070144936
cumulative_train_loss:  1.33751744702
*------------------------------------------------------------------------------*
Epoch 6, batch 3999:
batchly_train_loss:  1.48163020068
cumulative_train_loss:  1.35553604588
================================================================================
Epoch 6 of 8 took 1431.367s
  training loss:		1.355030
evaluating model...
VALID_LOSS:  1.6469141961
VALID_ACC:  0.456363636364
FULL_TRAIN_LOSS:  1.35334011174
FULL_TRAIN_ACC:  0.505472636816
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-04:14:28-2016_epoch=6
*------------------------------------------------------------------------------*
Epoch 7, batch 499:
batchly_train_loss:  1.35256788372
cumulative_train_loss:  1.35390276789
*------------------------------------------------------------------------------*
Epoch 7, batch 999:
batchly_train_loss:  1.29354636602
cumulative_train_loss:  1.32369435854
*------------------------------------------------------------------------------*
Epoch 7, batch 1499:
batchly_train_loss:  1.32144003083
cumulative_train_loss:  1.32294241468
*------------------------------------------------------------------------------*
Epoch 7, batch 1999:
batchly_train_loss:  1.38760047298
cumulative_train_loss:  1.33911501555
*------------------------------------------------------------------------------*
Epoch 7, batch 2499:
batchly_train_loss:  1.2503320569
cumulative_train_loss:  1.32135131835
*------------------------------------------------------------------------------*
Epoch 7, batch 2999:
batchly_train_loss:  1.19694631393
cumulative_train_loss:  1.30061023725
*------------------------------------------------------------------------------*
Epoch 7, batch 3499:
batchly_train_loss:  1.37699158485
cumulative_train_loss:  1.31152497683
*------------------------------------------------------------------------------*
Epoch 7, batch 3999:
batchly_train_loss:  1.3219826372
cumulative_train_loss:  1.31283251126
================================================================================
Epoch 7 of 8 took 1427.186s
  training loss:		1.312421
evaluating model...
VALID_LOSS:  1.58393281623
VALID_ACC:  0.462727272727
FULL_TRAIN_LOSS:  1.3614286004
FULL_TRAIN_ACC:  0.500945273632
saving model to ../saved_models/cifar_scq_loopy_unrolls=1_Mar--8-04:46:04-2016_epoch=7
{'valid_loss': [1.9144382584115214, 1.7351143634544051, 1.7513692697724061, 1.7534338553185691, 1.6390369683181902, 1.6093286844648615, 1.6469141961026688, 1.5839328162292174], 'full_train_acc': [0.31014925373134311, 0.38721393034825885, 0.45577114427860677, 0.4871641791044779, 0.47184079601990114, 0.48283582089552246, 0.5054726368159207, 0.5009452736318406], 'valid_acc': [0.34909090909090906, 0.4272727272727273, 0.33727272727272728, 0.39545454545454545, 0.40909090909090912, 0.39818181818181819, 0.4563636363636363, 0.46272727272727271], 'batchly_train_loss': [2.2797756082652243, 2.033575364217699, 1.9882955175211179, 1.9557415517658248, 1.950295482288624, 1.9036971304550758, 1.8841078773300923, 1.8395043822278929, 1.8537159300963966, 1.7960277510331675, 1.8175880219816438, 1.7521853804156278, 1.7942201597267546, 1.8822926552405634, 1.7665975535504999, 1.6623242403702652, 1.714486561459311, 1.6965165953653845, 1.712552883856536, 1.6447561746318571, 1.6075447402630345, 1.6117576346719404, 1.6195140084142003, 1.5230071011170279, 1.547564937591952, 1.6864412081133795, 1.5183154047902789, 1.4632661945149747, 1.5865060440993135, 1.5693080170207057, 1.6269296899824868, 1.5811996105208186, 1.4757771207630319, 1.4298211606528428, 1.5179079274734142, 1.5166032185365292, 1.4777597497418749, 1.4239090883813967, 1.4556486482628008, 1.5145566958772796, 1.3682884147812504, 1.3936531558021295, 1.4046306646455438, 1.4182166805845957, 1.2929610376302527, 1.3799621230032884, 1.3457892942392378, 1.3865297388594884, 1.2986411389196562, 1.3636722539454296, 1.295233050467443, 1.3711683288594076, 1.3661629606937535, 1.3367886306399794, 1.3307014493618181, 1.4816302006812712, 1.352567883723865, 1.2935463660221385, 1.321440030832449, 1.3876004729817211, 1.2503320569020784, 1.1969463139328129, 1.3769915848494712, 1.3219826371986532, 0.98113249992196216], 'cumulative_train_loss': [2.2843442768188629, 2.1588343105520131, 2.1019501234169589, 2.0653796952901171, 2.042353642268611, 2.0192365179249072, 1.9999269093803473, 1.9798690790287021, 1.8532915914711241, 1.8246310106713457, 1.822281781622078, 1.8047489148870983, 1.8026423212175606, 1.8159218033821158, 1.8088734681675385, 1.7905502338843078, 1.7142128376521284, 1.7053558595306362, 1.7077564680449435, 1.6919985157155066, 1.6751010016193715, 1.6645402535454443, 1.6581060944812496, 1.641214497411454, 1.5479603301479481, 1.6172700788793948, 1.5842631829190512, 1.553998803628388, 1.560502853342459, 1.5619708699610408, 1.5712533535308391, 1.5724969465528438, 1.4776678996319093, 1.4537205828255706, 1.4751306377447986, 1.4855039696086607, 1.4839545058498009, 1.4739435993028858, 1.4713292879224771, 1.4767340651111243, 1.3691114424583277, 1.3813945822700398, 1.3891451100804157, 1.3964166384706576, 1.3757172385426057, 1.3764249551916026, 1.3720471814058985, 1.3738579537806903, 1.2988180563764369, 1.3312776147192797, 1.3192547447220038, 1.3322396332005959, 1.3390270136514077, 1.3386538254200924, 1.3375174470179358, 1.3555360458755665, 1.3539027678860212, 1.3236943585447394, 1.3229424146780642, 1.3391150155544178, 1.3213513183450665, 1.3006102372493245, 1.3115249768320834, 1.3128325112615109], 'full_train_loss': [1.8918554915895984, 1.6915605695040377, 1.5900925677940381, 1.484776995039268, 1.4418547242257156, 1.3843063649881411, 1.3533401117430872, 1.3614286004020375]}
